{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/fasthill/ML-DL-study-alone/blob/main/5-1%20%EA%B2%B0%EC%A0%95%20%ED%8A%B8%EB%A6%AC.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5uA_6TRHEMHV"
   },
   "source": [
    "## calculate prediction accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "26KAIfzEEMHc"
   },
   "source": [
    "<table align=\"left\">\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/rickiepark/hg-mldl/blob/master/5-1.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />구글 코랩에서 실행하기</a>\n",
    "  </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "rIXeXBVTfZrS"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from functools import reduce\n",
    "import datetime\n",
    "import os, re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "import xgboost\n",
    "from lightgbm import LGBMClassifier\n",
    "import lightgbm\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression, SGDClassifier, SGDRegressor\n",
    "from sklearn.model_selection import cross_validate, train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten, Dropout\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# write list, dictionary to pickle\n",
    "def save_to_pickle(path, filename):\n",
    "    open_file = open(path, \"wb\")\n",
    "    pickle.dump(filename, open_file)\n",
    "    open_file.close()\n",
    "\n",
    "# read list, dictionary from pickle\n",
    "def load_from_pickle(path):\n",
    "    open_file = open(path, \"rb\")\n",
    "    loaded_file = pickle.load(open_file)\n",
    "    open_file.close()\n",
    "    return loaded_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "# write list, dictionary to csv\n",
    "# path = './xxx/', my_dict = filename\n",
    "\n",
    "def save_dict_to_csv(path, my_dict):\n",
    "    df = pd.DataFrame.from_dict(my_dict, orient='index') \n",
    "    df.to_csv (path, index=False, header=True)  \n",
    "    \n",
    "def save_list_to_csv(path, my_list):\n",
    "    df = pd.DataFrame(my_list, columns=['columns'])\n",
    "    df.to_csv (path, index=False, header=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_df(date, com_name, precision, y_predict, weight):\n",
    "    dict_temp = {}\n",
    "    dict_temp['date'] = date\n",
    "    dict_temp[f'{com_name}_precision'] = f'{precision:.2f}'\n",
    "    dict_temp[f'{com_name}_predict'] = f'{y_predict[0]}'\n",
    "    dict_temp[f'{com_name}_yes'] = f'{weight[0,1]:.2f}'\n",
    "    dict_temp[f'{com_name}_no'] = f'{weight[0,0]:.2f}'\n",
    "    df_t = pd.DataFrame.from_dict(dict_temp, orient='index').T\n",
    "    df_t.set_index('date', inplace=True)\n",
    "    return df_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_df_todays(date, com_name, result, y_predict, weight, cr, yes_no):\n",
    "    if yes_no:\n",
    "        precision = result.loc['test_precision'].iloc[-1] \n",
    "        tn = result.loc['test_tn'].iloc[-1]\n",
    "        fp = result.loc['test_fp'].iloc[-1]\n",
    "        fn = result.loc['test_fn'].iloc[-1]\n",
    "        tp = result.loc['test_tp'].iloc[-1]\n",
    "    else:\n",
    "        precision = result.loc['precision'].iloc[-1]\n",
    "        tn = result.loc['tn'].iloc[-1]\n",
    "        fp = result.loc['fp'].iloc[-1]\n",
    "        fn = result.loc['fn'].iloc[-1]\n",
    "        tp = result.loc['tp'].iloc[-1]\n",
    "    \n",
    "    dict_temp = {}\n",
    "    dict_temp['name'] = com_name\n",
    "    dict_temp[f'precision'] = f'{precision:.2f}'\n",
    "    dict_temp[f'predict'] = f'{y_predict[0]}'\n",
    "    dict_temp[f'yes'] = f'{weight[0,1]:.2f}'\n",
    "    dict_temp[f'no'] = f'{weight[0,0]:.2f}'\n",
    "    dict_temp[f'tn'] = f'{tn:.1f}'\n",
    "    dict_temp[f'fp'] = f'{fp:.1f}'\n",
    "    dict_temp[f'fn'] = f'{fn:.1f}'\n",
    "    dict_temp[f'tp'] = f'{tp:.1f}'\n",
    "    if ((y_predict[0] == 1) & (cr > 0)):\n",
    "        result = 'right'\n",
    "    elif ((y_predict[0] == 1) & (cr <= 0)):\n",
    "        result = 'wrong'\n",
    "    else:\n",
    "        result = 'draw'\n",
    "    dict_temp[f'result'] = result\n",
    "    \n",
    "    df_t = pd.DataFrame.from_dict(dict_temp, orient='index').T\n",
    "    df_t.set_index('name', inplace=True)\n",
    "    return df_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_new_format(f_name): #5월 16일 이전 생성 데이터 확인\n",
    "    mon = int(f_name[11:13])\n",
    "    day = int(f_name[13:15])\n",
    "    if (mon < 5):\n",
    "        return False\n",
    "    elif (mon == 5) & (day < 16) :\n",
    "        return False\n",
    "    else:\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_filename(dir): # find a filename in a directory\n",
    "    p = re.compile('lgbm_bs_df_(.{9}).*csv')\n",
    "    dir_list = os.listdir(dir)\n",
    "    for fname in dir_list:\n",
    "        aa = p.search(fname)\n",
    "        if aa is None:\n",
    "            continue\n",
    "        else:\n",
    "            return aa.group()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import sys, os\n",
    "\n",
    "module_path = os.path.abspath(os.path.join('.')) # 현재 폴더로 이동\n",
    "if module_path+\"\\\\data\\\\base_data\\\\common_data\" not in sys.path:\n",
    "    sys.path.append(module_path+\"\\\\data\\\\base_data\\\\common_data\") #  공통으로 사용하는 각종 리스트, 코드 등 \n",
    "    \n",
    "import common_data as cd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get stock market opening days\n",
    "base_data_directory = './data/base_data/stock_market_holydays/'\n",
    "OPENING_DAYS_KOR = pd.read_pickle(base_data_directory+'opening_days_kor.pkl') # 한국 개장일 데이터 \n",
    "OPENING_DAYS_USA = pd.read_pickle(base_data_directory+'opening_days_usa.pkl') # 미국 개장일 데이터 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_START_DATE = datetime.date(2022, 3, 2)\n",
    "TRAIN_END_DATE = datetime.date(2023, 3, 31)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_next_date(current_date):\n",
    "    current_index = list(OPENING_DAYS_KOR).index(current_date)\n",
    "    next_date =  OPENING_DAYS_KOR.iloc[current_index+1]\n",
    "    return next_date"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 여기까지"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_start_index = list(df_o.index).index(TRAIN_START_DATE)\n",
    "train_end_index = list(df_o.index).index(TRAIN_END_DATE)\n",
    "df_o_train = df_o.iloc[train_start_index:train_end_index+1]\n",
    "df_o_val = df_o.iloc[train_end_index+1:val_end_index+1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_END_DATE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cd.code_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "code = cd.code_all # 전체 회사 코드\n",
    "\n",
    "code_mid = {'373220': ['LG에너지솔루션', 'lgenergy'], '207940': ['삼성바이오로직스', 'ssbio'],\n",
    "            '000270': ['기아', 'kia'], '028260': ['삼성물산', 'sscnt'],\n",
    "            '015760': ['한국전력', 'koreaelec'], '034020': ['두산에너빌리티', 'doosanener'],\n",
    "            '051900': ['LG생활건강', 'lglife'], '259960': ['크래프톤', 'crafton'],\n",
    "            '361610': ['SK아이이테크놀로지', 'skietech'], '086280': ['현대글로비스', 'glovis'],\n",
    "            '302440': ['SK바이오사이언스', 'skbio'],\n",
    "            }\n",
    "\n",
    "code_bad = {'051910': ['LG화학', 'lgchemical'], '033780': ['KT&G', 'ktng'],\n",
    "            '005490': ['POSCO홀딩스', 'poscoholding'], '068270': ['셀트리온', 'celltrion'],\n",
    "            '066570': ['LG전자', 'lgelec'],  '096770': ['SK이노베이션', 'skinnovation'],\n",
    "            '030200': ['KT', 'kt'], '003550': ['LG', 'lg'],\n",
    "            '329180': ['현대중공업', 'hhi'], '003490': ['대한항공', 'koreanair'],\n",
    "            '036570': ['엔씨소프트', 'ncsoft'], '009830': ['한화솔루션', 'hanhwasol'],\n",
    "            '090430': ['아모레퍼시픽', 'amore'], '011170': ['롯데케미칼', 'lottechem'],\n",
    "            '138040': ['메리츠금융지주', 'meritz'], '011070': ['LG이노텍', 'lginnotek'],\n",
    "           }\n",
    "\n",
    "code_good = {'005930': ['삼성전자', 'sec'], '035420': ['NAVER', 'naver'],\n",
    "             '005380': ['현대차', 'hyunmotor'], '035720': ['카카오', 'kakao'],\n",
    "             '000660': ['SK하이닉스', 'skhynix'], '006400': ['삼성SDI', 'sdi'],\n",
    "             '005935': ['삼성전자우', 'secpre'], '105560': ['KB금융', 'kbbank'],\n",
    "             '012330': ['현대모비스', 'mobis'],  '055550': ['신한지주', 'shgroup'],\n",
    "             '003670': ['포스코퓨처엠', 'poscochemical'], '034730': ['SK', 'sk'], \n",
    "             '032830': ['삼성생명', 'sslife'], '086790': ['하나금융지주', 'hana'],\n",
    "             '009150': ['삼성전기', 'sselec'], '017670': ['SK텔레콤', 'sktelecom'],\n",
    "             '011200': ['HMM', 'hmm'], '000810': ['삼성화재', 'ssfire'], \n",
    "             '010950': ['S-Oil', 'soil'], '018260': ['삼성에스디에스', 'sds'],\n",
    "             '316140': ['우리금융지주', 'woorifg'], '024110': ['기업은행', 'ibk'], \n",
    "             '377300': ['카카오페이', 'kakaopay'], '028050': ['삼성엔지니어링', 'ssengineering'],\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory_for_predict = './data/data_for_ml/predict/'\n",
    "directory_for_data = './data/company_pkl/'\n",
    "directory_for_common = './data/common_pkl/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dji = pd.read_pickle(directory_for_common+'dji.pkl')\n",
    "df_sec = pd.read_pickle(directory_for_data+'sec_investors.pkl')\n",
    "df_common = pd.read_pickle(directory_for_predict+'0_df_common.pkl')\n",
    "df_company = pd.read_pickle(directory_for_predict+'df_sec_company.pkl')\n",
    "df_combine = pd.read_pickle(directory_for_predict+'df_sec_combine.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(directory_for_predict+ 'prediction/prediction_list.pkl'):\n",
    "    os.makedirs(directory_for_predict+'prediction')\n",
    "    prediction_list=pd.DataFrame()\n",
    "    fname_p = 'prediction_list.pkl'\n",
    "    path_p = directory_for_predict+'prediction/' + fname_p\n",
    "    prediction_list.to_pickle(path_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_list = pd.read_pickle(directory_for_predict+ 'prediction/prediction_list.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_date = datetime.date.today()\n",
    "# prediction_date = datetime.date(2023, 5, 10) # 예측을 필요로 하는 일자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_base = pd.DataFrame()\n",
    "df_todays = pd.DataFrame()\n",
    "\n",
    "if prediction_date not in list(opening_days_kor):\n",
    "    print(f'오늘 {prediction_date}은 휴장일입니다.')\n",
    "else:\n",
    "    print(f'오늘 {prediction_date}은 개장일입니다.')\n",
    "\n",
    "for key, val in code_good.items():\n",
    " \n",
    "    com_name = val[1]\n",
    "    \n",
    "    fname = f'df_{com_name}_combine.pkl'\n",
    "    f_name = directory_for_predict + fname\n",
    "    df_o = pd.read_pickle(f_name) \n",
    "    com_fname = f'{com_name}_historical.pkl'  # 실제와 예측을 비교하기 위하여 실제데이터을 불러 옴\n",
    "    f_com_name = directory_for_data + com_fname\n",
    "    com_data = pd.read_pickle(f_com_name)\n",
    "\n",
    "    current_data = df_o.loc[:, 'retail_1':'weekday'] # select columns except targets columns\n",
    "    \n",
    "    prediction_row = current_data[current_data.index == prediction_date]\n",
    "        \n",
    "    if(len(prediction_row) == 0):\n",
    "        # 데이터 최종 기록일 확인\n",
    "        print(f\"미국 dji   마지막 일자 : {df_dji['date'].iloc[-1].isoformat()} (거래일자)\")\n",
    "        lf1_index = list(opening_days_usa).index(df_dji['date'].iloc[-1]) + 1 # 현재 개장일 이후에 오는 개장일 날짜 index (+1 index)\n",
    "        print(f\"     미국 다음 개장일은 {list(opening_days_usa)[lf1_index]} 입니다.\")\n",
    "        print(f\"한국 주식  마지막 일자 : {df_sec['date'].iloc[-1].isoformat()[:10]} (거래일자)\")\n",
    "        lf1_index = list(opening_days_kor).index(df_sec['date'].iloc[-1].date()) + 1 # 현재 개장일 이후에 오는 개장일 날짜 index (+1 index)\n",
    "#         lf1_index = l_index + 1 # 현재 개장일 이후에 오는 개장일 날짜 index (+1 index)\n",
    "        print(f\"     한국 다음 개장일은 {list(opening_days_kor)[lf1_index]} 입니다.\")\n",
    "        print(f\"df_common  마지막 일자 : {df_common.index[-1].isoformat()} (예측일자)\")\n",
    "        print(f\"df_company 마지막 일자 : {df_company.index[-1].isoformat()} (예측일자)\")\n",
    "        print(f\"df_combine 마지막 일자 : {df_combine.index[-1].isoformat()} (예측일자)\")  \n",
    "        raise Exception(f\"예측을 위한 최근 데이터가 준비가 되어 있지 않음. 혹은 한국, 미국 주식 휴장 등. 예측 당일 최신자료로 진행하도록...\")\n",
    "    \n",
    "    com_row = com_data[com_data['date']  == prediction_date]\n",
    "#     com_row = com_data[com_data['date'].apply(lambda x: x.date())  == prediction_date]\n",
    "#     com_data['date'].apply(lambda x: x.date()) <  prediction_date\n",
    "\n",
    "    try:\n",
    "        cr = com_row['close_cr'].values[0] # 실제의 등락을 확인\n",
    "    except:\n",
    "        cr = -1  # 예측 당일 아침 실제 결과가 없을시 임시 지정\n",
    "        \n",
    "        #**************************************************************\n",
    "\n",
    "    # locate the model data directory\n",
    "    directory_model_data = f'./data/data_for_ml/model/model/{com_name}/'\n",
    "\n",
    "    # get the model data filepath\n",
    "    columns_pkl = directory_model_data + 'best_columns.pkl'\n",
    "    scaler_pkl = directory_model_data + 'best_scaler.pkl'\n",
    "    scaler_p_pkl = directory_model_data + 'best_scaler_p.pkl'\n",
    "    model_pkl = directory_model_data + 'best_model.pkl'\n",
    "    model_p_pkl = directory_model_data + 'best_model_p.pkl'\n",
    "    result_pkl = directory_model_data + 'best_result.pkl'\n",
    "    \n",
    "    # load result data\n",
    "#     result = load_from_pickle(result_pkl)[:-5] \n",
    "    result = load_from_pickle(result_pkl)\n",
    "    \n",
    "    yes_no = is_new_format(find_filename(directory_model_data))\n",
    "    if yes_no:\n",
    "        precision = result.loc['test_precision'].iloc[-1]\n",
    "    else:\n",
    "        precision = result.loc['precision'].iloc[-1]\n",
    "    \n",
    "    # load columns data\n",
    "    real_columns = load_from_pickle(columns_pkl)[:-5] # column 읽기. target columns 5개는 제외\n",
    "    real_data_df = prediction_row[real_columns] # select necessary columns\n",
    "    \n",
    "    # scale the data\n",
    "    scaler = joblib.load(scaler_pkl) # scaler 읽기\n",
    "#     scaler = load_from_pickle(scaler_p_pkl) # scaler 읽기\n",
    "    real_scaled = scaler.transform(real_data_df)\n",
    "    \n",
    "    # apply the scaled real_data to the model\n",
    "    model = joblib.load(model_pkl) # model 읽기\n",
    "#     model = load_from_pickle(model_p_pkl) # model made with pickle 읽기\n",
    "\n",
    "    y_predict = model.predict(real_scaled)\n",
    "    weight = model.predict_proba(real_scaled)\n",
    "\n",
    "    df_temp = to_df(prediction_row.index[-1], com_name, precision, y_predict, weight)\n",
    "    df_base = pd.concat([df_base, df_temp],axis=1)\n",
    "    df_temp_todays = to_df_todays(prediction_row.index[-1], com_name, result, y_predict, weight, cr, yes_no)\n",
    "    df_todays = pd.concat([df_todays, df_temp_todays],axis=0)\n",
    "    \n",
    "#     print(f'**date: {prediction_row.index[-1].date()}, {precision:.2f}, {com_name}, 예측: {y_predict}, 가능성:{weight}')\n",
    "df_todays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_from_pickle(result_pkl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_date in list(opening_days_kor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# save current prediction data\n",
    "\n",
    "prediction_list = pd.concat([prediction_list, df_base], axis=0)\n",
    "prediction_list = prediction_list[~prediction_list.index.duplicated(keep='last')]\n",
    "\n",
    "directory_for_predict = './data/data_for_ml/predict/'\n",
    "fname_p = 'prediction_list.pkl'\n",
    "fname_c = 'prediction_list.csv'\n",
    "path_p = directory_for_predict+'prediction/' + fname_p\n",
    "path_c = directory_for_predict+'prediction/' + fname_c\n",
    "prediction_list.to_pickle(path_p)\n",
    "prediction_list.to_csv(path_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 결과를 회사별로 확인하기\n",
    "directory_for_predict = './data/data_for_ml/predict/'\n",
    "fname_p = 'prediction_list.pkl'\n",
    "predict_list = pd.read_pickle(directory_for_predict+'prediction/' + fname_p)\n",
    "\n",
    "for i, (key, val) in enumerate(code_good.items()):\n",
    "    print(\"***\", i, key, val)\n",
    "    k =  i * 4\n",
    "    globals()[f'{val[1]}_df'] = predict_list.iloc[:, k:k+4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "secpre_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_results(lgbm, model, train_scaled, val_scaled, test_scaled, train_target, val_target, test_target):\n",
    "# model = lgbmgs.best_estimator_  # 최적의 파라미터로 모델 생성\n",
    "    y_predict_train = model.predict(train_scaled)\n",
    "    y_predict_val = model.predict(val_scaled)\n",
    "    y_predict_test = model.predict(test_scaled)\n",
    "    result_dict= {}\n",
    "    result_dict['best_score'] = lgbm.best_score_ \n",
    "    result_dict['best_index'] = lgbm.best_index_\n",
    "    result_dict['acc_train'] = model.score(train_scaled, train_target)\n",
    "    result_dict['acc_val'] = model.score(val_scaled, val_target)\n",
    "    result_dict['acc_test'] = model.score(test_scaled, test_target)\n",
    "    result_dict['train_precision'] = precision_score(train_target, y_predict_train)\n",
    "    cm = confusion_matrix(train_target, y_predict_train)\n",
    "    result_dict['train_tn'] = cm[0,0]\n",
    "    result_dict['train_fp'] = cm[0,1]\n",
    "    result_dict['train_fn'] = cm[1,0]\n",
    "    result_dict['train_tp'] = cm[1,1]\n",
    "    result_dict['val_precision'] = precision_score(val_target, y_predict_val)\n",
    "    cm = confusion_matrix(val_target, y_predict_val)\n",
    "    result_dict['val_tn'] = cm[0,0]\n",
    "    result_dict['val_fp'] = cm[0,1]\n",
    "    result_dict['val_fn'] = cm[1,0]\n",
    "    result_dict['val_tp'] = cm[1,1]\n",
    "    result_dict['test_precision'] = precision_score(test_target, y_predict_test)\n",
    "    result_dict['recall'] = recall_score(test_target, y_predict_test)\n",
    "    result_dict['f1score'] = f1_score(test_target, y_predict_test)\n",
    "    result_dict['roc'] = roc_auc_score(test_target, y_predict_test)\n",
    "    cm = confusion_matrix(test_target, y_predict_test)\n",
    "    result_dict['test_tn'] = cm[0,0]\n",
    "    result_dict['test_fp'] = cm[0,1]\n",
    "    result_dict['test_fn'] = cm[1,0]\n",
    "    result_dict['test_tp'] = cm[1,1]\n",
    "#     result_dict['precision_neg'] = cm[0,0] / (cm[0,0] + cm[1,0])\n",
    "    return result_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_base = pd.DataFrame()\n",
    "df_todays = pd.DataFrame()\n",
    "\n",
    "if prediction_date not in list(opening_days_kor):\n",
    "    print(f'오늘 {prediction_date}은 휴장일입니다.')\n",
    "else:\n",
    "    print(f'오늘 {prediction_date}은 개장일입니다.')\n",
    "\n",
    "for key, val in code_good.items():\n",
    " \n",
    "    com_name = val[1]\n",
    "    \n",
    "    fname = f'df_{com_name}_combine.pkl'\n",
    "    f_name = directory_for_predict + fname\n",
    "    df_o = pd.read_pickle(f_name) \n",
    "    com_fname = f'{com_name}_historical.pkl'  # 실제와 예측을 비교하기 위하여 실제데이터을 불러 옴\n",
    "    f_com_name = directory_for_data + com_fname\n",
    "    com_data = pd.read_pickle(f_com_name)\n",
    "\n",
    "    current_data = df_o.loc[:, 'retail_1':'weekday'] # select columns except targets columns\n",
    "    \n",
    "    prediction_row = current_data[current_data.index == prediction_date]\n",
    "        \n",
    "    if(len(prediction_row) == 0):\n",
    "        # 데이터 최종 기록일 확인\n",
    "        print(f\"미국 dji   마지막 일자 : {df_dji['date'].iloc[-1].isoformat()} (거래일자)\")\n",
    "        lf1_index = list(opening_days_usa).index(df_dji['date'].iloc[-1]) + 1 # 현재 개장일 이후에 오는 개장일 날짜 index (+1 index)\n",
    "        print(f\"     미국 다음 개장일은 {list(opening_days_usa)[lf1_index]} 입니다.\")\n",
    "        print(f\"한국 주식  마지막 일자 : {df_sec['date'].iloc[-1].isoformat()[:10]} (거래일자)\")\n",
    "        lf1_index = list(opening_days_kor).index(df_sec['date'].iloc[-1].date()) + 1 # 현재 개장일 이후에 오는 개장일 날짜 index (+1 index)\n",
    "#         lf1_index = l_index + 1 # 현재 개장일 이후에 오는 개장일 날짜 index (+1 index)\n",
    "        print(f\"     한국 다음 개장일은 {list(opening_days_kor)[lf1_index]} 입니다.\")\n",
    "        print(f\"df_common  마지막 일자 : {df_common.index[-1].isoformat()} (예측일자)\")\n",
    "        print(f\"df_company 마지막 일자 : {df_company.index[-1].isoformat()} (예측일자)\")\n",
    "        print(f\"df_combine 마지막 일자 : {df_combine.index[-1].isoformat()} (예측일자)\")  \n",
    "        raise Exception(f\"예측을 위한 최근 데이터가 준비가 되어 있지 않음. 혹은 한국, 미국 주식 휴장 등. 예측 당일 최신자료로 진행하도록...\")\n",
    "    \n",
    "    com_row = com_data[com_data['date']  == prediction_date]\n",
    "#     com_row = com_data[com_data['date'].apply(lambda x: x.date())  == prediction_date]\n",
    "#     com_data['date'].apply(lambda x: x.date()) <  prediction_date\n",
    "\n",
    "    try:\n",
    "        cr = com_row['close_cr'].values[0] # 실제의 등락을 확인\n",
    "    except:\n",
    "        cr = -1  # 예측 당일 아침 실제 결과가 없을시 임시 지정\n",
    "        \n",
    "        #**************************************************************\n",
    "\n",
    "    # locate the model data directory\n",
    "    directory_model_data = f'./data/data_for_ml/model/model/{com_name}/'\n",
    "\n",
    "    # get the model data filepath\n",
    "    columns_pkl = directory_model_data + 'best_columns.pkl'\n",
    "    scaler_pkl = directory_model_data + 'best_scaler.pkl'\n",
    "    scaler_p_pkl = directory_model_data + 'best_scaler_p.pkl'\n",
    "    model_pkl = directory_model_data + 'best_model.pkl'\n",
    "    model_p_pkl = directory_model_data + 'best_model_p.pkl'\n",
    "    result_pkl = directory_model_data + 'best_result.pkl'\n",
    "    \n",
    "    # load result data\n",
    "#     result = load_from_pickle(result_pkl)[:-5] \n",
    "    result = load_from_pickle(result_pkl)\n",
    "    \n",
    "    yes_no = is_new_format(find_filename(directory_model_data))\n",
    "    if yes_no:\n",
    "        precision = result.loc['test_precision'].iloc[-1]\n",
    "    else:\n",
    "        precision = result.loc['precision'].iloc[-1]\n",
    "    \n",
    "    # load columns data\n",
    "    real_columns = load_from_pickle(columns_pkl)[:-5] # column 읽기. target columns 5개는 제외\n",
    "    real_data_df = prediction_row[real_columns] # select necessary columns\n",
    "    \n",
    "    # scale the data\n",
    "    scaler = joblib.load(scaler_pkl) # scaler 읽기\n",
    "#     scaler = load_from_pickle(scaler_p_pkl) # scaler 읽기\n",
    "    real_scaled = scaler.transform(real_data_df)\n",
    "    \n",
    "    # apply the scaled real_data to the model\n",
    "    model = joblib.load(model_pkl) # model 읽기\n",
    "#     model = load_from_pickle(model_p_pkl) # model made with pickle 읽기\n",
    "\n",
    "    y_predict = model.predict(real_scaled)\n",
    "    weight = model.predict_proba(real_scaled)\n",
    "\n",
    "    df_temp = to_df(prediction_row.index[-1], com_name, precision, y_predict, weight)\n",
    "    df_base = pd.concat([df_base, df_temp],axis=1)\n",
    "    df_temp_todays = to_df_todays(prediction_row.index[-1], com_name, result, y_predict, weight, cr, yes_no)\n",
    "    df_todays = pd.concat([df_todays, df_temp_todays],axis=0)\n",
    "    \n",
    "#     print(f'**date: {prediction_row.index[-1].date()}, {precision:.2f}, {com_name}, 예측: {y_predict}, 가능성:{weight}')\n",
    "df_todays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read selected columns, scaler and model to be appllied\n",
    "\n",
    "com_name = 'sec'\n",
    "# locate the model data directory\n",
    "directory_model_data = f'./data/data_for_ml/model/model/{com_name}/'\n",
    "\n",
    "# get the model data filepath\n",
    "columns_pkl = directory_model_data + 'best_columns.pkl' # save with pickle.dump \n",
    "scaler_pkl = directory_model_data + 'best_scaler.pkl'  # one saved with joblib.dump\n",
    "scaler_p_pkl = directory_model_data + 'best_scaler_p.pkl' # one saved with pickle.dump\n",
    "model_pkl = directory_model_data + 'best_model.pkl'  # one saved with joblib.dump\n",
    "model_p_pkl = directory_model_data + 'best_model_p.pkl' # one saved with pickle.dump\n",
    "result_pkl = directory_model_data + 'best_result.pkl'\n",
    "\n",
    "# load result data\n",
    "result = load_from_pickle(result_pkl) # with pickle.load\n",
    "\n",
    "yes_no = is_new_format(find_filename(directory_model_data))\n",
    "if yes_no:\n",
    "    precision = result.loc['test_precision'].iloc[-1]\n",
    "else:\n",
    "    precision = result.loc['precision'].iloc[-1]\n",
    "\n",
    "# load columns data\n",
    "real_columns = load_from_pickle(columns_pkl)[:-5] # column 읽기. target columns 5개는 제외\n",
    "# scale the data\n",
    "scaler = joblib.load(scaler_pkl) # scaler 읽기\n",
    "# apply the scaled real_data to the model\n",
    "model = joblib.load(model_pkl) # model 읽기\n",
    "\n",
    "choose dates to be predicted from companu_combined.\n",
    "1. model fit에 사용된 데이터 날짜 확인(언제부터 언제까지)\n",
    "2. test data 취득 (가장 최근 날짜 제외. 취득 날짜 에러 방지.)\n",
    "   start_date = \n",
    "    end_date = \n",
    "    \n",
    "3. 한개의 회사 자료로 테스트 후.\n",
    "4. code_company_good dict 이용하여 for loop\n",
    "\n",
    "# read each row one by one\n",
    "for i_row in len(df_tobetested):\n",
    "    prediction_row = ----------\n",
    "    real_data_df = prediction_row[real_columns] # select necessary columns\n",
    "    real_scaled = scaler.transform(real_data_df)\n",
    "\n",
    "    y_predict = model.predict(real_scaled)\n",
    "    weight = model.predict_proba(real_scaled)\n",
    "    if y_predict == 1: # 예측이 True일 경우\n",
    "        find value of cr_.5 # \n",
    "        if True: # 실제가 True\n",
    "            sum_count  # 일치하는 갯수 합계\n",
    "        else:\n",
    "            sum_count: # 일치하지 않는 갯수 합계\n",
    "        make dataframe \n",
    "            'no, predict, actual, weight(proba), 전일 최종가, 최저가, 최고가, 변화율(cr)' \n",
    "    else:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 여기서 부터 시작"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory_for_predict = './data/data_for_ml/predict/'\n",
    "directory_for_data = './data/company_pkl/'\n",
    "directory_for_common = './data/common_pkl/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "com_name = 'mobis'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read selected columns, scaler and model to be appllied\n",
    "\n",
    "# locate the model data directory\n",
    "directory_model_data = f'./data/data_for_ml/model/model/{com_name}/'\n",
    "\n",
    "# get the model data filepath\n",
    "columns_pkl = directory_model_data + 'best_columns.pkl' # save with pickle.dump \n",
    "scaler_pkl = directory_model_data + 'best_scaler.pkl'  # one saved with joblib.dump\n",
    "scaler_p_pkl = directory_model_data + 'best_scaler_p.pkl' # one saved with pickle.dump\n",
    "model_pkl = directory_model_data + 'best_model.pkl'  # one saved with joblib.dump\n",
    "model_p_pkl = directory_model_data + 'best_model_p.pkl' # one saved with pickle.dump\n",
    "result_pkl = directory_model_data + 'best_result.pkl'\n",
    "\n",
    "# load result data\n",
    "result = load_from_pickle(result_pkl) # with pickle.load\n",
    "\n",
    "yes_no = is_new_format(find_filename(directory_model_data))\n",
    "if yes_no:\n",
    "    precision = result.loc['test_precision'].iloc[-1]\n",
    "else:\n",
    "    precision = result.loc['precision'].iloc[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load columns data\n",
    "real_columns = load_from_pickle(columns_pkl)[:-5] # column 읽기. target columns 5개는 제외\n",
    "# scale the data\n",
    "scaler = joblib.load(scaler_pkl) # scaler 읽기\n",
    "# apply the scaled real_data to the model\n",
    "model = joblib.load(model_pkl) # model 읽기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_START_DATE = datetime.date(2022, 3, 2)\n",
    "TRAIN_END_DATE = datetime.date(2023, 3, 31)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = f'df_{com_name}_combine.pkl'\n",
    "f_name = directory_for_predict + fname\n",
    "df_o = pd.read_pickle(f_name) \n",
    "com_fname = f'{com_name}_historical.pkl'  # 실제와 예측을 비교하기 위하여 실제데이터을 불러 옴\n",
    "f_com_name = directory_for_data + com_fname\n",
    "com_data = pd.read_pickle(f_com_name)\n",
    "com_data['date'] = com_data['date'].dt.date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_data = df_o.loc[:, 'retail_1':'weekday'] # select columns except targets columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "next_date = find_next_date(TRAIN_END_DATE) # 최초 시작날짜 설정 (훈련데이터 이후 첫째 날짜)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "st_id = list(OPENING_DAYS_KOR).index(next_date) # 훈련데이터 이후 첫째 날짜 index == text data 첫째 날짜\n",
    "end_id = list(OPENING_DAYS_KOR).index(df_o.index[-1]) # test data 마지막 날짜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_result_df(start_id, end_id):\n",
    "    predict_value = {}\n",
    "    p0 = []\n",
    "    p1 = []\n",
    "    p2 = []\n",
    "    p3 = []\n",
    "    p4 = []\n",
    "    p5 = []\n",
    "    p6 = []\n",
    "    p7 = []\n",
    "    p8 = []\n",
    "    p9 = []\n",
    "    p_date = []\n",
    "\n",
    "    n_count = 0 # 총 갯수\n",
    "    np_count = 0\n",
    "    # prediction_date = next_date\n",
    "    for prediction_date in OPENING_DAYS_KOR[st_id:end_id]:\n",
    "        prediction_row = current_data[current_data.index == prediction_date]\n",
    "        if(len(prediction_row) == 0):\n",
    "            print(\"**** No data on\", prediction_date, \"*****\")\n",
    "            continue\n",
    "    #     print(prediction_row.iloc[:, 0:5])\n",
    "        n_count = n_count + 1\n",
    "        com_row = com_data[com_data['date']  == prediction_date]\n",
    "\n",
    "        # predict on,off low price, high price\n",
    "        real_data_df = prediction_row[real_columns] # select necessary columns\n",
    "        real_scaled = scaler.transform(real_data_df)\n",
    "\n",
    "        y_predict = model.predict(real_scaled)\n",
    "        weight = model.predict_proba(real_scaled)\n",
    "\n",
    "        if y_predict[0] == 1:\n",
    "            np_count = np_count + 1\n",
    "    #         print('prediction data: ', prediction_date, 'prediction result :', y_predict[0])\n",
    "            ratio = com_row['close_cr'].iloc[0]\n",
    "            if ratio >= 0.5:\n",
    "                t_f = True\n",
    "            else:\n",
    "                t_f = False\n",
    "\n",
    "            p0.append(com_name)\n",
    "            p1.append(y_predict[0])\n",
    "            p2.append(t_f)\n",
    "            p3.append(ratio)\n",
    "            p_value = com_row['close'].iloc[0] / (1 + ratio/100.)\n",
    "            p4.append(f'{p_value:.1f}')\n",
    "            p_c_value = p_value * (1 - 0.005) #  구매 기준: 0.005% 이하로 떨어지면 구매 가능\n",
    "            p5.append(f'{p_c_value:.1f}') # 0.5% 적용\n",
    "            p6.append(com_row['low'].iloc[0])\n",
    "            p7.append(f\"{(p_c_value - com_row['low'].iloc[0]):.1f}\")\n",
    "            if p_c_value >= com_row['low'].iloc[0] :\n",
    "                p8_yesno = 'yes'\n",
    "            else:\n",
    "                p8_yesno = 'no'\n",
    "            p8.append(p8_yesno)\n",
    "            if (p8_yesno == 'yes') & t_f :\n",
    "                final = 'yes'\n",
    "            else:\n",
    "                final = 'no'\n",
    "            p9.append(final)\n",
    "            p_date.append(prediction_date)\n",
    "\n",
    "    predict_value['date'] = p_date\n",
    "    predict_value['company'] = p0\n",
    "    predict_value['prediction'] = p1\n",
    "    predict_value['result'] = p2\n",
    "    predict_value['cr_ratio'] = p3\n",
    "    predict_value['p_close'] = p4\n",
    "    predict_value['p_c_05%'] = p5\n",
    "    predict_value['low'] = p6\n",
    "    predict_value['diff'] = p7\n",
    "    predict_value['chance_to_buy'] = p8\n",
    "    predict_value['final'] = p9  # 최종 취득 가능 판단\n",
    "\n",
    "    pre_ratio = np_count / n_count\n",
    "    print(f\"Count total: {n_count}, count_yes: {np_count}, ratio: {pre_ratio:.2f}\")\n",
    "    result_df = pd.DataFrame(predict_value)\n",
    "    \n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**** No data on 2023-04-10 *****\n",
      "Count total: 50, count_yes: 11, ratio: 0.22\n"
     ]
    }
   ],
   "source": [
    "dff = calc_result_df(st_id, end_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>company</th>\n",
       "      <th>prediction</th>\n",
       "      <th>result</th>\n",
       "      <th>cr_ratio</th>\n",
       "      <th>p_close</th>\n",
       "      <th>p_c_05%</th>\n",
       "      <th>low</th>\n",
       "      <th>diff</th>\n",
       "      <th>chance_to_buy</th>\n",
       "      <th>final</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2023-04-18</td>\n",
       "      <td>mobis</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>-1.32</td>\n",
       "      <td>228009.7</td>\n",
       "      <td>226869.7</td>\n",
       "      <td>223000.0</td>\n",
       "      <td>3869.7</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2023-04-19</td>\n",
       "      <td>mobis</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>0.89</td>\n",
       "      <td>224997.5</td>\n",
       "      <td>223872.5</td>\n",
       "      <td>225000.0</td>\n",
       "      <td>-1127.5</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2023-04-28</td>\n",
       "      <td>mobis</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>-1.81</td>\n",
       "      <td>221509.3</td>\n",
       "      <td>220401.8</td>\n",
       "      <td>216500.0</td>\n",
       "      <td>3901.8</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2023-05-02</td>\n",
       "      <td>mobis</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>2.53</td>\n",
       "      <td>217497.3</td>\n",
       "      <td>216409.8</td>\n",
       "      <td>218000.0</td>\n",
       "      <td>-1590.2</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2023-05-08</td>\n",
       "      <td>mobis</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>1.61</td>\n",
       "      <td>217990.4</td>\n",
       "      <td>216900.4</td>\n",
       "      <td>219000.0</td>\n",
       "      <td>-2099.6</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2023-05-16</td>\n",
       "      <td>mobis</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>-2.00</td>\n",
       "      <td>225000.0</td>\n",
       "      <td>223875.0</td>\n",
       "      <td>219000.0</td>\n",
       "      <td>4875.0</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2023-06-02</td>\n",
       "      <td>mobis</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>0.68</td>\n",
       "      <td>221493.8</td>\n",
       "      <td>220386.4</td>\n",
       "      <td>220000.0</td>\n",
       "      <td>386.4</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2023-06-08</td>\n",
       "      <td>mobis</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>0.23</td>\n",
       "      <td>219495.2</td>\n",
       "      <td>218397.7</td>\n",
       "      <td>218000.0</td>\n",
       "      <td>397.7</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2023-06-13</td>\n",
       "      <td>mobis</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>0.45</td>\n",
       "      <td>222001.0</td>\n",
       "      <td>220891.0</td>\n",
       "      <td>220500.0</td>\n",
       "      <td>391.0</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2023-06-14</td>\n",
       "      <td>mobis</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>-0.22</td>\n",
       "      <td>222990.6</td>\n",
       "      <td>221875.6</td>\n",
       "      <td>222000.0</td>\n",
       "      <td>-124.4</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2023-06-16</td>\n",
       "      <td>mobis</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>-0.67</td>\n",
       "      <td>222994.1</td>\n",
       "      <td>221879.1</td>\n",
       "      <td>220500.0</td>\n",
       "      <td>1379.1</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          date company  prediction  result  cr_ratio   p_close   p_c_05%   \n",
       "0   2023-04-18   mobis           1   False     -1.32  228009.7  226869.7  \\\n",
       "1   2023-04-19   mobis           1    True      0.89  224997.5  223872.5   \n",
       "2   2023-04-28   mobis           1   False     -1.81  221509.3  220401.8   \n",
       "3   2023-05-02   mobis           1    True      2.53  217497.3  216409.8   \n",
       "4   2023-05-08   mobis           1    True      1.61  217990.4  216900.4   \n",
       "5   2023-05-16   mobis           1   False     -2.00  225000.0  223875.0   \n",
       "6   2023-06-02   mobis           1    True      0.68  221493.8  220386.4   \n",
       "7   2023-06-08   mobis           1   False      0.23  219495.2  218397.7   \n",
       "8   2023-06-13   mobis           1   False      0.45  222001.0  220891.0   \n",
       "9   2023-06-14   mobis           1   False     -0.22  222990.6  221875.6   \n",
       "10  2023-06-16   mobis           1   False     -0.67  222994.1  221879.1   \n",
       "\n",
       "         low     diff chance_to_buy final  \n",
       "0   223000.0   3869.7           yes    no  \n",
       "1   225000.0  -1127.5            no    no  \n",
       "2   216500.0   3901.8           yes    no  \n",
       "3   218000.0  -1590.2            no    no  \n",
       "4   219000.0  -2099.6            no    no  \n",
       "5   219000.0   4875.0           yes    no  \n",
       "6   220000.0    386.4           yes   yes  \n",
       "7   218000.0    397.7           yes    no  \n",
       "8   220500.0    391.0           yes    no  \n",
       "9   222000.0   -124.4            no    no  \n",
       "10  220500.0   1379.1           yes    no  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_value = {}\n",
    "p0 = []\n",
    "p1 = []\n",
    "p2 = []\n",
    "p3 = []\n",
    "p4 = []\n",
    "p5 = []\n",
    "p6 = []\n",
    "p7 = []\n",
    "p8 = []\n",
    "p9 = []\n",
    "\n",
    "n_count = 0 # 총 갯수\n",
    "np_count = 0\n",
    "# prediction_date = next_date\n",
    "for prediction_date in OPENING_DAYS_KOR[st_id:end_id]:\n",
    "    prediction_row = current_data[current_data.index == prediction_date]\n",
    "    if(len(prediction_row) == 0):\n",
    "        print(\"**** No data on\", prediction_date, \"*****\")\n",
    "        continue\n",
    "#     print(prediction_row.iloc[:, 0:5])\n",
    "    n_count = n_count + 1\n",
    "    com_row = com_data[com_data['date']  == prediction_date]\n",
    "    \n",
    "    # predict on,off low price, high price\n",
    "    real_data_df = prediction_row[real_columns] # select necessary columns\n",
    "    real_scaled = scaler.transform(real_data_df)\n",
    "\n",
    "    y_predict = model.predict(real_scaled)\n",
    "    weight = model.predict_proba(real_scaled)\n",
    "    \n",
    "    if y_predict[0] == 1:\n",
    "        np_count = np_count + 1\n",
    "#         print('prediction data: ', prediction_date, 'prediction result :', y_predict[0])\n",
    "        ratio = com_row['close_cr'].iloc[0]\n",
    "        if ratio >= 0.5:\n",
    "            t_f = True\n",
    "        else:\n",
    "            t_f = False\n",
    "        \n",
    "        p0.append(com_name)\n",
    "        p1.append(y_predict[0])\n",
    "        p2.append(t_f)\n",
    "        p3.append(ratio)\n",
    "        p_value = com_row['close'].iloc[0] / (1 + ratio/100.)\n",
    "        p4.append(f'{p_value:.1f}')\n",
    "        p_c_value = p_value * (1 - 0.005) #  구매 기준: 0.005% 이하로 떨어지면 구매 가능\n",
    "        p5.append(f'{p_c_value:.1f}') # 0.5% 적용\n",
    "        p6.append(com_row['low'].iloc[0])\n",
    "        p7.append(f\"{(p_c_value - com_row['low'].iloc[0]):.1f}\")\n",
    "        if p_c_value >= com_row['low'].iloc[0] :\n",
    "            p8_yesno = 'yes'\n",
    "        else:\n",
    "            p8_yesno = 'no'\n",
    "        p8.append(p8_yesno)\n",
    "        if (p8_yesno == 'yes') & t_f :\n",
    "            final = 'yes'\n",
    "        else:\n",
    "            final = 'no'\n",
    "        p9.append(final)\n",
    "            \n",
    "predict_value['company'] = p0\n",
    "predict_value['prediction'] = p1\n",
    "predict_value['result'] = p2\n",
    "predict_value['cr_ratio'] = p3\n",
    "predict_value['p_close'] = p4\n",
    "predict_value['p_c_05%'] = p5\n",
    "predict_value['low'] = p6\n",
    "predict_value['diff'] = p7\n",
    "predict_value['chance_to_buy'] = p8\n",
    "predict_value['final'] = p9  # 최종 취득 가능 판단\n",
    "\n",
    "pre_ratio = np_count / n_count\n",
    "print(f\"Count total: {n_count}, count_yes: {np_count}, ratio: {pre_ratio:.2f}\")\n",
    "result_df = pd.DataFrame(predict_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p0 = p1 = p2 = p3 = p4 = p5 = p6 = p7 = p8 = p9 = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(predict_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "com_row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_data = df_o.loc[:, 'retail_1':'weekday'] # select columns except targets columns\n",
    "\n",
    "prediction_row = current_data[current_data.index == prediction_date]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_o.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OPENING_DAYS_KOR[802:852]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "5-1 결정 트리.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
