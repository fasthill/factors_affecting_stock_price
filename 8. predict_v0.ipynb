{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/fasthill/ML-DL-study-alone/blob/main/5-1%20%EA%B2%B0%EC%A0%95%20%ED%8A%B8%EB%A6%AC.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5uA_6TRHEMHV"
   },
   "source": [
    "## Testing with real world data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "26KAIfzEEMHc"
   },
   "source": [
    "<table align=\"left\">\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/rickiepark/hg-mldl/blob/master/5-1.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />구글 코랩에서 실행하기</a>\n",
    "  </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "rIXeXBVTfZrS"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from functools import reduce\n",
    "import datetime\n",
    "import os, re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "import xgboost\n",
    "from lightgbm import LGBMClassifier\n",
    "import lightgbm\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression, SGDClassifier, SGDRegressor\n",
    "from sklearn.model_selection import cross_validate, train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten, Dropout\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# write list, dictionary to pickle\n",
    "def save_to_pickle(path, filename):\n",
    "    open_file = open(path, \"wb\")\n",
    "    pickle.dump(filename, open_file)\n",
    "    open_file.close()\n",
    "\n",
    "# read list, dictionary from pickle\n",
    "def load_from_pickle(path):\n",
    "    open_file = open(path, \"rb\")\n",
    "    loaded_file = pickle.load(open_file)\n",
    "    open_file.close()\n",
    "    return loaded_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "# write list, dictionary to csv\n",
    "# path = './xxx/', my_dict = filename\n",
    "\n",
    "def save_dict_to_csv(path, my_dict):\n",
    "    df = pd.DataFrame.from_dict(my_dict, orient='index') \n",
    "    df.to_csv (path, index=False, header=True)  \n",
    "    \n",
    "def save_list_to_csv(path, my_list):\n",
    "    df = pd.DataFrame(my_list, columns=['columns'])\n",
    "    df.to_csv (path, index=False, header=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_df(date, com_name, precision, y_predict, weight):\n",
    "    dict_temp = {}\n",
    "    dict_temp['date'] = date\n",
    "    dict_temp[f'{com_name}_precision'] = f'{precision:.2f}'\n",
    "    dict_temp[f'{com_name}_predict'] = f'{y_predict[0]}'\n",
    "    dict_temp[f'{com_name}_yes'] = f'{weight[0,1]:.2f}'\n",
    "    dict_temp[f'{com_name}_no'] = f'{weight[0,0]:.2f}'\n",
    "    df_t = pd.DataFrame.from_dict(dict_temp, orient='index').T\n",
    "    df_t.set_index('date', inplace=True)\n",
    "    return df_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_df_todays(date, com_name, result, y_predict, weight, cr):\n",
    "    precision = result.loc['precision'].iloc[-1]\n",
    "    tn = result.loc['tn'].iloc[-1]\n",
    "    fp = result.loc['fp'].iloc[-1]\n",
    "    fn = result.loc['fn'].iloc[-1]\n",
    "    tp = result.loc['tp'].iloc[-1]\n",
    "    \n",
    "    dict_temp = {}\n",
    "    dict_temp['name'] = com_name\n",
    "    dict_temp[f'precision'] = f'{precision:.2f}'\n",
    "    dict_temp[f'predict'] = f'{y_predict[0]}'\n",
    "    dict_temp[f'yes'] = f'{weight[0,1]:.2f}'\n",
    "    dict_temp[f'no'] = f'{weight[0,0]:.2f}'\n",
    "    dict_temp[f'tn'] = f'{tn:.1f}'\n",
    "    dict_temp[f'fp'] = f'{fp:.1f}'\n",
    "    dict_temp[f'fn'] = f'{fn:.1f}'\n",
    "    dict_temp[f'tp'] = f'{tp:.1f}'\n",
    "    if ((y_predict[0] == 1) & (cr > 0)):\n",
    "        result = 'right'\n",
    "    elif ((y_predict[0] == 1) & (cr <= 0)):\n",
    "        result = 'wrong'\n",
    "    else:\n",
    "        result = 'draw'\n",
    "    dict_temp[f'result'] = result\n",
    "    \n",
    "    df_t = pd.DataFrame.from_dict(dict_temp, orient='index').T\n",
    "    df_t.set_index('name', inplace=True)\n",
    "    return df_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "code = {'005930' : ['삼성전자', 'sec'], '373220' : ['LG에너지솔루션', 'lgenergy'], \n",
    "        '000660' : ['SK하이닉스', 'skhynix'], '207940' : ['삼성바이오로직스', 'ssbio'],\n",
    "        '006400' : ['삼성SDI', 'sdi'], '051910' : ['LG화학', 'lgchemical'],\n",
    "        '005935' : ['삼성전자우', 'secpre'], '005380' : ['현대차', 'hyunmotor'],\n",
    "        '035420' : ['NAVER', 'naver'], '000270' : ['기아','kia'],\n",
    "        '035720' : ['카카오', 'kakao'], '005490' : ['POSCO홀딩스', 'poscoholding'],\n",
    "        '105560' : ['KB금융', 'kbbank'], '028260' : ['삼성물산', 'sscnt'],\n",
    "        '068270' : ['셀트리온', 'celltrion'], '012330' : ['현대모비스', 'mobis'],\n",
    "        '055550' : ['신한지주', 'shgroup'], '066570' : ['LG전자', 'lgelec'],\n",
    "        '003670' : ['포스코퓨처엠', 'poscochemical'], '096770' : ['SK이노베이션', 'skinnovation'],\n",
    "        '033780' : ['KT&G', 'ktng'], '030200' : ['KT', 'kt']}\n",
    "\n",
    "code_good = {'005930' : ['삼성전자', 'sec'], '035420' : ['NAVER', 'naver'],\n",
    "             '035720' : ['카카오', 'kakao'], '012330' : ['현대모비스', 'mobis'],\n",
    "             '051910' : ['LG화학', 'lgchemical'], '005935' : ['삼성전자우', 'secpre'],\n",
    "             '373220' : ['LG에너지솔루션', 'lgenergy'],\n",
    "            }\n",
    "\n",
    "code_good = {'005930' : ['삼성전자', 'sec'], '035420' : ['NAVER', 'naver'],\n",
    "             '035720' : ['카카오', 'kakao'], '012330' : ['현대모비스', 'mobis'],\n",
    "             '051910' : ['LG화학', 'lgchemical'],'005935' : ['삼성전자우', 'secpre'],\n",
    "             '000270' : ['기아','kia'], '373220' : ['LG에너지솔루션', 'lgenergy'],\n",
    "             '005380' : ['현대차', 'hyunmotor'], '000660' : ['SK하이닉스', 'skhynix'],\n",
    "             '006400' : ['삼성SDI', 'sdi'],\n",
    "            }\n",
    "\n",
    "code_mid = {'105560' : ['KB금융', 'kbbank'],\n",
    "            '003670' : ['포스코퓨처엠', 'poscochemical'],\n",
    "            }\n",
    "\n",
    "code_bad = { '030200' : ['KT', 'kt'],\n",
    "             '033780' : ['KT&G', 'ktng'], '066570' : ['LG전자', 'lgelec'], \n",
    "             '005490' : ['POSCO홀딩스', 'poscoholding'], '055550' : ['신한지주', 'shgroup'],\n",
    "             '096770' : ['SK이노베이션', 'skinnovation'],\n",
    "             '207940' : ['삼성바이오로직스', 'ssbio'], '028260' : ['삼성물산', 'sscnt'],\n",
    "             '068270' : ['셀트리온', 'celltrion'],\n",
    "           }\n",
    "code_good = {'005930' : ['삼성전자', 'sec'], '035420' : ['NAVER', 'naver'],\n",
    "#             '000660' : ['SK하이닉스', 'skhynix'],\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory_for_predict = './data/data_for_ml/predict/'\n",
    "directory_for_data = './data/company_pkl/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(directory_for_predict+ 'prediction/prediction_list.pkl'):\n",
    "    os.makedirs(directory_for_predict+'prediction')\n",
    "    prediction_list=pd.DataFrame()\n",
    "    fname_p = 'prediction_list.pkl'\n",
    "    path_p = directory_for_predict+'prediction/' + fname_p\n",
    "    prediction_list.to_pickle(path_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_list = pd.read_pickle(directory_for_predict+ 'prediction/prediction_list.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_date = datetime.date.today()\n",
    "# prediction_date = datetime.date(2023, 4, 24) # 예측을 필요로 하는 일자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found array with 0 sample(s) (shape=(0, 95)) while a minimum of 1 is required by StandardScaler.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [13], line 50\u001b[0m\n\u001b[0;32m     48\u001b[0m     scaler \u001b[38;5;241m=\u001b[39m joblib\u001b[38;5;241m.\u001b[39mload(scaler_pkl) \u001b[38;5;66;03m# scaler 읽기\u001b[39;00m\n\u001b[0;32m     49\u001b[0m \u001b[38;5;66;03m#     scaler = load_from_pickle(scaler_p_pkl) # scaler 읽기\u001b[39;00m\n\u001b[1;32m---> 50\u001b[0m     real_scaled \u001b[38;5;241m=\u001b[39m \u001b[43mscaler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreal_data_df\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     52\u001b[0m     \u001b[38;5;66;03m# apply the scaled real_data to the model\u001b[39;00m\n\u001b[0;32m     53\u001b[0m     model \u001b[38;5;241m=\u001b[39m joblib\u001b[38;5;241m.\u001b[39mload(model_pkl) \u001b[38;5;66;03m# model 읽기\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\utils\\_set_output.py:140\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[1;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[0;32m    138\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[0;32m    139\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 140\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m f(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    141\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m    142\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[0;32m    143\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[0;32m    144\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[0;32m    145\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[0;32m    146\u001b[0m         )\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\preprocessing\\_data.py:992\u001b[0m, in \u001b[0;36mStandardScaler.transform\u001b[1;34m(self, X, copy)\u001b[0m\n\u001b[0;32m    989\u001b[0m check_is_fitted(\u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m    991\u001b[0m copy \u001b[38;5;241m=\u001b[39m copy \u001b[38;5;28;01mif\u001b[39;00m copy \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy\n\u001b[1;32m--> 992\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    993\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    994\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    995\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcsr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    996\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    997\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mFLOAT_DTYPES\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    998\u001b[0m \u001b[43m    \u001b[49m\u001b[43mforce_all_finite\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mallow-nan\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    999\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1001\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sparse\u001b[38;5;241m.\u001b[39missparse(X):\n\u001b[0;32m   1002\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwith_mean:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\base.py:565\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[1;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[0;32m    563\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mValidation should be done on X, y or both.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    564\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m no_val_y:\n\u001b[1;32m--> 565\u001b[0m     X \u001b[38;5;241m=\u001b[39m check_array(X, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_params)\n\u001b[0;32m    566\u001b[0m     out \u001b[38;5;241m=\u001b[39m X\n\u001b[0;32m    567\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_y:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\utils\\validation.py:931\u001b[0m, in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[0;32m    929\u001b[0m     n_samples \u001b[38;5;241m=\u001b[39m _num_samples(array)\n\u001b[0;32m    930\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m n_samples \u001b[38;5;241m<\u001b[39m ensure_min_samples:\n\u001b[1;32m--> 931\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    932\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound array with \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m sample(s) (shape=\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m) while a\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    933\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m minimum of \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m is required\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    934\u001b[0m             \u001b[38;5;241m%\u001b[39m (n_samples, array\u001b[38;5;241m.\u001b[39mshape, ensure_min_samples, context)\n\u001b[0;32m    935\u001b[0m         )\n\u001b[0;32m    937\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ensure_min_features \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m array\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[0;32m    938\u001b[0m     n_features \u001b[38;5;241m=\u001b[39m array\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\n",
      "\u001b[1;31mValueError\u001b[0m: Found array with 0 sample(s) (shape=(0, 95)) while a minimum of 1 is required by StandardScaler."
     ]
    }
   ],
   "source": [
    "df_base = pd.DataFrame()\n",
    "df_todays = pd.DataFrame()\n",
    "\n",
    "for key, val in code_good.items():\n",
    " \n",
    "    com_name = val[1]\n",
    "    \n",
    "    fname = f'df_{com_name}_combine.pkl'\n",
    "    f_name = directory_for_predict + fname\n",
    "    df_o = pd.read_pickle(f_name) \n",
    "    com_fname = f'{com_name}_historical.pkl'  # 실제와 예측을 비교하기 위하여 실제데이터을 불러 옴\n",
    "    f_com_name = directory_for_data + com_fname\n",
    "    com_data = pd.read_pickle(f_com_name)\n",
    "\n",
    "    current_data = df_o.loc[:, 'retail_1':'weekday'] # select columns except targets columns\n",
    "    \n",
    "    prediction_row = current_data[current_data.index == prediction_date]\n",
    "    \n",
    "    com_row = com_data[com_data['date'].apply(lambda x: x.date())  == prediction_date]\n",
    "#     com_data['date'].apply(lambda x: x.date()) <  prediction_date\n",
    "    try:\n",
    "        cr = com_row['close_cr'].values[0] # 실제의 등락을 확인\n",
    "    except:\n",
    "        cr = -1.0 # 예측 당일 아침 실제 결과가 없을시 임시 지정\n",
    "        \n",
    "        #**************************************************************\n",
    "\n",
    "    # locate the model data directory\n",
    "    directory_model_data = f'./machine_learning/{com_name}/'\n",
    "\n",
    "    # get the model data filepath\n",
    "    columns_pkl = directory_model_data + 'best_columns.pkl'\n",
    "    scaler_pkl = directory_model_data + 'best_scaler.pkl'\n",
    "    scaler_p_pkl = directory_model_data + 'best_scaler_p.pkl'\n",
    "    model_pkl = directory_model_data + 'best_model.pkl'\n",
    "    model_p_pkl = directory_model_data + 'best_model_p.pkl'\n",
    "    result_pkl = directory_model_data + 'best_result.pkl'\n",
    "    \n",
    "    # load result data\n",
    "    result = load_from_pickle(result_pkl)[:-5] \n",
    "    precision = result.loc['precision'].iloc[-1]\n",
    "    \n",
    "    # load columns data\n",
    "    real_columns = load_from_pickle(columns_pkl)[:-5] # column 읽기. target columns 5개는 제외\n",
    "    real_data_df = prediction_row[real_columns] # select necessary columns\n",
    "    \n",
    "    # scale the data\n",
    "    scaler = joblib.load(scaler_pkl) # scaler 읽기\n",
    "#     scaler = load_from_pickle(scaler_p_pkl) # scaler 읽기\n",
    "    real_scaled = scaler.transform(real_data_df)\n",
    "    \n",
    "    # apply the scaled real_data to the model\n",
    "    model = joblib.load(model_pkl) # model 읽기\n",
    "#     model = load_from_pickle(model_p_pkl) # model made with pickle 읽기\n",
    "\n",
    "    y_predict = model.predict(real_scaled)\n",
    "    weight = model.predict_proba(real_scaled)\n",
    "\n",
    "    df_temp = to_df(prediction_row.index[-1], com_name, precision, y_predict, weight)\n",
    "    df_base = pd.concat([df_base, df_temp],axis=1)\n",
    "    df_temp_todays = to_df_todays(prediction_row.index[-1], com_name, result, y_predict, weight, cr)\n",
    "    df_todays = pd.concat([df_todays, df_temp_todays],axis=0)\n",
    "    \n",
    "#     print(f'**date: {prediction_row.index[-1].date()}, {precision:.2f}, {com_name}, 예측: {y_predict}, 가능성:{weight}')\n",
    "df_todays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>retail_1</th>\n",
       "      <th>foreigner_1</th>\n",
       "      <th>institution_1</th>\n",
       "      <th>financial_1</th>\n",
       "      <th>invtrust_1</th>\n",
       "      <th>pension_1</th>\n",
       "      <th>privequity_1</th>\n",
       "      <th>insurance_1</th>\n",
       "      <th>corporateetc_1</th>\n",
       "      <th>foreigneretc_1</th>\n",
       "      <th>...</th>\n",
       "      <th>ixtr_cr_2</th>\n",
       "      <th>ixut_cr_2</th>\n",
       "      <th>nbi_cr_2</th>\n",
       "      <th>bkx_cr_2</th>\n",
       "      <th>open_2</th>\n",
       "      <th>high_2</th>\n",
       "      <th>low_2</th>\n",
       "      <th>close_2</th>\n",
       "      <th>vol_2</th>\n",
       "      <th>weekday</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>0 rows × 95 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [retail_1, foreigner_1, institution_1, financial_1, invtrust_1, pension_1, privequity_1, insurance_1, corporateetc_1, foreigneretc_1, dji_cr, bond_kor_10_cr, bond_kor_2_cr, kosdaq_cr, kospi_cr, krw_cr, ixic_cr, sox_cr, spx_cr, bond_usa_10_cr, bond_usa_2_cr, bond_usa_3m_cr, vix_cr, spsy_cr, spny_cr, spxhc_cr, splrcd_cr, splrci_cr, splrcu_cr, splrcs_cr, splrct_cr, splrcl_cr, splrcm_cr, ixbk_cr, ixfn_cr, ixid_cr, ixis_cr, ixk_cr, ixtr_cr, ixut_cr, nbi_cr, bkx_cr, open_1, high_1, low_1, close_1, vol_1, retail_2, foreigner_2, institution_2, financial_2, invtrust_2, pension_2, privequity_2, insurance_2, corporateetc_2, foreigneretc_2, dji_cr_2, bond_kor_10_cr_2, bond_kor_2_cr_2, kosdaq_cr_2, kospi_cr_2, krw_cr_2, ixic_cr_2, sox_cr_2, spx_cr_2, bond_usa_10_cr_2, bond_usa_2_cr_2, bond_usa_3m_cr_2, vix_cr_2, spsy_cr_2, spny_cr_2, spxhc_cr_2, splrcd_cr_2, splrci_cr_2, splrcu_cr_2, splrcs_cr_2, splrct_cr_2, splrcl_cr_2, splrcm_cr_2, ixbk_cr_2, ixfn_cr_2, ixid_cr_2, ixis_cr_2, ixk_cr_2, ixtr_cr_2, ixut_cr_2, nbi_cr_2, bkx_cr_2, open_2, high_2, low_2, close_2, vol_2, weekday]\n",
       "Index: []\n",
       "\n",
       "[0 rows x 95 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "real_data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# save current prediction data\n",
    "\n",
    "prediction_list = pd.concat([prediction_list, df_base], axis=0)\n",
    "prediction_list = prediction_list[~prediction_list.index.duplicated(keep='last')]\n",
    "\n",
    "directory_for_predict = './data/data_for_ml/predict/'\n",
    "fname_p = 'prediction_list.pkl'\n",
    "fname_c = 'prediction_list.csv'\n",
    "path_p = directory_for_predict+'prediction/' + fname_p\n",
    "path_c = directory_for_predict+'prediction/' + fname_c\n",
    "prediction_list.to_pickle(path_p)\n",
    "prediction_list.to_csv(path_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** 0 005930 ['삼성전자', 'sec']\n",
      "*** 1 035420 ['NAVER', 'naver']\n"
     ]
    }
   ],
   "source": [
    "# 결과를 회사별로 확인하기\n",
    "directory_for_predict = './data/data_for_ml/predict/'\n",
    "fname_p = 'prediction_list.pkl'\n",
    "predict_list = pd.read_pickle(directory_for_predict+'prediction/' + fname_p)\n",
    "\n",
    "for i, (key, val) in enumerate(code_good.items()):\n",
    "    print(\"***\", i, key, val)\n",
    "    k =  i * 4\n",
    "    globals()[f'{val[1]}_df'] = predict_list.iloc[:, k:k+4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sec_precision</th>\n",
       "      <th>sec_predict</th>\n",
       "      <th>sec_yes</th>\n",
       "      <th>sec_no</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2023-04-03</th>\n",
       "      <td>1.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0.52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-04-04</th>\n",
       "      <td>1.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0.44</td>\n",
       "      <td>0.56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-04-05</th>\n",
       "      <td>1.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0.34</td>\n",
       "      <td>0.66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-04-06</th>\n",
       "      <td>1.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0.37</td>\n",
       "      <td>0.63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-04-07</th>\n",
       "      <td>1.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0.41</td>\n",
       "      <td>0.59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-04-24</th>\n",
       "      <td>0.87</td>\n",
       "      <td>0</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0.52</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           sec_precision sec_predict sec_yes sec_no\n",
       "date                                               \n",
       "2023-04-03          1.00           0    0.48   0.52\n",
       "2023-04-04          1.00           0    0.44   0.56\n",
       "2023-04-05          1.00           0    0.34   0.66\n",
       "2023-04-06          1.00           0    0.37   0.63\n",
       "2023-04-07          1.00           0    0.41   0.59\n",
       "2023-04-24          0.87           0    0.48   0.52"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sec_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "5-1 결정 트리.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
