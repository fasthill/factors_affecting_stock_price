{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/fasthill/ML-DL-study-alone/blob/main/5-1%20%EA%B2%B0%EC%A0%95%20%ED%8A%B8%EB%A6%AC.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5uA_6TRHEMHV"
   },
   "source": [
    "## Testing with real world data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "26KAIfzEEMHc"
   },
   "source": [
    "<table align=\"left\">\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/rickiepark/hg-mldl/blob/master/5-1.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />구글 코랩에서 실행하기</a>\n",
    "  </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "rIXeXBVTfZrS"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from functools import reduce\n",
    "import datetime\n",
    "import os, re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "import xgboost\n",
    "from lightgbm import LGBMClassifier\n",
    "import lightgbm\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression, SGDClassifier, SGDRegressor\n",
    "from sklearn.model_selection import cross_validate, train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten, Dropout\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# write list, dictionary to pickle\n",
    "def save_to_pickle(path, filename):\n",
    "    open_file = open(path, \"wb\")\n",
    "    pickle.dump(filename, open_file)\n",
    "    open_file.close()\n",
    "\n",
    "# read list, dictionary from pickle\n",
    "def load_from_pickle(path):\n",
    "    open_file = open(path, \"rb\")\n",
    "    loaded_file = pickle.load(open_file)\n",
    "    open_file.close()\n",
    "    return loaded_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "# write list, dictionary to csv\n",
    "# path = './xxx/', my_dict = filename\n",
    "\n",
    "def save_dict_to_csv(path, my_dict):\n",
    "    df = pd.DataFrame.from_dict(my_dict, orient='index') \n",
    "    df.to_csv (path, index=False, header=True)  \n",
    "    \n",
    "def save_list_to_csv(path, my_list):\n",
    "    df = pd.DataFrame(my_list, columns=['columns'])\n",
    "    df.to_csv (path, index=False, header=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_df(date, com_name, precision, y_predict, weight):\n",
    "    dict_temp = {}\n",
    "    dict_temp['date'] = date\n",
    "    dict_temp[f'{com_name}_precision'] = f'{precision:.2f}'\n",
    "    dict_temp[f'{com_name}_predict'] = f'{y_predict[0]}'\n",
    "    dict_temp[f'{com_name}_yes'] = f'{weight[0,0]:.2f}'\n",
    "    dict_temp[f'{com_name}_no'] = f'{weight[0,1]:.2f}'\n",
    "    df_t = pd.DataFrame.from_dict(dict_temp, orient='index').T\n",
    "    df_t.set_index('date', inplace=True)\n",
    "    return df_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "code = {'005930' : ['삼성전자', 'sec'], '373220' : ['LG에너지솔루션', 'lgenergy'], \n",
    "        '000660' : ['SK하이닉스', 'skhinix'], '207940' : ['삼성바이오로직스', 'ssbio'],\n",
    "        '006400' : ['삼성SDI', 'sdi'], '051910' : ['LG화학', 'lgchemical'],\n",
    "        '005935' : ['삼성전자우', 'secpre'], '005380' : ['현대차', 'hyunmotor'],\n",
    "        '035420' : ['NAVER', 'naver'], '000270' : ['기아','kia'],\n",
    "        '035720' : ['카카오', 'kakao'], '005490' : ['POSCO홀딩스', 'poscoholding'],\n",
    "        '105560' : ['KB금융', 'kbbank'], '028260' : ['삼성물산', 'sscnt'],\n",
    "        '068270' : ['셀트리온', 'celltrion'], '012330' : ['현대모비스', 'mobis'],\n",
    "        '055550' : ['신한지주', 'shgroup'], '066570' : ['LG전자', 'lgelec'],\n",
    "        '003670' : ['포스코케미칼', 'poscochemical'], '096770' : ['SK이노베이션', 'skinnovation'],\n",
    "        '033780' : ['KT&G', 'ktng'], '030200' : ['KT', 'kt']}\n",
    "\n",
    "code_good = {'005930' : ['삼성전자', 'sec'], '035420' : ['NAVER', 'naver'],\n",
    "             '035720' : ['카카오', 'kakao'], '012330' : ['현대모비스', 'mobis'],\n",
    "             '051910' : ['LG화학', 'lgchemical'], '005935' : ['삼성전자우', 'secpre'],\n",
    "             '373220' : ['LG에너지솔루션', 'lgenergy'],\n",
    "#              '005380' : ['현대차', 'hyunmotor'] # to bad code.\n",
    "            }\n",
    "\n",
    "code_good = {'005930' : ['삼성전자', 'sec'], \n",
    "            }\n",
    "\n",
    "code_mid = {'105560' : ['KB금융', 'kbbank'],\n",
    "            '003670' : ['포스코케미칼', 'poscochemical'], '006400' : ['삼성SDI', 'sdi'],\n",
    "            }\n",
    "\n",
    "code_bad = { '000270' : ['기아','kia'], '030200' : ['KT', 'kt'],\n",
    "             '033780' : ['KT&G', 'ktng'], '066570' : ['LG전자', 'lgelec'], \n",
    "             '005490' : ['POSCO홀딩스', 'poscoholding'], '055550' : ['신한지주', 'shgroup'],\n",
    "             '000660' : ['SK하이닉스', 'skhinix'], '096770' : ['SK이노베이션', 'skinnovation'],\n",
    "             '207940' : ['삼성바이오로직스', 'ssbio'], '028260' : ['삼성물산', 'sscnt'],\n",
    "             '068270' : ['셀트리온', 'celltrion'],\n",
    "           }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory_for_predict = '../data/data_for_ml/predict/'\n",
    "\n",
    "if not os.path.exists(directory_for_predict+ 'prediction/prediction_list.pkl'):\n",
    "    os.makedirs(directory_for_predict+'prediction')\n",
    "    prediction_list=pd.DataFrame()\n",
    "    fname_p = 'prediction_list.pkl'\n",
    "    path_p = directory_for_predict+'prediction/' + fname_p\n",
    "    prediction_list.to_pickle(path_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_list = pd.read_pickle(directory_for_predict+ 'prediction/prediction_list.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sec_precision</th>\n",
       "      <th>sec_predict</th>\n",
       "      <th>sec_yes</th>\n",
       "      <th>sec_no</th>\n",
       "      <th>naver_precision</th>\n",
       "      <th>naver_predict</th>\n",
       "      <th>naver_yes</th>\n",
       "      <th>naver_no</th>\n",
       "      <th>kakao_precision</th>\n",
       "      <th>kakao_predict</th>\n",
       "      <th>...</th>\n",
       "      <th>lgchemical_yes</th>\n",
       "      <th>lgchemical_no</th>\n",
       "      <th>secpre_precision</th>\n",
       "      <th>secpre_predict</th>\n",
       "      <th>secpre_yes</th>\n",
       "      <th>secpre_no</th>\n",
       "      <th>lgenergy_precision</th>\n",
       "      <th>lgenergy_predict</th>\n",
       "      <th>lgenergy_yes</th>\n",
       "      <th>lgenergy_no</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2023-04-03</th>\n",
       "      <td>0.93</td>\n",
       "      <td>0</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.48</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0.52</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.50</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0.62</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.89</td>\n",
       "      <td>1</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.51</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           sec_precision sec_predict sec_yes sec_no naver_precision  \\\n",
       "date                                                                  \n",
       "2023-04-03          0.93           0    0.52   0.48            1.00   \n",
       "\n",
       "           naver_predict naver_yes naver_no kakao_precision kakao_predict  \\\n",
       "date                                                                        \n",
       "2023-04-03             1      0.48     0.52            1.00             0   \n",
       "\n",
       "            ... lgchemical_yes lgchemical_no secpre_precision secpre_predict  \\\n",
       "date        ...                                                                \n",
       "2023-04-03  ...           0.50          0.50             1.00              0   \n",
       "\n",
       "           secpre_yes secpre_no lgenergy_precision lgenergy_predict  \\\n",
       "date                                                                  \n",
       "2023-04-03       0.62      0.38               0.89                1   \n",
       "\n",
       "           lgenergy_yes lgenergy_no  \n",
       "date                                 \n",
       "2023-04-03         0.49        0.51  \n",
       "\n",
       "[1 rows x 28 columns]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction_list.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****** ../machine_learning/sec/estimator_bs_0404_0930_v1.pkl\n",
      "**date: 2023-04-04, 0.93, sec, 예측: [0], 가능성:[[0.5349483 0.4650517]]\n"
     ]
    }
   ],
   "source": [
    "for key, val in code_good.items():\n",
    " \n",
    "    com_name = val[1]\n",
    "    \n",
    "    fname = f'df_{com_name}_sel.pkl'\n",
    "    f_name = directory_for_predict + fname\n",
    "    df_o = pd.read_pickle(f_name) \n",
    "\n",
    "    current_data = df_o.loc[:, 'retail_1':'weekday'] # select columns except targets columns\n",
    "\n",
    "    last_row = current_data.iloc[[-1]] # get the last row data == previous date data\n",
    "    \n",
    "    # locate the model data directory\n",
    "    directory_model_data = f'../machine_learning/{com_name}/'\n",
    "\n",
    "    # get the model data filepath\n",
    "    columns_pkl = directory_model_data + 'best_columns.pkl'\n",
    "    scaler_pkl = directory_model_data + 'best_scaler.pkl'\n",
    "    scaler_pkl = directory_model_data + 'scaler_bs_0404_0930_v1.pkl'\n",
    "    estimator_pkl = directory_model_data + 'best_estimator.pkl'\n",
    "    estimator_pkl = directory_model_data + 'estimator_bs_0404_0930_v1.pkl'\n",
    "    result_pkl = directory_model_data + 'best_result.pkl'\n",
    "    \n",
    "    # load result data\n",
    "    result = load_from_pickle(result_pkl)[:-5] \n",
    "    precision = result.loc['precision'].iloc[-1]\n",
    "    \n",
    "    # load columns data\n",
    "    real_columns = load_from_pickle(columns_pkl)[:-5] # column 읽기. target columns 5개는 제외\n",
    "    real_data_df = last_row[real_columns] # select necessary columns\n",
    "    \n",
    "    # scale the data\n",
    "#     scaler = joblib.load(scaler_pkl) # scaler 읽기\n",
    "    scaler = load_from_pickle(scaler_pkl) # scaler 읽기\n",
    "    real_scaled = scaler.transform(real_data_df)\n",
    "    \n",
    "    # apply the scaled real_data to the model\n",
    "    print(\"******\", estimator_pkl)\n",
    "#     estimator = joblib.load(estimator_pkl) # model 읽기\n",
    "    estimator = load_from_pickle(estimator_pkl) # model 읽기\n",
    "    model = estimator.best_estimator_\n",
    "\n",
    "    y_predict = model.predict(real_scaled)\n",
    "    weight = model.predict_proba(real_scaled)\n",
    "\n",
    "    df_temp = to_df(last_row.index[-1].date(), com_name, precision, y_predict, weight)\n",
    "    df_base = pd.concat([df_base, df_temp],axis=1)\n",
    "    \n",
    "    print(f'**date: {last_row.index[-1].date()}, {precision:.2f}, {com_name}, 예측: {y_predict}, 가능성:{weight}')\n",
    "    \n",
    "# prediciton_list = pd.concat([prediction_list, df_base])\n",
    "# directory_for_predict = '../data/data_for_ml/predict/'\n",
    "# fname_p = '0_result.pkl'\n",
    "# fname_c = '0_result.csv'\n",
    "# path_p = directory_for_predict + fname_p\n",
    "# path_c = directory_for_predict + fname_c\n",
    "# df_base.to_pickle(path_p)\n",
    "# df_base.to_csv(path_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sec_precision</th>\n",
       "      <th>sec_predict</th>\n",
       "      <th>sec_yes</th>\n",
       "      <th>sec_no</th>\n",
       "      <th>naver_precision</th>\n",
       "      <th>naver_predict</th>\n",
       "      <th>naver_yes</th>\n",
       "      <th>naver_no</th>\n",
       "      <th>kakao_precision</th>\n",
       "      <th>kakao_predict</th>\n",
       "      <th>...</th>\n",
       "      <th>lgchemical_yes</th>\n",
       "      <th>lgchemical_no</th>\n",
       "      <th>secpre_precision</th>\n",
       "      <th>secpre_predict</th>\n",
       "      <th>secpre_yes</th>\n",
       "      <th>secpre_no</th>\n",
       "      <th>lgenergy_precision</th>\n",
       "      <th>lgenergy_predict</th>\n",
       "      <th>lgenergy_yes</th>\n",
       "      <th>lgenergy_no</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2023-04-03</th>\n",
       "      <td>0.93</td>\n",
       "      <td>0</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.48</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0.52</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.50</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0.62</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.89</td>\n",
       "      <td>1</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.51</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           sec_precision sec_predict sec_yes sec_no naver_precision  \\\n",
       "date                                                                  \n",
       "2023-04-03          0.93           0    0.52   0.48            1.00   \n",
       "\n",
       "           naver_predict naver_yes naver_no kakao_precision kakao_predict  \\\n",
       "date                                                                        \n",
       "2023-04-03             1      0.48     0.52            1.00             0   \n",
       "\n",
       "            ... lgchemical_yes lgchemical_no secpre_precision secpre_predict  \\\n",
       "date        ...                                                                \n",
       "2023-04-03  ...           0.50          0.50             1.00              0   \n",
       "\n",
       "           secpre_yes secpre_no lgenergy_precision lgenergy_predict  \\\n",
       "date                                                                  \n",
       "2023-04-03       0.62      0.38               0.89                1   \n",
       "\n",
       "           lgenergy_yes lgenergy_no  \n",
       "date                                 \n",
       "2023-04-03         0.49        0.51  \n",
       "\n",
       "[1 rows x 28 columns]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction_list.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "Can't get attribute 'rv_continuous_frozen' on <module 'scipy.stats._distn_infrastructure' from 'c:\\\\users\\\\user\\\\appdata\\\\local\\\\programs\\\\python\\\\python39\\\\lib\\\\site-packages\\\\scipy\\\\stats\\\\_distn_infrastructure.py'>",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[28], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m estimator \u001b[38;5;241m=\u001b[39m \u001b[43mload_from_pickle\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator_pkl\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[4], line 12\u001b[0m, in \u001b[0;36mload_from_pickle\u001b[1;34m(path)\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_from_pickle\u001b[39m(path):\n\u001b[0;32m     11\u001b[0m     open_file \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(path, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 12\u001b[0m     loaded_file \u001b[38;5;241m=\u001b[39m \u001b[43mpickle\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mopen_file\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     13\u001b[0m     open_file\u001b[38;5;241m.\u001b[39mclose()\n\u001b[0;32m     14\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loaded_file\n",
      "\u001b[1;31mAttributeError\u001b[0m: Can't get attribute 'rv_continuous_frozen' on <module 'scipy.stats._distn_infrastructure' from 'c:\\\\users\\\\user\\\\appdata\\\\local\\\\programs\\\\python\\\\python39\\\\lib\\\\site-packages\\\\scipy\\\\stats\\\\_distn_infrastructure.py'>"
     ]
    }
   ],
   "source": [
    "estimator = load_from_pickle(estimator_pkl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\user\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (2.0.0)\n",
      "Requirement already satisfied: numpy>=1.20.3 in c:\\users\\user\\appdata\\roaming\\python\\python39\\site-packages (from pandas) (1.24.2)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\user\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from pandas) (2022.7)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\user\\appdata\\roaming\\python\\python39\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\user\\appdata\\roaming\\python\\python39\\site-packages (from pandas) (2022.7.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\user\\appdata\\roaming\\python\\python39\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade --user pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./data/data_for_ml/predict/0_result.pkl'"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sec_precision</th>\n",
       "      <th>sec_predict</th>\n",
       "      <th>sec_yes</th>\n",
       "      <th>sec_no</th>\n",
       "      <th>naver_precision</th>\n",
       "      <th>naver_predict</th>\n",
       "      <th>naver_yes</th>\n",
       "      <th>naver_no</th>\n",
       "      <th>kakao_precision</th>\n",
       "      <th>kakao_predict</th>\n",
       "      <th>...</th>\n",
       "      <th>lgchemical_yes</th>\n",
       "      <th>lgchemical_no</th>\n",
       "      <th>secpre_precision</th>\n",
       "      <th>secpre_predict</th>\n",
       "      <th>secpre_yes</th>\n",
       "      <th>secpre_no</th>\n",
       "      <th>lgenergy_precision</th>\n",
       "      <th>lgenergy_predict</th>\n",
       "      <th>lgenergy_yes</th>\n",
       "      <th>lgenergy_no</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2023-04-03</th>\n",
       "      <td>0.93</td>\n",
       "      <td>0</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.48</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0.52</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.50</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0.62</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.89</td>\n",
       "      <td>1</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.51</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           sec_precision sec_predict sec_yes sec_no naver_precision  \\\n",
       "date                                                                  \n",
       "2023-04-03          0.93           0    0.52   0.48            1.00   \n",
       "\n",
       "           naver_predict naver_yes naver_no kakao_precision kakao_predict  \\\n",
       "date                                                                        \n",
       "2023-04-03             1      0.48     0.52            1.00             0   \n",
       "\n",
       "            ... lgchemical_yes lgchemical_no secpre_precision secpre_predict  \\\n",
       "date        ...                                                                \n",
       "2023-04-03  ...           0.50          0.50             1.00              0   \n",
       "\n",
       "           secpre_yes secpre_no lgenergy_precision lgenergy_predict  \\\n",
       "date                                                                  \n",
       "2023-04-03       0.62      0.38               0.89                1   \n",
       "\n",
       "           lgenergy_yes lgenergy_no  \n",
       "date                                 \n",
       "2023-04-03         0.49        0.51  \n",
       "\n",
       "[1 rows x 28 columns]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_t.index = [last_row.index[-1].date()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./data/data_for_ml/predict/0_result.pkl'"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_temp = to_df(last_row.index[-1].date(), com_name, precision, y_predict, weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lgenergy_precision</th>\n",
       "      <th>lgenergy_predict</th>\n",
       "      <th>lgenergy_yes</th>\n",
       "      <th>lgenergy_no</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2023-04-03</th>\n",
       "      <td>0.89</td>\n",
       "      <td>1</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.51</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           lgenergy_precision lgenergy_predict lgenergy_yes lgenergy_no\n",
       "date                                                                   \n",
       "2023-04-03               0.89                1         0.49        0.51"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_base = pd.concat([df_base, df_temp],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lgenergy_precision</th>\n",
       "      <th>lgenergy_predict</th>\n",
       "      <th>lgenergy_yes</th>\n",
       "      <th>lgenergy_no</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2023-04-03</th>\n",
       "      <td>0.89</td>\n",
       "      <td>1</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.51</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           lgenergy_precision lgenergy_predict lgenergy_yes lgenergy_no\n",
       "date                                                                   \n",
       "2023-04-03               0.89                1         0.49        0.51"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle['naver']['scaler'] = ['scaler_bs_0403_1409_v1.pkl' ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle['naver']['estimator'] = ['lgbm_bs_0403_1409_v1.pkl']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_data = {}\n",
    "pickle_code = {}\n",
    "\n",
    "model_data['columns'] = 'best_columns.pkl'\n",
    "model_data['scaler'] = 'best_scaler.pkl'\n",
    "model_data['estimator'] = 'best_estimator.pkl'\n",
    "\n",
    "pickle_code['naver'] = model_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# locate the model data directory\n",
    "com_name = list(code.values())[0][1]\n",
    "directory_model_data = f'../machine_learning/{com_name}/'\n",
    "\n",
    "# get the model data filepath\n",
    "columns_pkl = directory_model_data + 'columns_bs_0331_2228_101_100%_ver8.pkl'\n",
    "scaler_pkl = directory_model_data + 'scaler_bs_0331_2228_v8.pkl'\n",
    "model_pkl = directory_model_data + 'lgbm_bs_0331_2228_v8.pkl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load columns data\n",
    "real_columns = load_from_pickle(columns_pkl)[:-5] # column 읽기. target columns 5개는 제외\n",
    "real_data_df = real_data[real_columns] # select necessary columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scale the data\n",
    "scaler = joblib.load(scaler_pkl) # scaler 읽기\n",
    "real_scaled = scaler.transform(real_data_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply the scaled real_data to the model\n",
    "model_ = joblib.load(model_pkl) # model 읽기\n",
    "model = model_.best_estimator_\n",
    "\n",
    "y_predict = model.predict(real_scaled)\n",
    "weight = model.predict_proba(real_scaled)\n",
    "\n",
    "print(y_predict, weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_o.iloc[[-8]]['cr_00'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "code = {'005930' : ['삼성전자', 'sec'], '373220' : ['LG에너지솔루션', 'lgenergy'], \n",
    "        '000660' : ['SK하이닉스', 'skhinix'], '207940' : ['삼성바이오로직스', 'ssbio'],\n",
    "        '006400' : ['삼성SDI', 'sdi'], '051910' : ['LG화학', 'lgchemical'],\n",
    "        '005935' : ['삼성전자우', 'secpre'], '005380' : ['현대차', 'hyunmotor'],\n",
    "        '035420' : ['NAVER', 'naver'], '000270' : ['기아','kia'],\n",
    "        '035720' : ['카카오', 'kakao'], '005490' : ['POSCO홀딩스', 'poscoholding'],\n",
    "        '105560' : ['KB금융', 'kbbank'], '028260' : ['삼성물산', 'sscnt'],\n",
    "        '068270' : ['셀트리온', 'celltrion'], '012330' : ['현대모비스', 'mobis'],\n",
    "        '055550' : ['신한지주', 'shgroup'], '066570' : ['LG전자', 'lgelec'],\n",
    "        '003670' : ['포스코케미칼', 'poscochemical'], '096770' : ['SK이노베이션', 'skinnovation'],\n",
    "        '033780' : ['KT&G', 'ktng']}\n",
    "\n",
    "code = {'005930' : ['삼성전자', 'sec']}\n",
    "\n",
    "com_name = 'sec'\n",
    "\n",
    "directory_for_predict = '../data/data_for_ml/predict/'\n",
    "fname = f'df_{com_name}_sel.pkl'\n",
    "f_name = directory_for_predict + fname\n",
    "df_o = pd.read_pickle(f_name) \n",
    "\n",
    "df_read = df_o.loc[:, 'retail_1':'weekday'] # select columns except targets columns\n",
    "real_data = df_read.iloc[[-1]] # get the last row data == previous date data\n",
    "\n",
    "com_name = list(code.values())[0][1]\n",
    "\n",
    "directory_model_data = f'../machine_learning/{com_name}/'\n",
    "\n",
    "# get the model data filepath\n",
    "\n",
    "columns_pkl = directory_model_data + 'columns_0402_2349_60_93%_ver2.pkl'\n",
    "scaler_pkl = directory_model_data + 'scaler_0402_2349_v2.pkl'\n",
    "model_pkl = directory_model_data + 'lgbm_0402_2349_v2.pkl'\n",
    "\n",
    "real_columns = load_from_pickle(columns_pkl)[:-5] # column 읽기. target columns 5개는 제외\n",
    "real_data_df = real_data[real_columns] # select necessary columns\n",
    "\n",
    "scaler = joblib.load(scaler_pkl) # scaler 읽기\n",
    "real_scaled = scaler.transform(real_data_df)\n",
    "\n",
    "# apply the scaled real_data to the model\n",
    "model_ = joblib.load(model_pkl) # model 읽기\n",
    "model = model_.best_estimator_\n",
    "\n",
    "y_predict = model.predict(real_scaled)\n",
    "weight = model.predict_proba(real_scaled)\n",
    "\n",
    "print(y_predict, weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "com_name = 'sec'\n",
    "\n",
    "directory_for_ml = '../data/data_for_ml/expand_date/'\n",
    "fname = f'df_{com_name}_sel.pkl'\n",
    "f_name = directory_for_ml + fname\n",
    "df_o = pd.read_pickle(f_name) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_read = df_o.loc[:, 'retail_1':'weekday']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_f = df_read.iloc[[-1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_p(test_target, y_predict_list): \n",
    "    ps = precision_score(test_target, y_predict_list)\n",
    "    rs = recall_score(test_target, y_predict_list)\n",
    "    fs = f1_score(test_target, y_predict_list)\n",
    "    roc = roc_auc_score(test_target, y_predict_list)\n",
    "    collect_list = [ps, rs, fs, roc]\n",
    "    return collect_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_df_from_estimator(estimator, num):\n",
    "    df_t = pd.DataFrame.from_dict(estimator, orient='index')\n",
    "    df_t.columns = [f'value{num}']\n",
    "    df_t.index.name = 'parameter'\n",
    "    return df_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_results(model, train_scaled, val_scaled, test_scaled, train_target, val_target, test_target,\n",
    "                 test_scaled1, test_scaled2, test_scaled3, test_target1, test_target2, test_target3):\n",
    "# model = lgbmgs.best_estimator_  # 최적의 파라미터로 모델 생성\n",
    "    y_predict = model.predict(test_scaled)\n",
    "    result_dict= {}\n",
    "    result_dict['best_score'] = lgbmgs.best_score_ \n",
    "    result_dict['best_index'] = lgbmgs.best_index_\n",
    "    result_dict['acc_train'] = model.score(train_scaled, train_target)\n",
    "    result_dict['acc_val'] = model.score(val_scaled, val_target)\n",
    "    result_dict['acc_test'] = model.score(test_scaled, test_target)\n",
    "    result_dict['precision'] = precision_score(test_target, y_predict)\n",
    "    result_dict['recall'] = recall_score(test_target, y_predict)\n",
    "    result_dict['f1score'] = f1_score(test_target, y_predict)\n",
    "    result_dict['roc'] = roc_auc_score(test_target, y_predict)\n",
    "    cm = confusion_matrix(test_target, y_predict)\n",
    "    result_dict['tn'] = cm[0,0]\n",
    "    result_dict['fp'] = cm[0,1]\n",
    "    result_dict['fn'] = cm[1,0]\n",
    "    result_dict['tp'] = cm[1,1]\n",
    "    #------------------------------------\n",
    "    y_predict = model.predict(test_scaled1)\n",
    "    result_dict['acc_test1'] = model.score(test_scaled1, test_target1)\n",
    "    result_dict['precision1'] = precision_score(test_target1, y_predict)\n",
    "    result_dict['recall1'] = recall_score(test_target1, y_predict)\n",
    "    result_dict['f1score1'] = f1_score(test_target1, y_predict)\n",
    "    result_dict['roc1'] = roc_auc_score(test_target1, y_predict)\n",
    "    cm = confusion_matrix(test_target1, y_predict)\n",
    "    result_dict['tn1'] = cm[0,0]\n",
    "    result_dict['fp1'] = cm[0,1]\n",
    "    result_dict['fn1'] = cm[1,0]\n",
    "    result_dict['tp1'] = cm[1,1]\n",
    "    #------------------------------------\n",
    "    y_predict = model.predict(test_scaled2)\n",
    "    result_dict['acc_test2'] = model.score(test_scaled2, test_target2)\n",
    "    result_dict['precision2'] = precision_score(test_target2, y_predict)\n",
    "    result_dict['recall2'] = recall_score(test_target2, y_predict)\n",
    "    result_dict['f1score2'] = f1_score(test_target2, y_predict)\n",
    "    result_dict['roc2'] = roc_auc_score(test_target2, y_predict)\n",
    "    cm = confusion_matrix(test_target2, y_predict)\n",
    "    result_dict['tn2'] = cm[0,0]\n",
    "    result_dict['fp2'] = cm[0,1]\n",
    "    result_dict['fn2'] = cm[1,0]\n",
    "    result_dict['tp2'] = cm[1,1]\n",
    "    #------------------------------------\n",
    "    y_predict = model.predict(test_scaled3)\n",
    "    result_dict['acc_test3'] = model.score(test_scaled3, test_target3)\n",
    "    result_dict['precision3'] = precision_score(test_target3, y_predict)\n",
    "    result_dict['recall3'] = recall_score(test_target3, y_predict)\n",
    "    result_dict['f1score3'] = f1_score(test_target3, y_predict)\n",
    "    result_dict['roc3'] = roc_auc_score(test_target3, y_predict)\n",
    "    cm = confusion_matrix(test_target3, y_predict)\n",
    "    result_dict['tn3'] = cm[0,0]\n",
    "    result_dict['fp3'] = cm[0,1]\n",
    "    result_dict['fn3'] = cm[1,0]\n",
    "    result_dict['tp3'] = cm[1,1]\n",
    "    return result_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save parameters and columns used for the analysis\n",
    "def save_parameters(iter, com_name, dt, precision, params_o, new_col):\n",
    "    save_to_pickle(f'./{com_name}/params_{dt}_{round(precision*100):2d}%_ver{iter}.pkl', params_o)\n",
    "    save_dict_to_csv(f'./{com_name}/params_{dt}_{round(precision*100):2d}%_ver{iter}.pkl', params_o)\n",
    "    save_to_pickle(f'./{com_name}/columns_{dt}_{len(new_col)}_{round(precision*100):2d}%_ver{iter}.pkl', new_col)\n",
    "    save_list_to_csv(f'./{com_name}/columns_{dt}_{len(new_col)}_{round(precision*100):2d}%_ver{iter}.pkl', new_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "code = {'005930' : ['삼성전자', 'sec'], '373220' : ['LG에너지솔루션', 'lgenergy'], \n",
    "        '000660' : ['SK하이닉스', 'skhinix'], '207940' : ['삼성바이오로직스', 'ssbio'],\n",
    "        '006400' : ['삼성SDI', 'sdi'], '051910' : ['LG화학', 'lgchemical'],\n",
    "        '005935' : ['삼성전자우', 'secpre'], '005380' : ['현대차', 'hyunmotor'],\n",
    "        '035420' : ['NAVER', 'naver'], '000270' : ['기아','kia'],\n",
    "        '035720' : ['카카오', 'kakao'], '005490' : ['POSCO홀딩스', 'poscoholding'],\n",
    "        '105560' : ['KB금융', 'kbbank'], '028260' : ['삼성물산', 'sscnt'],\n",
    "        '068270' : ['셀트리온', 'celltrion'], '012330' : ['현대모비스', 'mobis'],\n",
    "        '055550' : ['신한지주', 'shgroup'], '066570' : ['LG전자', 'lgelec'],\n",
    "        '003670' : ['포스코케미칼', 'poscochemical'], '096770' : ['SK이노베이션', 'skinnovation'],\n",
    "        '033780' : ['KT&G', 'ktng']}\n",
    "\n",
    "code = {'005930' : ['삼성전자', 'sec']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_inv1 = ['retail_1', 'foreigner_1', 'institution_1', 'financial_1', 'invtrust_1', 'pension_1', \n",
    "#             'privequity_1', 'bank_1', 'insurance_1', 'financeetc_1', 'corporateetc_1', \n",
    "            'privequity_1',  'insurance_1', 'corporateetc_1', # bank_1, 'financeetc_1 제외\n",
    "            'foreigneretc_1']\n",
    "col_inv2 = ['retail_2', 'foreigner_2', 'institution_2', 'financial_2', 'invtrust_2', 'pension_2',\n",
    "#             'privequity_2', 'bank_2', 'insurance_2', 'financeetc_2', 'corporateetc_2', \n",
    "            'privequity_2', 'insurance_2', 'corporateetc_2', # bank_2, 'financeetc_2 제외\n",
    "            'foreigneretc_2']\n",
    "col_his1 = ['open_1', 'high_1', 'low_1', 'close_1', 'vol_1']\n",
    "col_his2 = ['open_2', 'high_2', 'low_2', 'close_2', 'vol_2']\n",
    "col_cr = ['weekday', 'cr_00', 'cr_05', 'cr_10', 'cr_15', 'cr_20']\n",
    "col_common1 = [\"dji_cr\", \"dji_f_cr\", \"dxy_cr\", \"ixic_f_cr\", \"bond_kor_10_cr\", \"bond_kor_2_cr\", \"kosdaq_cr\", \"kospi_cr\", \n",
    "         \"krw_cr\", \"ixic_cr\", \"spx_f_cr\", \"sox_cr\", \"spx_cr\", \"bond_usa_10_cr\", \"bond_usa_2_cr\", \"bond_usa_3m_cr\", \n",
    "         \"vix_cr\", \"wti_cr\", \"spsy_cr\", \"spny_cr\", \"spxhc_cr\", \"splrcd_cr\", \"splrci_cr\", \"splrcu_cr\", \"splrcs_cr\",\n",
    "         \"splrct_cr\", \"splrcl_cr\", \"splrcm_cr\", \"ixbk_cr\", \"ixfn_cr\", \"ixid_cr\", \"ixis_cr\", \"ixk_cr\", \"ixtr_cr\",\n",
    "         \"ixut_cr\", \"nbi_cr\", \"bkx_cr\"]\n",
    "col_common2 = [\"dji_cr_2\", \"dji_f_cr_2\", \"dxy_cr_2\", \"ixic_f_cr_2\", \"bond_kor_10_cr_2\", \"bond_kor_2_cr_2\", \"kosdaq_cr_2\", \"kospi_cr_2\",\n",
    "         \"krw_cr_2\", \"ixic_cr_2\", \"spx_f_cr_2\", \"sox_cr_2\", \"spx_cr_2\", \"bond_usa_10_cr_2\", \"bond_usa_2_cr_2\", \"bond_usa_3m_cr_2\",\n",
    "         \"vix_cr_2\", \"wti_cr_2\", \"spsy_cr_2\", \"spny_cr_2\", \"spxhc_cr_2\", \"splrcd_cr_2\", \"splrci_cr_2\", \"splrcu_cr_2\",\n",
    "         \"splrcs_cr_2\", \"splrct_cr_2\", \"splrcl_cr_2\", \"splrcm_cr_2\", \"ixbk_cr_2\", \"ixfn_cr_2\", \"ixid_cr_2\",\n",
    "         \"ixis_cr_2\", \"ixk_cr_2\", \"ixtr_cr_2\", \"ixut_cr_2\", \"nbi_cr_2\", \"bkx_cr_2\"]\n",
    "new_col = col_inv1 + col_common1 + col_his1 + col_inv2 + col_common2 + col_his2 + col_cr\n",
    "\n",
    "# bank, financeetc는 결측치가 많아서 사용하지 않음.\n",
    "# df.drop(['bank_1', 'bank_2', 'financeetc_1', 'financeetc_2'], axis=1, inplace=True)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 최초의 empty df 생성\n",
    "df_base = pd.DataFrame(pd.Series([],dtype=pd.StringDtype(), name='parameter')).set_index('parameter')\n",
    "iter = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "com_name = 'sec'\n",
    "\n",
    "directory_for_ml = '../data/data_for_ml/expand_date/'\n",
    "fname = f'df_{com_name}_sel.pkl'\n",
    "f_name = directory_for_ml + fname\n",
    "df_o = pd.read_pickle(f_name) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_col = load_from_pickle('./sec/columns_0323_1641_50_81%_ver8.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 반복 작업시 여기서 부터 진행 (feature importance로 선정된 새로운 column으로)\n",
    "\n",
    "df = df_o[new_col]  # 새롭게 선정된 column으로 진행\n",
    "\n",
    "# train, val,: 8, test: 2\n",
    "split_ratio = 0.8\n",
    "split_n = int(len(df)*split_ratio)\n",
    "\n",
    "test_interval = int((len(df) - split_n)/3)\n",
    "data = df.iloc[:split_n, :-5]\n",
    "target = df.iloc[:split_n, -5]\n",
    "test_input = df.iloc[split_n:, :-5]\n",
    "test_target = df.iloc[split_n:, -5]\n",
    "test_input1 = df.iloc[split_n:split_n+test_interval, :-5]\n",
    "test_input2 = df.iloc[split_n+test_interval: split_n+test_interval*2, :-5]\n",
    "test_input3 = df.iloc[split_n+test_interval*2:, :-5]\n",
    "test_target1 = df.iloc[split_n:split_n+test_interval, -5]\n",
    "test_target2 = df.iloc[split_n+test_interval: split_n+test_interval*2, -5]\n",
    "test_target3 = df.iloc[split_n+test_interval*2:, -5]\n",
    "\n",
    "train_input, val_input, train_target, val_target = \\\n",
    "     train_test_split(data, target, random_state=42, test_size=0.2, stratify=target)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(train_input)\n",
    "train_scaled = scaler.transform(train_input)\n",
    "val_scaled = scaler.transform(val_input)\n",
    "test_scaled = scaler.transform(test_input)\n",
    "test_scaled1 = scaler.transform(test_input1)\n",
    "test_scaled2 = scaler.transform(test_input2)\n",
    "test_scaled3 = scaler.transform(test_input3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_base = {'boosting_type' : ['gbdt'], # ['gbdt', dart', 'goss'], # dart : 신경망의 드롭아웃을 적용시킨 방법, \n",
    "              'num_leaves' :  [6, 7, 8,], # 두번째로 중요, num_leaves는 작은 데이터면 작은 숫자로\n",
    "              'learning_rate' :  [0.001, 0.0015, 0.0017, 0.002, 0.0025],\n",
    "#               'max_delta_step' :  [0.4, 0.5, 0.6],\n",
    "              'n_estimators' :  [ 1100, 1200, 1300, 1400],\n",
    "              'colsample_bytree' :  [ 0.5, 0.6, 0.7, 0.8], # = feature fraction, column sampling. 위의 subsample과 같이 튜닝.\n",
    "              'subsample' : [1], #  [0.1, 0.2, 0.3, 0.4], # 세번째로 중요. = bagging fraction, row sampling. 아래 colsample_bytree과 같이 튜닝.\n",
    "              'max_depth' :  [-1], # 가장 먼저 튜닝 필요 -1이 default (무한깊이) 일반적으로 default가 가장 좋음.\n",
    "              'objective': ['binary'],\n",
    "              'metric': ['binary_logloss'],\n",
    "              'scale_pos_weight': [1.0, 1.5, 2], # posiive  증가, class imbalance 경감, scale_pos_weight > 0.0, default=1.0. \n",
    "#               'min_child_samples' : [30],\n",
    "#               'lambda_l1': [0, 5], # default 0\n",
    "#               'lambda_l2': [0, 5], #default 0\n",
    "              }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "#     'cv' : 7,\n",
    "    'cv' : None,\n",
    "    'scoring' : None,\n",
    "#     'scoring' : 'precision',\n",
    "#     'scoring' : 'accuracy',\n",
    "    'num_col' : len(new_col)\n",
    "             }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_base = load_from_pickle('./sec/params_0323_1641_81%_ver8.pkl') # 읽고 수정하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_base = {'boosting_type': ['gbdt'],\n",
    " 'num_leaves': [6,],\n",
    " 'learning_rate': [0.0015,],\n",
    " 'n_estimators': [1300],\n",
    " 'colsample_bytree': [0.6,],\n",
    " 'subsample': [1],\n",
    " 'max_depth': [-1],\n",
    " 'objective': ['binary'],\n",
    " 'metric': ['binary_logloss'],\n",
    " 'scale_pos_weight': [1.5]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_o = param_base.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# directory가 없으면 만드는 과정\n",
    "if not os.path.exists(com_name):\n",
    "    os.makedirs(com_name)\n",
    "    \n",
    "while True:\n",
    "    iter = iter + 1\n",
    "\n",
    "    lgbm = None\n",
    "    lgbmgs = None\n",
    "\n",
    "    lgbm = lightgbm.LGBMClassifier(random_state=42)\n",
    "\n",
    "    lgbmgs = GridSearchCV(estimator = lgbm,\n",
    "                          param_grid = params_o,\n",
    "#                           cv = 10, # StratifiedKFold us default for binary or multiclass\n",
    "#                           scoring = 'precision', \n",
    "#                           scoring = 'accuracy', \n",
    "#                           scoring = ['accuracy', 'precision'], # refit 사용해야 함. 고로 사용하지 않음.\n",
    "                          cv = param_grid['cv'],\n",
    "                          scoring = param_grid['scoring'],\n",
    "                          error_score='raise',\n",
    "                          verbose = 1,\n",
    "                          n_jobs=-1, # 자동 검색 적용\n",
    "                         )\n",
    "                          \n",
    "    print(\"*** after lgbmgs ******\")\n",
    "    lgbmgs.fit(train_scaled, train_target, eval_metric = 'logloss') \n",
    "#     lgbmgs.fit(train_scaled, train_target, eval_metric = 'logloss', eval_set = (val_scaled, val_target)) \n",
    "    # eval_set가 있어야 \"early_stopping_rounds\"를 사용할 수 있음.\n",
    "\n",
    "# save model\n",
    "    stamp = datetime.datetime.today().isoformat() # 파일명 끝에 생성날짜 시간 추가\n",
    "    dt = re.sub(r'[-:T]', '', stamp[5:16])\n",
    "    dt = f'{dt[:4]}_{dt[4:]}'\n",
    "    joblib.dump(lgbmgs, f'./{com_name}/lgbm_{dt}_v{iter}.pkl') # gridsearchcv 저장\n",
    "    joblib.dump(scaler, f'./{com_name}/scaler_{dt}_v{iter}.pkl') # scaler 저장\n",
    "    \n",
    "    df_estimator = make_df_from_estimator(lgbmgs.best_estimator_.get_params(), iter)\n",
    "    result_dict = calc_results(lgbmgs.best_estimator_, \n",
    "                               train_scaled, val_scaled, test_scaled,  \n",
    "                               train_target, val_target, test_target,\n",
    "                               test_scaled1, test_scaled2, test_scaled3,  \n",
    "                               test_target1, test_target2, test_target3\n",
    "                              )\n",
    "    \n",
    "    df_result = make_df_from_estimator(result_dict, iter)\n",
    "    df_grid = make_df_from_estimator(param_grid, iter)\n",
    "    df_concat = pd.concat([df_estimator, df_grid, df_result])\n",
    "\n",
    "    df_base = pd.merge(df_base,df_concat, how='outer', left_index=True, right_index=True)\n",
    "    \n",
    "#  4가지 조건이 만족되면 break하고 완료\n",
    "    val_test = df_concat.loc['acc_val'].iloc[0]\n",
    "    acc_test = df_concat.loc['acc_test'].iloc[0]\n",
    "    precision = df_concat.loc['precision'].iloc[0]\n",
    "    f1score = df_concat.loc['f1score'].iloc[0]\n",
    "    \n",
    "    if (val_test >= 0.75 ) & (acc_test > 0.75) & (precision >= 0.8) & (f1score >= 0.6) :\n",
    "        df_base.to_csv(f'./{com_name}/lgbm_df_{dt}_{round(precision*100):2d}%_ver{iter}.csv')\n",
    "        df_base.to_pickle(f'./{com_name}/lgbm_df_{dt}_{round(precision*100):2d}%_ver{iter}.pkl')\n",
    "#         params_o = make_new_parameter(params_o, df_concat)\n",
    "#         save_to_pickle(f'./{com_name}/params_{dt}_{round(precision*100):2d}%_ver{iter}.pkl', params_o)\n",
    "#         save_dict_to_csv(f'./{com_name}/params_{dt}_{round(precision*100):2d}%_ver{iter}.pkl', params_o)\n",
    "#         save_to_pickle(f'./{com_name}/columns_{dt}_{len(new_col)}_{round(precision*100):2d}%_ver{iter}.pkl', new_col)\n",
    "#         save_list_to_csv(f'./{com_name}/columns_{dt}_{len(new_col)}_{round(precision*100):2d}%_ver{iter}.pkl', new_col)\n",
    "        save_parameters(iter, com_name, dt, precision, params_o, new_col)\n",
    "        break\n",
    "    if iter >= 1 : \n",
    "        df_base.to_csv(f'./{com_name}/lgbm_df_{dt}_{round(precision*100):2d}%_ver{iter}.csv')\n",
    "        df_base.to_pickle(f'./{com_name}/lgbm_df_{dt}_{round(precision*100):2d}%_ver{iter}.pkl')\n",
    "#         params_o = make_new_parameter(params_o, df_concat)\n",
    "#         save_to_pickle(f'./{com_name}/params_{dt}_{round(precision*100):2d}%_ver{iter}.pkl', params_o)\n",
    "#         save_dict_to_csv(f'./{com_name}/params_{dt}_{round(precision*100):2d}%_ver{iter}.pkl', params_o)\n",
    "#         save_to_pickle(f'./{com_name}/columns_{dt}_{len(new_col)}_{round(precision*100):2d}%_ver{iter}.pkl', new_col)\n",
    "#         save_list_to_csv(f'./{com_name}/columns_{dt}_{len(new_col)}_{round(precision*100):2d}%_ver{iter}.pkl', new_col)\n",
    "        save_parameters(iter, com_name, dt, precision, params_o, new_col)\n",
    "        break\n",
    "    print(\"******* No.{}  Process is Done! ********\".format(iter))\n",
    "#     params_o = make_new_parameter(params_o, df_concat)\n",
    "#     save_to_pickle(f'./{com_name}/params_{dt}_{round(precision*100):2d}%_ver{iter}.pkl', params_o)\n",
    "#     save_dict_to_csv(f'./{com_name}/params_{dt}_{round(precision*100):2d}%_ver{iter}.pkl', params_o)\n",
    "#     save_to_pickle(f'./{com_name}/columns_{dt}_{len(new_col)}_{round(precision*100):2d}%_ver{iter}.pkl', new_col)\n",
    "#     save_list_to_csv(f'./{com_name}/columns_{dt}_{len(new_col)}_{round(precision*100):2d}%_ver{iter}.pkl', new_col)\n",
    "    save_parameters(iter, com_name, dt, precision, params_o, new_col)\n",
    "    \n",
    "print(\"**** End of Process ****\")\n",
    "# save model, save df, stoppping 기준 수립\n",
    "# 일단위로 정확도 측정, 정확도, 정밀도?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_base.head(60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "칼럼수를 90으로 줄이니까, precision이 90%으로 향상됨."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(lgbmgs.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = lgbmgs.best_estimator_\n",
    "feature_df = pd.DataFrame(model.booster_.feature_importance(importance_type='gain'), \n",
    "                      index=data.columns, columns=['importance']).sort_values(by='importance', \n",
    "                                                                              ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(feature_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_df.head(90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_col = list(feature_df.index[:80]) +  ['cr_00', 'cr_05', 'cr_10', 'cr_15', 'cr_20']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.booster_.feature_importance(importance_type='gain')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.booster_.feature_importance(importance_type='split')\n",
    "# 큰 특징을 가지는 feature는 tree상위레벨에서 적게 사용됨."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 사용된 column을 불러와서 신규column 지정하여 최적의 column 찾기에 사용\n",
    "directory_for_ml = '../machine_learning/sec/'\n",
    "fname = f'columns_ver1_0323_1535_72_0.74%.pkl'\n",
    "f_name = directory_for_ml + fname\n",
    "new_col = load_from_pickle(f_name)\n",
    "new_col =  new_col[:61]+new_col[-5:] # 원하는 column 선정"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 정밀도, f1-score, \n",
    "2. confusion matrix ((1,1), (2,2), 두개가 큰 비중이면 good, (1,2)은 틀린것을 맞다라고 구분, (2,1)은 맞는 것을 틀린 것이다 라고 결정하는 항목) 따라서\n",
    "    (2,2) -> (1,2) -> (1,1)로 확인하고. <br>\n",
    "    (1,2)가 크면 모델 제외 (정밀도(precision = TP / (TP + FP) )가 높아야 함. 낮으면 손해를 보게 됨.), <br>\n",
    "    재현율(Recall = TP / (TP + FN) ) 은 손해를 끼치지는 않음.\n",
    "    \n",
    "<img src=\"https://raw.githubusercontent.com/fasthill/My-gist/main/data/picture/confusion_matrix.png\" width=\"800\"/> <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory_for_ml = '../data/data_for_ml/expand_date/'\n",
    "ss = pd.read_csv(directory_for_ml+'df_sec_sel.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "5-1 결정 트리.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
