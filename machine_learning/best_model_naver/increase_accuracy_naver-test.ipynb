{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LlS4gyYqJVpn"
   },
   "source": [
    "# 가장 좋은 결과를 낼 수 있는 feature항목 추출"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tQBDVUjBbIsL"
   },
   "source": [
    "## 모든 feature를 사용한 결과와, 선택 추출된 feature만 사용한 결과 정확도에 차이가 남\n",
    "#### logistic 회귀 이용하여 coef_ 항목에서 영향력이 높은 feature를 선택. 최적의 갯수 선택"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ILi_LPl9JVpw"
   },
   "source": [
    "### 데이터 준비하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "EwImyTS9S3QQ"
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.model_selection import cross_validate, train_test_split\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.datasets import fashion_mnist\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten, Dropout\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau, LearningRateScheduler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import precision_score, recall_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "def list_to_pickle(filename, listname):\n",
    "    open_file = open(filename, \"wb\")\n",
    "    pickle.dump(listname, open_file)\n",
    "    open_file.close()\n",
    "\n",
    "def list_from_pickle(filename):\n",
    "    open_file = open(filename, \"rb\")\n",
    "    loaded_list = pickle.load(open_file)\n",
    "    open_file.close()\n",
    "    return loaded_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 분석용 데이터 입력\n",
    "stock_name = 'naver'\n",
    "directory_for_ml = '../data/data_for_ml/'\n",
    "fname = f'df_{stock_name}_sel.pkl'\n",
    "f_name = directory_for_ml + fname\n",
    "df = pd.read_pickle(f_name)\n",
    "plt_title = fname[3:7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns=['value', 'r_open_high_5', 'close_cr_1', 'r_open_high_1'], inplace=True )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_columns = ['kospi_cr', 'kosdaq_cr', 'dxy_cr', 'spx_cr', 'krw_cr', 'financeetc',\n",
    "               'bank', 'bond_usa_10_cr', 'spx_f_cr', 'bond_kor_10_cr', 'close_cr_5']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new = df[new_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_data = 220\n",
    "data = df_new.iloc[:num_data, :-1]\n",
    "data_test = df_new.iloc[num_data:, :-1]\n",
    "target = df_new.iloc[:num_data, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_input, test_input, train_target, test_target = train_test_split(data, target, random_state=42, test_size=0.2, stratify=target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss = StandardScaler()\n",
    "ss.fit(train_input)\n",
    "train_scaled = ss.transform(train_input)\n",
    "test_scaled = ss.transform(test_input)\n",
    "data_test_scaled = ss.transform(data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_fn(inp_num, a_layer=None):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(12, activation='relu', input_shape=(inp_num,)))\n",
    "#     model.add(Dropout(0.1))\n",
    "    model.add(Dense(6, activation='sigmoid'))\n",
    "#     model.add(Dropout(0.1))\n",
    "    model.add(Dense(3, activation='sigmoid'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_rate = 1e-7 # default value\n",
    "adam_custom = tf.keras.optimizers.Adam(\n",
    "    learning_rate=0.000005, # default  0.001 , best fit 0.0001 for skhinix\n",
    "    beta_1=0.9, beta_2=0.999, epsilon=d_rate, amsgrad=False,  # default\n",
    "    weight_decay=None, clipnorm=None, clipvalue=None, global_clipnorm=None, use_ema=False, ema_momentum=0.99,\n",
    "    ema_overwrite_frequency=None, jit_compile=True, name='Adam' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = model_fn(10, len(data_new.columns), 2, Dropout(0.3))\n",
    "try :\n",
    "    model = None\n",
    "except:\n",
    "    pass\n",
    "\n",
    "model = model_fn(len(data.columns))\n",
    "# model.compile(optimizer='adam', loss='binary_crossentropy', \n",
    "#               metrics=['accuracy'])\n",
    "model.compile(optimizer=adam_custom , loss='binary_crossentropy', \n",
    "              metrics=['accuracy'])\n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checkpoint_cb = ModelCheckpoint('best_model.h5', save_best_only=True)\n",
    "checkpoint_cb = keras.callbacks.ModelCheckpoint(\n",
    "    filepath='best_model/best_model_{epoch:02d}-{val_loss:.2f}-{val_accuracy:.2f}.h5', \\\n",
    "     save_best_only=True)\n",
    "# checkpoint_cb = keras.callbacks.ModelCheckpoint(filepath='best_model/skhinix_model.h5', \\\n",
    "#                                                 monitor='val_accuracy', mode='max', save_best_only=True)\n",
    "# checkpoint_cb = keras.callbacks.ModelCheckpoint(filepath='best_model/skhinix_model.h5', save_best_only=True)\n",
    "# earlystopping_cb = keras.callbacks.EarlyStopping(patience=100, monitor='val_accuracy', mode='max', restore_best_weights=True)\n",
    "earlystopping_cb = keras.callbacks.EarlyStopping(patience=200, monitor='val_loss', mode='min', restore_best_weights=True)\n",
    "# earlystopping_cb = keras.callbacks.EarlyStopping(patience=100, restore_best_weights=True)\n",
    "\n",
    "reducelr = tf.keras.callbacks.ReduceLROnPlateau( monitor='val_loss', factor=0.9,  \n",
    "                                                patience=100, verbose=0, mode='auto', min_delta=0.0001, #0.0001\n",
    "                                                cooldown=0, min_lr=0 )\n",
    "\n",
    "def scheduler(epoch, lr):\n",
    "    if epoch < 1000:\n",
    "        print(\"epoch, m, lr\", epoch, m, lr)\n",
    "        return lr\n",
    "    else:\n",
    "        print(\"epoch, m, lr\", epoch, m, lr)\n",
    "        return lr * tf.math.exp(-0.01)\n",
    "\n",
    "lrscheduler = tf.keras.callbacks.LearningRateScheduler(scheduler, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(train_scaled, train_target, epochs=30000, verbose=0, batch_size=20,\n",
    "#                     callbacks=[checkpoint_cb, earlystopping_cb, reducelr],\n",
    "                    callbacks=[earlystopping_cb, reducelr],\n",
    "                    validation_data=(test_scaled, test_target))\n",
    "\n",
    "# loss 가 최저로 내려가지 않거나 큰 상태에서 머무르면 adam (optimizer)의 learning rate를 줄이면서 loss가 작아지는지 시도해 볼 것\n",
    "# batch size 도 조절\n",
    "# layer를 추가하면서 진행도 많이 도움됨.(mobis 경우에 적용) -> 과대적합이 됨.\n",
    "# batch_size를 늘리거나 줄이면 val_loss의 꺽이는 영역이 위아래로 바뀌니 lr을 고정하고 조금씩 조정하면서 최적의 사이즈를 찾아야 함.\n",
    "# --> 이후 lr을 아주 조금씩 줄이면서 시도함. (0.0001 -> 0.00009 -> 0.00008 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('loss')\n",
    "plt.legend(['train', 'val'])\n",
    "plt.title(plt_title)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('accuracy')\n",
    "plt.legend(['accuracy', 'val_accuracy'])\n",
    "plt.title(plt_title)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 정밀도 : 양성으로 예측된 것(TP+FP) 중 얼마나 많은 샘플이 진짜 양성(TP)인지 측정\n",
    "model.evaluate(test_scaled, test_target)\n",
    "y_predict = model.predict(np.array(test_scaled))\n",
    "y_predict_list = [1 if i > 0.5 else 0 for i in y_predict[:, 0]]\n",
    "print(\"정밀도\", precision_score(test_target, y_predict_list)) \n",
    "print(\"재현율\", recall_score(test_target, y_predict_list))\n",
    "print(\"f1_acore\", f1_score(test_target, y_predict_list))\n",
    "print(\"roc_auc_score\", roc_auc_score(test_target, y_predict_list))\n",
    "\n",
    "print(\"confusion matrix\", confusion_matrix(test_target, y_predict_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test_target = df_new.iloc[num_data:, -1]\n",
    "model.evaluate(data_test_scaled, data_test_target)\n",
    "y_predict = model.predict(np.array(data_test_scaled))\n",
    "y_predict_list = [1 if i > 0.5 else 0 for i in y_predict[:, 0]]\n",
    "print(\"정밀도\", precision_score(data_test_target, y_predict_list)) \n",
    "print(\"재현율\", recall_score(data_test_target, y_predict_list))\n",
    "print(\"f1_acore\", f1_score(data_test_target, y_predict_list))\n",
    "print(\"roc_auc_score\", roc_auc_score(data_test_target, y_predict_list))\n",
    "\n",
    "print(\"confusion matrix\", confusion_matrix(data_test_target, y_predict_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare = [ [x, y] for x, y in zip(data_test_target, y_predict)]\n",
    "# compare = [ [x, y] for x, y in zip(test_target, y_predict_list)]\n",
    "compare # 실제값. 예측값"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict(np.array(data_test_scaled))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "4-1 로지스틱 회귀.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
