{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/fasthill/ML-DL-study-alone/blob/main/5-1%20%EA%B2%B0%EC%A0%95%20%ED%8A%B8%EB%A6%AC.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5uA_6TRHEMHV"
   },
   "source": [
    "## Analyse from LightGBM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "26KAIfzEEMHc"
   },
   "source": [
    "<table align=\"left\">\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/rickiepark/hg-mldl/blob/master/5-1.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />구글 코랩에서 실행하기</a>\n",
    "  </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "rIXeXBVTfZrS"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from functools import reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "import xgboost\n",
    "from lightgbm import LGBMClassifier\n",
    "import lightgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression, SGDClassifier, SGDRegressor\n",
    "from sklearn.model_selection import cross_validate, train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten, Dropout\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# confusion matrix to list 변환\n",
    "def matrix_to_list(confu_matrix):\n",
    "    m_list = []\n",
    "    tn = confu_matrix[0,0]\n",
    "    fp = confu_matrix[0,1]\n",
    "    fn = confu_matrix[1,0]\n",
    "    tp = confu_matrix[1,1]\n",
    "    m_list.extend([tn, fp, fn, tp])\n",
    "    return m_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_p(test_target, y_predict_list): \n",
    "    ps = precision_score(test_target, y_predict_list)\n",
    "    rs = recall_score(test_target, y_predict_list)\n",
    "    fs = f1_score(test_target, y_predict_list)\n",
    "    roc = roc_auc_score(test_target, y_predict_list)\n",
    "#     cm = matrix_to_list(confusion_matrix(test_target, y_predict_list))\n",
    "    collect_list = [ps, rs, fs, roc]\n",
    "#     collect_list.extend(cm)\n",
    "    return collect_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def modelfit(alg, dtrain, predictors,useTrainCV=True, cv_folds=5, early_stopping_rounds=50):\n",
    "    \n",
    "    if useTrainCV:\n",
    "        xgb_param = alg.get_xgb_params()\n",
    "        xgtrain = xgb.DMatrix(dtrain[predictors].values, label=dtrain[target].values)\n",
    "        cvresult = xgb.cv(xgb_param, xgtrain, num_boost_round=alg.get_params()['n_estimators'], nfold=cv_folds,\n",
    "            metrics='auc', early_stopping_rounds=early_stopping_rounds, show_progress=False)\n",
    "        alg.set_params(n_estimators=cvresult.shape[0])\n",
    "    \n",
    "    #Fit the algorithm on the data\n",
    "    alg.fit(dtrain[predictors], dtrain['Disbursed'],eval_metric='auc')\n",
    "        \n",
    "    #Predict training set:\n",
    "    dtrain_predictions = alg.predict(dtrain[predictors])\n",
    "    dtrain_predprob = alg.predict_proba(dtrain[predictors])[:,1]\n",
    "        \n",
    "#     #Print model report:\n",
    "#     print(\"\\nModel Report\")\n",
    "#     print(\"Accuracy : %.4g\" % metrics.accuracy_score(dtrain['Disbursed'].values, dtrain_predictions))\n",
    "#     print(\"AUC Score (Train): %f\" % metrics.roc_auc_score(dtrain['Disbursed'], dtrain_predprob))\n",
    "                    \n",
    "    feat_imp = pd.Series(alg.booster().get_fscore()).sort_values(ascending=False)\n",
    "    feat_imp.plot(kind='bar', title='Feature Importances')\n",
    "    plt.ylabel('Feature Importance Score')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Choose all predictors except target & IDcols\n",
    "# predictors = [x for x in train.columns if x not in [target, IDcol]]\n",
    "# xgb1 = XGBClassifier(\n",
    "#  learning_rate =0.1,\n",
    "#  n_estimators=1000,\n",
    "#  max_depth=5,\n",
    "#  min_child_weight=1,\n",
    "#  gamma=0,\n",
    "#  subsample=0.8,\n",
    "#  colsample_bytree=0.8,\n",
    "#  objective= 'binary:logistic',\n",
    "#  nthread=4,\n",
    "#  scale_pos_weight=1,\n",
    "#  seed=27)\n",
    "# modelfit(xgb1, train, predictors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "code = {'005930' : ['삼성전자', 'sec'], '373220' : ['LG에너지솔루션', 'lgenergy'], \n",
    "        '000660' : ['SK하이닉스', 'skhinix'], '207940' : ['삼성바이오로직스', 'ssbio'],\n",
    "        '006400' : ['삼성SDI', 'sdi'], '051910' : ['LG화학', 'lgchemical'],\n",
    "        '005935' : ['삼성전자우', 'secpre'], '005380' : ['현대차', 'hyunmotor'],\n",
    "        '035420' : ['NAVER', 'naver'], '000270' : ['기아','kia'],\n",
    "        '035720' : ['카카오', 'kakao'], '005490' : ['POSCO홀딩스', 'poscoholding'],\n",
    "        '105560' : ['KB금융', 'kbbank'], '028260' : ['삼성물산', 'sscnt'],\n",
    "        '068270' : ['셀트리온', 'celltrion'], '012330' : ['현대모비스', 'mobis'],\n",
    "        '055550' : ['신한지주', 'shgroup'], '066570' : ['LG전자', 'lgelec'],\n",
    "        '003670' : ['포스코케미칼', 'poscochemical'], '096770' : ['SK이노베이션', 'skinnovation'],\n",
    "        '033780' : ['KT&G', 'ktng']}\n",
    "\n",
    "code = {'005930' : ['삼성전자', 'sec']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = f'df_sec_sel.pkl'\n",
    "stock_name = 'sec'\n",
    "directory_for_ml = '../data/data_for_ml/'\n",
    "f_name = directory_for_ml + fname\n",
    "df = pd.read_pickle(f_name) \n",
    "\n",
    "# train, val,: 8, test: 2\n",
    "split_ratio = 0.8\n",
    "split_n = int(len(df)*split_ratio)\n",
    "\n",
    "data = df.iloc[:split_n, :-5]\n",
    "target = df.iloc[:split_n, -4]\n",
    "test_input = df.iloc[split_n:, :-5]\n",
    "test_target = df.iloc[split_n:, -4]\n",
    "\n",
    "train_input, val_input, train_target, val_target = \\\n",
    "     train_test_split(data, target, random_state=42, test_size=0.2, stratify=target)\n",
    "\n",
    "ss = StandardScaler()\n",
    "ss.fit(train_input)\n",
    "train_scaled = ss.transform(train_input)\n",
    "val_scaled = ss.transform(val_input)\n",
    "test_scaled = ss.transform(test_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "train accuracy: 1.0000, val accuracy 0.7907, test accuracy 0.8519\n",
      "precision : 0.7368, recall : 0.8235, f1score : 0.7778, roc : 0.8442\n",
      "[[32  5]\n",
      " [ 3 14]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kange\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\lightgbm\\engine.py:177: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
      "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n"
     ]
    }
   ],
   "source": [
    "lgbm = None\n",
    "lgbm = lightgbm.LGBMClassifier(random_state=42,\n",
    "                               learning_rate =0.22, # default = 0.1\n",
    "                               num_iterations=500, # default=100\n",
    "                               max_depth=5, # <= 0 means no limit, default=-1\n",
    "                               bagging_fraction=0.9, # 0.0 < bagging_fraction <= 1.0, default=1\n",
    "                               feature_fraction=0.8, # 0.0 < feature_fraction <= 1.0, default=1\n",
    "                               objective= 'binary', \n",
    "#                                num_threads = 0, # == nthread=4, # Gpu 수, default=0, 자동 검색후 적용\n",
    "                               max_delta_step = 0.5, # best value found, default = 0\n",
    "#                             scale_pos_weight=40, # class imbalance 경감, scale_pos_weight > 0.0, default=1.0\n",
    "                              ) \n",
    "lgbm.fit(train_scaled, train_target, eval_metric = 'logloss')\n",
    "y_pred = lgbm.predict(val_scaled)\n",
    "# val_error = mean_squared_error(val_target, y_pred) # 책에 없음\n",
    "# print(\"Validation MSE:\", val_error)           # 책에 없음\n",
    "\n",
    "train_score_lgbm = lgbm.score(train_scaled, train_target)\n",
    "val_score_lgbm = lgbm.score(val_scaled, val_target)\n",
    "test_score_lgbm = lgbm.score(test_scaled, test_target)\n",
    "ps, rs, fs, roc = predict_p(test_target, lgbm.predict(test_scaled))\n",
    "cm = confusion_matrix(test_target, lgbm.predict(test_scaled))\n",
    "\n",
    "print(\"train accuracy: {:.4f}, val accuracy {:.4f}, test accuracy {:.4f}\".\n",
    "      format(train_score_lgbm, val_score_lgbm, test_score_lgbm))\n",
    "print(\"precision : {:.4f}, recall : {:.4f}, f1score : {:.4f}, roc : {:.4f}\".\n",
    "     format (ps, rs, fs, roc))\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1296 candidates, totalling 6480 fits\n",
      "[LightGBM] [Warning] Unknown parameter: eval_metric\n",
      "Best Estimator: LGBMClassifier(colsample_bytree=0.41, eval_metric='logloss',\n",
      "               learning_rate=0.004, max_delta_step=0.5, max_depth=2,\n",
      "               metric='loss', n_estimators=5, num_iterations=500, num_leaves=3,\n",
      "               objective='binary', random_state=42, subsample=0.005)\n",
      "Best Parameters: {'boosting_type': 'gbdt', 'colsample_bytree': 0.41, 'learning_rate': 0.004, 'max_depth': 2, 'metric': 'loss', 'n_estimators': 5, 'num_leaves': 3, 'objective': 'binary', 'subsample': 0.005}\n",
      "Best Score: 0.9333333333333332, Best Index: 0\n",
      "train accuracy: 0.8081, val accuracy 0.7442, test accuracy 0.7593\n",
      "precision : 1.0000, recall : 0.2353, f1score : 0.3810, roc : 0.6176\n",
      "[[37  0]\n",
      " [13  4]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kange\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\lightgbm\\engine.py:177: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
      "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n"
     ]
    }
   ],
   "source": [
    "params = {\n",
    "    \"boosting_type\" : ['gbdt'],\n",
    "    \"max_depth\": [2, 3, 4],\n",
    "    \"num_leaves\": [3, 4, 5],\n",
    "    \"learning_rate\": [0.004, 0.005, 0.006],\n",
    "    \"objective\": ['binary'],\n",
    "    \"metric\": ['loss'],\n",
    "    \"colsample_bytree\": [0.41, 0.42, 0.43, 0.45],\n",
    "    \"subsample\": [0.005, 0.01, 0.015],\n",
    "    \"n_estimators\": [5, 10, 20, 30],\n",
    "#     \"num_iterations\": [100, 200, 300, 400, 500],\n",
    "#     \"num_class\": [1]\n",
    "#     \"metric\" : \"rmse\",\n",
    "#     \"bagging_frequency\" : 5,\n",
    "#     \"bagging_seed\" : 2018,\n",
    "#     \"verbosity\" : -1,\n",
    "\n",
    "#     # Selected rounded-off params\n",
    "#     'bagging_fraction': 0.7,\n",
    "#     'feature_fraction': 0.1,\n",
    "#     'lambda_l1': 1,\n",
    "#     'lambda_l2': 0,\n",
    "#     'max_depth': 9,\n",
    "#     'min_child_weight': 5,\n",
    "#     'min_split_gain': 0,\n",
    "#     'num_leaves': 24\n",
    "}\n",
    "lgbm = None\n",
    "lgbmgs = None\n",
    "\n",
    "lgbm = lightgbm.LGBMClassifier( random_state=42, \n",
    "                               learning_rate =0.22, # default = 0.1\n",
    "                               num_iterations=500, # default=100\n",
    "                               max_depth=5, # <= 0 means no limit, default=-1\n",
    "#                                bagging_fraction=0.9, # 0.0 < bagging_fraction <= 1.0, default=1\n",
    "#                                feature_fraction=0.8, # 0.0 < feature_fraction <= 1.0, default=1\n",
    "                               objective= 'binary', \n",
    "#                                num_threads = 0, # == nthread=4, # Gpu 수, default=0, 자동 검색후 적용\n",
    "                               max_delta_step = 0.5, # best value found, default = 0\n",
    "#                                scale_pos_weight=40, # class imbalance 경감, scale_pos_weight > 0.0, default=1.0\n",
    "                               eval_metric = 'logloss'\n",
    "                                )\n",
    "\n",
    "lgbmgs = GridSearchCV(estimator = lgbm,\n",
    "                      param_grid = params,\n",
    "                      cv = StratifiedKFold(n_splits=5),\n",
    "                      scoring = 'precision', \n",
    "                      verbose = 1,\n",
    "                      n_jobs=4,\n",
    "                      )\n",
    "\n",
    "lgbmgs.fit(train_scaled, train_target)\n",
    "    \n",
    "print(\"Best Estimator: {}\".format(lgbmgs.best_estimator_)) \n",
    "print(\"Best Parameters: {}\".format(lgbmgs.best_params_))  # 최적 파라미터.\n",
    "print('Best Score: {}, Best Index: {}'.format(lgbmgs.best_score_ , lgbmgs.best_index_))  # 교차검증된 점수를 보여줌.\n",
    "\n",
    "model = lgbmgs.best_estimator_  # 최적의 파라미터로 모델 생성\n",
    "y_pred = model.predict(test_scaled)\n",
    "\n",
    "train_score = model.score(train_scaled, train_target)\n",
    "val_score = model.score(val_scaled, val_target)\n",
    "test_score = model.score(test_scaled, test_target)\n",
    "ps, rs, fs, roc = predict_p(test_target, model.predict(test_scaled))\n",
    "cm = confusion_matrix(test_target, model.predict(test_scaled))\n",
    "\n",
    "print(\"train accuracy: {:.4f}, val accuracy {:.4f}, test accuracy {:.4f}\".\n",
    "      format(train_score, val_score, test_score))\n",
    "print(\"precision : {:.4f}, recall : {:.4f}, f1score : {:.4f}, roc : {:.4f}\".\n",
    "     format (ps, rs, fs, roc))\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean_test_score  :  [0.5        0.91806667        nan 0.5        0.90966667        nan\n",
      " 0.5        0.91546667        nan 0.5        0.91293333        nan\n",
      " 0.5        0.90963333        nan 0.5        0.9145            nan\n",
      " 0.5        0.90633333        nan 0.5        0.9128            nan\n",
      " 0.5        0.9112            nan 0.5        0.91456667        nan\n",
      " 0.5        0.9088            nan 0.5        0.91283333        nan\n",
      " 0.5        0.90803333        nan 0.5        0.91466667        nan\n",
      " 0.5        0.90866667        nan 0.5        0.90796667        nan\n",
      " 0.5        0.91706667        nan 0.5        0.92043333        nan\n",
      " 0.5        0.90883333        nan 0.5        0.90966667        nan\n",
      " 0.5        0.91113333        nan 0.5        0.89726667        nan\n",
      " 0.5        0.89386667        nan 0.5        0.9096            nan\n",
      " 0.5        0.90566667        nan 0.5        0.90386667        nan\n",
      " 0.5        0.90383333        nan 0.5        0.91293333        nan\n",
      " 0.5        0.90873333        nan 0.5        0.9136            nan\n",
      " 0.5        0.91213333        nan 0.5        0.90473333        nan\n",
      " 0.5        0.91293333        nan 0.5        0.9113            nan\n",
      " 0.5        0.91946667        nan 0.5        0.92043333        nan\n",
      " 0.5        0.91806667        nan 0.5        0.91126667        nan\n",
      " 0.5        0.91123333        nan 0.5        0.9063            nan\n",
      " 0.5        0.9079            nan 0.5        0.91213333        nan\n",
      " 0.5        0.90473333        nan 0.5        0.91056667        nan\n",
      " 0.5        0.91453333        nan 0.5        0.9121            nan\n",
      " 0.5        0.91123333        nan 0.5        0.90786667        nan\n",
      " 0.5        0.91213333        nan 0.5        0.90473333        nan\n",
      " 0.5        0.9071            nan 0.5        0.91213333        nan\n",
      " 0.5        0.9178            nan 0.5        0.92123333        nan\n",
      " 0.5        0.91806667        nan 0.5        0.91126667        nan\n",
      " 0.5        0.91123333        nan 0.5        0.9063            nan\n",
      " 0.5        0.9054            nan 0.5        0.91213333        nan\n",
      " 0.5        0.90473333        nan 0.5        0.91056667        nan\n",
      " 0.5        0.91133333        nan 0.5        0.9121            nan\n",
      " 0.5        0.91123333        nan 0.5        0.90786667        nan\n",
      " 0.5        0.91213333        nan 0.5        0.90473333        nan\n",
      " 0.5        0.9071            nan 0.5        0.91213333        nan\n",
      " 0.5        0.9178            nan 0.5        0.92123333        nan\n",
      " 0.5        0.91383333        nan 0.5        0.92016667        nan\n",
      " 0.5        0.90876667        nan 0.5        0.91373333        nan\n",
      " 0.5        0.91866667        nan 0.5        0.91713333        nan\n",
      " 0.5        0.91706667        nan 0.5        0.91056667        nan\n",
      " 0.5        0.918             nan 0.5        0.9172            nan\n",
      " 0.5        0.9172            nan 0.5        0.91886667        nan\n",
      " 0.5        0.90806667        nan 0.5        0.91626667        nan\n",
      " 0.5        0.9089            nan 0.5        0.91143333        nan\n",
      " 0.5        0.9114            nan 0.5        0.91553333        nan\n",
      " 0.5        0.9057            nan 0.5        0.90633333        nan\n",
      " 0.5        0.9146            nan 0.5        0.9145            nan\n",
      " 0.5        0.9161            nan 0.5        0.91293333        nan\n",
      " 0.5        0.9172            nan 0.5        0.9072            nan\n",
      " 0.5        0.92206667        nan 0.5        0.91543333        nan\n",
      " 0.5        0.91956667        nan 0.5        0.91726667        nan\n",
      " 0.5        0.91293333        nan 0.5        0.9056            nan\n",
      " 0.5        0.91296667        nan 0.5        0.91566667        nan\n",
      " 0.5        0.9014            nan 0.5        0.90813333        nan\n",
      " 0.5        0.90896667        nan 0.5        0.91553333        nan\n",
      " 0.5        0.9162            nan 0.5        0.91193333        nan\n",
      " 0.5        0.91466667        nan 0.5        0.91696667        nan\n",
      " 0.5        0.918             nan 0.5        0.90886667        nan\n",
      " 0.5        0.92706667        nan 0.5        0.91713333        nan\n",
      " 0.5        0.91956667        nan 0.5        0.91886667        nan\n",
      " 0.5        0.91293333        nan 0.5        0.9056            nan\n",
      " 0.5        0.91296667        nan 0.5        0.91406667        nan\n",
      " 0.5        0.9014            nan 0.5        0.90643333        nan\n",
      " 0.5        0.90896667        nan 0.5        0.91553333        nan\n",
      " 0.5        0.9162            nan 0.5        0.91193333        nan\n",
      " 0.5        0.91466667        nan 0.5        0.91696667        nan\n",
      " 0.5        0.918             nan 0.5        0.90886667        nan\n",
      " 0.5        0.92706667        nan 0.5        0.91713333        nan\n",
      " 0.5        0.91956667        nan 0.5        0.91886667        nan\n",
      " 0.5        0.91293333        nan 0.5        0.9056            nan\n",
      " 0.5        0.91296667        nan 0.5        0.91406667        nan\n",
      " 0.5        0.9014            nan 0.5        0.90643333        nan]\n",
      "std_test_score  :  [0.         0.05141494        nan 0.         0.05938677        nan\n",
      " 0.         0.06358454        nan 0.         0.06214952        nan\n",
      " 0.         0.07168088        nan 0.         0.06333386        nan\n",
      " 0.         0.06287898        nan 0.         0.07082705        nan\n",
      " 0.         0.06620955        nan 0.         0.06517511        nan\n",
      " 0.         0.06366914        nan 0.         0.07063521        nan\n",
      " 0.         0.06915107        nan 0.         0.06263465        nan\n",
      " 0.         0.07132523        nan 0.         0.06577524        nan\n",
      " 0.         0.05580846        nan 0.         0.06146838        nan\n",
      " 0.         0.06809887        nan 0.         0.0697576         nan\n",
      " 0.         0.07091728        nan 0.         0.06829264        nan\n",
      " 0.         0.06742645        nan 0.         0.0641076         nan\n",
      " 0.         0.06805398        nan 0.         0.06688659        nan\n",
      " 0.         0.06477988        nan 0.         0.06573352        nan\n",
      " 0.         0.06991523        nan 0.         0.06904794        nan\n",
      " 0.         0.06664461        nan 0.         0.06457335        nan\n",
      " 0.         0.06274911        nan 0.         0.06244201        nan\n",
      " 0.         0.06074742        nan 0.         0.0605792         nan\n",
      " 0.         0.06177725        nan 0.         0.06354286        nan\n",
      " 0.         0.06911003        nan 0.         0.06823981        nan\n",
      " 0.         0.07404508        nan 0.         0.06706012        nan\n",
      " 0.         0.06715145        nan 0.         0.06258271        nan\n",
      " 0.         0.06771635        nan 0.         0.06360124        nan\n",
      " 0.         0.06727701        nan 0.         0.06536686        nan\n",
      " 0.         0.06664461        nan 0.         0.06457335        nan\n",
      " 0.         0.06265652        nan 0.         0.06281281        nan\n",
      " 0.         0.06293404        nan 0.         0.06153785        nan\n",
      " 0.         0.06177725        nan 0.         0.06354286        nan\n",
      " 0.         0.06911003        nan 0.         0.06823981        nan\n",
      " 0.         0.07292466        nan 0.         0.06706012        nan\n",
      " 0.         0.06715145        nan 0.         0.06258271        nan\n",
      " 0.         0.06564966        nan 0.         0.06360124        nan\n",
      " 0.         0.06727701        nan 0.         0.06536686        nan\n",
      " 0.         0.06664461        nan 0.         0.06457335        nan\n",
      " 0.         0.06265652        nan 0.         0.06281281        nan\n",
      " 0.         0.06293404        nan 0.         0.06153785        nan\n",
      " 0.         0.05609159        nan 0.         0.06456349        nan\n",
      " 0.         0.07349079        nan 0.         0.06899973        nan\n",
      " 0.         0.06325706        nan 0.         0.06208269        nan\n",
      " 0.         0.06772597        nan 0.         0.06578928        nan\n",
      " 0.         0.06994355        nan 0.         0.06919613        nan\n",
      " 0.         0.0686925         nan 0.         0.06061751        nan\n",
      " 0.         0.06326616        nan 0.         0.06375148        nan\n",
      " 0.         0.06151654        nan 0.         0.05651328        nan\n",
      " 0.         0.05883599        nan 0.         0.05991266        nan\n",
      " 0.         0.06259716        nan 0.         0.06670591        nan\n",
      " 0.         0.0609504         nan 0.         0.07393496        nan\n",
      " 0.         0.07198986        nan 0.         0.06355335        nan\n",
      " 0.         0.06249928        nan 0.         0.06340552        nan\n",
      " 0.         0.05970209        nan 0.         0.07008817        nan\n",
      " 0.         0.0677641         nan 0.         0.06317653        nan\n",
      " 0.         0.06513837        nan 0.         0.06277461        nan\n",
      " 0.         0.06449198        nan 0.         0.05671527        nan\n",
      " 0.         0.06481346        nan 0.         0.05723775        nan\n",
      " 0.         0.06050798        nan 0.         0.05784869        nan\n",
      " 0.         0.06110786        nan 0.         0.07479903        nan\n",
      " 0.         0.06092764        nan 0.         0.06588141        nan\n",
      " 0.         0.06336876        nan 0.         0.061184          nan\n",
      " 0.         0.06312955        nan 0.         0.07134684        nan\n",
      " 0.         0.0677641         nan 0.         0.06482484        nan\n",
      " 0.         0.06513837        nan 0.         0.06277461        nan\n",
      " 0.         0.06449198        nan 0.         0.05484702        nan\n",
      " 0.         0.06481346        nan 0.         0.06098994        nan\n",
      " 0.         0.06050798        nan 0.         0.05784869        nan\n",
      " 0.         0.06110786        nan 0.         0.07479903        nan\n",
      " 0.         0.06092764        nan 0.         0.06588141        nan\n",
      " 0.         0.06336876        nan 0.         0.061184          nan\n",
      " 0.         0.06312955        nan 0.         0.07134684        nan\n",
      " 0.         0.0677641         nan 0.         0.06482484        nan\n",
      " 0.         0.06513837        nan 0.         0.06277461        nan\n",
      " 0.         0.06449198        nan 0.         0.05484702        nan\n",
      " 0.         0.06481346        nan 0.         0.06098994        nan]\n"
     ]
    }
   ],
   "source": [
    "for i in ['mean_test_score', 'std_test_score']:\n",
    "        print(i,\" : \", gsearch1.cv_results_[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['mean_fit_time', 'std_fit_time', 'mean_score_time', 'std_score_time', 'param_max_depth', 'param_min_child_weight', 'params', 'split0_test_score', 'split1_test_score', 'split2_test_score', 'split3_test_score', 'split4_test_score', 'mean_test_score', 'std_test_score', 'rank_test_score'])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gsearch1.cv_results_.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train accuracy: 0.9884, val accuracy 0.8140, test accuracy 0.8148\n",
      "precision : 0.6667, recall : 0.8235, f1score : 0.7368, roc : 0.8172\n",
      "[[30  7]\n",
      " [ 3 14]]\n"
     ]
    }
   ],
   "source": [
    "# use best fit model using .best_estimator\n",
    "\n",
    "best_xgb = gsearch1.best_estimator_\n",
    "y_pred = best_xgb.predict(val_scaled)\n",
    "# val_error = mean_squared_error(val_target, y_pred) # 책에 없음\n",
    "# print(\"Validation MSE:\", val_error)           # 책에 없음\n",
    "\n",
    "train_score_xgb = best_xgb.score(train_scaled, train_target)\n",
    "val_score_xgb = best_xgb.score(val_scaled, val_target)\n",
    "test_score_xgb = best_xgb.score(test_scaled, test_target)\n",
    "ps, rs, fs, roc = predict_p(test_target, best_xgb.predict(test_scaled))\n",
    "cm = confusion_matrix(test_target, best_xgb.predict(test_scaled))\n",
    "\n",
    "print(\"train accuracy: {:.4f}, val accuracy {:.4f}, test accuracy {:.4f}\".\n",
    "      format(train_score_xgb, val_score_xgb, test_score_xgb))\n",
    "print(\"precision : {:.4f}, recall : {:.4f}, f1score : {:.4f}, roc : {:.4f}\".\n",
    "     format (ps, rs, fs, roc))\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Fitting 5 folds for each of 18000 candidates, totalling 90000 fits\n",
    "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=0.5 will be ignored. Current value: feature_fraction=0.8\n",
    "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=0.5 will be ignored. Current value: bagging_fraction=0.9\n",
    "Best Estimator: LGBMClassifier(bagging_fraction=0.9, colsample_bytree=0.5, feature_fraction=0.8,\n",
    "               learning_rate=0.05, max_delta_step=0.5, max_depth=2,\n",
    "               metric='auc', num_iterations=500, num_leaves=4,\n",
    "               objective='binary', random_state=42, subsample=0.5)\n",
    "Best Parameters: {'boosting_type': 'gbdt', 'colsample_bytree': 0.5, 'learning_rate': 0.05, 'max_depth': 2, 'metric': 'auc', 'n_estimators': 100, 'num_leaves': 4, 'objective': 'binary', 'subsample': 0.5}\n",
    "Best Score: 0.8375180375180376, Best Index: 726\n",
    "train accuracy: 1.0000, val accuracy 0.7907, test accuracy 0.8148\n",
    "precision : 0.7059, recall : 0.7059, f1score : 0.7059, roc : 0.7854\n",
    "[[32  5]\n",
    " [ 5 12]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Fitting 5 folds for each of 7200 candidates, totalling 36000 fits\n",
    "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=0.3 will be ignored. Current value: feature_fraction=0.8\n",
    "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=0.3 will be ignored. Current value: bagging_fraction=0.9\n",
    "Best Estimator: LGBMClassifier(bagging_fraction=0.9, colsample_bytree=0.3, feature_fraction=0.8,\n",
    "               max_delta_step=0.5, max_depth=4, metric='rmse', n_estimators=80,\n",
    "               num_iterations=500, num_leaves=6, objective='binary',\n",
    "               random_state=42, subsample=0.3)\n",
    "Best Parameters: {'boosting_type': 'gbdt', 'colsample_bytree': 0.3, 'learning_rate': 0.1, 'max_depth': 4, 'metric': 'rmse', 'n_estimators': 80, 'num_leaves': 6, 'objective': 'binary', 'subsample': 0.3}\n",
    "Best Score: 0.8377777777777778, Best Index: 915\n",
    "train accuracy: 1.0000, val accuracy 0.7907, test accuracy 0.8519\n",
    "precision : 0.7368, recall : 0.8235, f1score : 0.7778, roc : 0.8442\n",
    "[[32  5]\n",
    " [ 3 14]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Fitting 5 folds for each of 12500 candidates, totalling 62500 fits\n",
    "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=0.1 will be ignored. Current value: feature_fraction=0.8\n",
    "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=0.1 will be ignored. Current value: bagging_fraction=0.9\n",
    "Best Estimator: LGBMClassifier(bagging_fraction=0.9, colsample_bytree=0.1, feature_fraction=0.8,\n",
    "               learning_rate=0.07, max_delta_step=0.5, max_depth=5,\n",
    "               metric='rmse', n_estimators=50, num_iterations=500, num_leaves=6,\n",
    "               objective='binary', random_state=42, subsample=0.1)\n",
    "Best Parameters: {'boosting_type': 'gbdt', 'colsample_bytree': 0.1, 'learning_rate': 0.07, 'max_depth': 5, 'metric': 'rmse', 'n_estimators': 50, 'num_leaves': 6, 'objective': 'binary', 'subsample': 0.1}\n",
    "Best Score: 0.8377777777777778, Best Index: 315\n",
    "train accuracy: 1.0000, val accuracy 0.8140, test accuracy 0.8519\n",
    "precision : 0.7368, recall : 0.8235, f1score : 0.7778, roc : 0.8442\n",
    "[[32  5]\n",
    " [ 3 14]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fitting 5 folds for each of 12500 candidates, totalling 62500 fits\n",
    "Best Estimator: LGBMClassifier(colsample_bytree=0.4, learning_rate=0.03, max_delta_step=0.5,\n",
    "               max_depth=3, metric='loss', n_estimators=30, num_iterations=500,\n",
    "               num_leaves=4, objective='binary', random_state=42,\n",
    "               subsample=0.1)\n",
    "Best Parameters: {'boosting_type': 'gbdt', 'colsample_bytree': 0.4, 'learning_rate': 0.03, 'max_depth': 3, 'metric': 'loss', 'n_estimators': 30, 'num_leaves': 4, 'objective': 'binary', 'subsample': 0.1}\n",
    "Best Score: 0.8342857142857143, Best Index: 7605\n",
    "train accuracy: 1.0000, val accuracy 0.7907, test accuracy 0.8704\n",
    "precision : 0.7500, recall : 0.8824, f1score : 0.8108, roc : 0.8736\n",
    "[[32  5]\n",
    " [ 2 15]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Fitting 5 folds for each of 13500 candidates, totalling 67500 fits\n",
    "[LightGBM] [Warning] Unknown parameter: eval_metric\n",
    "Best Estimator: LGBMClassifier(colsample_bytree=0.35, eval_metric='logloss', learning_rate=0.02,\n",
    "               max_delta_step=0.5, max_depth=3, metric='loss', n_estimators=10,\n",
    "               num_iterations=100, num_leaves=4, objective='binary',\n",
    "               random_state=42, subsample=0.05)\n",
    "Best Parameters: {'boosting_type': 'gbdt', 'colsample_bytree': 0.35, 'learning_rate': 0.02, 'max_depth': 3, 'metric': 'loss', 'n_estimators': 10, 'num_iterations': 100, 'num_leaves': 4, 'objective': 'binary', 'subsample': 0.05}\n",
    "Best Score: 1.0, Best Index: 1204\n",
    "train accuracy: 0.7849, val accuracy 0.7442, test accuracy 0.7037\n",
    "precision : 1.0000, recall : 0.0588, f1score : 0.1111, roc : 0.5294\n",
    "[[37  0]\n",
    " [16  1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Fitting 5 folds for each of 2700 candidates, totalling 13500 fits\n",
    "[LightGBM] [Warning] Unknown parameter: eval_metric\n",
    "Best Estimator: LGBMClassifier(colsample_bytree=0.45, eval_metric='logloss', learning_rate=0.01,\n",
    "               max_delta_step=0.5, max_depth=2, metric='loss', n_estimators=10,\n",
    "               num_iterations=300, num_leaves=3, objective='binary',\n",
    "               random_state=42, subsample=0.05)\n",
    "Best Parameters: {'boosting_type': 'gbdt', 'colsample_bytree': 0.45, 'learning_rate': 0.01, 'max_depth': 2, 'metric': 'loss', 'n_estimators': 10, 'num_leaves': 3, 'objective': 'binary', 'subsample': 0.05}\n",
    "Best Score: 0.8638888888888889, Best Index: 1800\n",
    "train accuracy: 0.9128, val accuracy 0.8140, test accuracy 0.8519\n",
    "precision : 0.8000, recall : 0.7059, f1score : 0.7500, roc : 0.8124\n",
    "[[34  3]\n",
    " [ 5 12]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Fitting 5 folds for each of 2880 candidates, totalling 14400 fits\n",
    "[LightGBM] [Warning] Unknown parameter: eval_metric\n",
    "Best Estimator: LGBMClassifier(colsample_bytree=0.4, eval_metric='logloss', learning_rate=0.01,\n",
    "               max_delta_step=0.5, max_depth=2, metric='loss', n_estimators=5,\n",
    "               num_iterations=200, num_leaves=3, objective='binary',\n",
    "               random_state=42, subsample=0.03)\n",
    "Best Parameters: {'boosting_type': 'gbdt', 'colsample_bytree': 0.4, 'learning_rate': 0.01, 'max_depth': 2, 'metric': 'loss', 'n_estimators': 5, 'num_leaves': 3, 'objective': 'binary', 'subsample': 0.03}\n",
    "Best Score: 0.9333333333333332, Best Index: 144\n",
    "train accuracy: 0.8023, val accuracy 0.7442, test accuracy 0.7593\n",
    "precision : 1.0000, recall : 0.2353, f1score : 0.3810, roc : 0.6176\n",
    "[[37  0]\n",
    " [13  4]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Fitting 5 folds for each of 2880 candidates, totalling 14400 fits\n",
    "[LightGBM] [Warning] Unknown parameter: eval_metric\n",
    "Best Estimator: LGBMClassifier(colsample_bytree=0.42, eval_metric='logloss', learning_rate=0.01,\n",
    "               max_delta_step=0.5, max_depth=2, metric='loss', n_estimators=5,\n",
    "               num_iterations=300, num_leaves=3, objective='binary',\n",
    "               random_state=42, subsample=0.03)\n",
    "Best Parameters: {'boosting_type': 'gbdt', 'colsample_bytree': 0.42, 'learning_rate': 0.01, 'max_depth': 2, 'metric': 'loss', 'n_estimators': 5, 'num_leaves': 3, 'objective': 'binary', 'subsample': 0.03}\n",
    "Best Score: 0.8638888888888889, Best Index: 720\n",
    "train accuracy: 0.9070, val accuracy 0.8140, test accuracy 0.8519\n",
    "precision : 0.8000, recall : 0.7059, f1score : 0.7500, roc : 0.8124\n",
    "[[34  3]\n",
    " [ 5 12]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Fitting 5 folds for each of 1296 candidates, totalling 6480 fits\n",
    "[LightGBM] [Warning] Unknown parameter: eval_metric\n",
    "Best Estimator: LGBMClassifier(colsample_bytree=0.42, eval_metric='logloss',\n",
    "               learning_rate=0.005, max_delta_step=0.5, max_depth=2,\n",
    "               metric='loss', n_estimators=5, num_iterations=500, num_leaves=3,\n",
    "               objective='binary', random_state=42, subsample=0.01)\n",
    "Best Parameters: {'boosting_type': 'gbdt', 'colsample_bytree': 0.42, 'learning_rate': 0.005, 'max_depth': 2, 'metric': 'loss', 'n_estimators': 5, 'num_leaves': 3, 'objective': 'binary', 'subsample': 0.01}\n",
    "Best Score: 0.8355555555555556, Best Index: 432\n",
    "train accuracy: 0.8895, val accuracy 0.8372, test accuracy 0.8519\n",
    "precision : 0.8462, recall : 0.6471, f1score : 0.7333, roc : 0.7965\n",
    "[[35  2]\n",
    " [ 6 11]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Fitting 5 folds for each of 1296 candidates, totalling 6480 fits\n",
    "[LightGBM] [Warning] Unknown parameter: eval_metric\n",
    "Best Estimator: LGBMClassifier(colsample_bytree=0.41, eval_metric='logloss',\n",
    "               learning_rate=0.004, max_delta_step=0.5, max_depth=2,\n",
    "               metric='loss', n_estimators=5, num_iterations=500, num_leaves=3,\n",
    "               objective='binary', random_state=42, subsample=0.005)\n",
    "Best Parameters: {'boosting_type': 'gbdt', 'colsample_bytree': 0.41, 'learning_rate': 0.004, 'max_depth': 2, 'metric': 'loss', 'n_estimators': 5, 'num_leaves': 3, 'objective': 'binary', 'subsample': 0.005}\n",
    "Best Score: 0.9333333333333332, Best Index: 0\n",
    "train accuracy: 0.8081, val accuracy 0.7442, test accuracy 0.7593\n",
    "precision : 1.0000, recall : 0.2353, f1score : 0.3810, roc : 0.6176\n",
    "[[37  0]\n",
    " [13  4]]"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "5-1 결정 트리.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
