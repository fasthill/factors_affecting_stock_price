{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/fasthill/ML-DL-study-alone/blob/main/5-1%20%EA%B2%B0%EC%A0%95%20%ED%8A%B8%EB%A6%AC.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5uA_6TRHEMHV"
   },
   "source": [
    "## Analyse from LightGBM\n",
    "\n",
    "### train, val, test data로 분류"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "26KAIfzEEMHc"
   },
   "source": [
    "<table align=\"left\">\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/rickiepark/hg-mldl/blob/master/5-1.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />구글 코랩에서 실행하기</a>\n",
    "  </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "rIXeXBVTfZrS"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from functools import reduce\n",
    "import datetime\n",
    "import os, re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "import xgboost\n",
    "from lightgbm import LGBMClassifier\n",
    "import lightgbm\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skopt import BayesSearchCV\n",
    "# parameter ranges are specified by one of below\n",
    "from skopt.space import Real, Categorical, Integer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression, SGDClassifier, SGDRegressor\n",
    "from sklearn.model_selection import cross_validate, train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import average_precision_score\n",
    "from sklearn.metrics import make_scorer\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten, Dropout\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAIN_START_DATE = datetime.date(2022, 3, 2)\n",
    "# TRAIN_END_DATE = datetime.date(2023, 3, 31)\n",
    "# VAL_END_DATE = datetime.date(2023, 4, 28)\n",
    "TRAIN_START_DATE = datetime.date(2022, 5, 2)\n",
    "TRAIN_END_DATE = datetime.date(2023, 4, 28)\n",
    "VAL_END_DATE = datetime.date(2023, 5, 26)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAIN_START_DATE = datetime.date(2022, 5, 2)\n",
    "TRAIN_START_DATE = datetime.date(2022, 7, 6)  # 7월 6일 이후부터 3월 28일까지 train data (cv, validate data까지)\n",
    "# TRAIN_START_DATE = datetime.date(2022, 9, 7)\n",
    "# TRAIN_START_DATE = datetime.date(2022, 10, 4)\n",
    "TRAIN_END_DATE = datetime.date(2023, 3, 28) # 3월 28일 이후부터 5월 12일까지 test data\n",
    "VAL_END_DATE = datetime.date(2023, 5, 12)  # 5월 12일 이후부터 현재까지 실제 진행 data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# write list, dictionary to pickle\n",
    "def save_to_pickle(path, filename):\n",
    "    open_file = open(path, \"wb\")\n",
    "    pickle.dump(filename, open_file)\n",
    "    open_file.close()\n",
    "\n",
    "# read list, dictionary from pickle\n",
    "def load_from_pickle(path):\n",
    "    open_file = open(path, \"rb\")\n",
    "    loaded_file = pickle.load(open_file)\n",
    "    open_file.close()\n",
    "    return loaded_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "# write list, dictionary to csv\n",
    "# path = './xxx/', my_dict = filename\n",
    "\n",
    "def save_dict_to_csv(path, my_dict):\n",
    "    fields=my_dict.keys()\n",
    "    values = my_dict.values()\n",
    "    pd.DataFrame([fields, values], index=['parameter','value']).T.to_csv(path)\n",
    "#     df = pd.DataFrame.from_dict(my_dict, orient='index') \n",
    "#     df.to_csv (path, index=False, header=True)  \n",
    "    \n",
    "def save_list_to_csv(path, my_list):\n",
    "    df = pd.DataFrame(my_list, columns=['columns'])\n",
    "    df.to_csv (path, index=False, header=True) \n",
    "    \n",
    "# def load_dict_from_csv(path):\n",
    "#     df = pd.read_csv(path, header=None)\n",
    "#     my_dict = df.to_dict()\n",
    "#     return my_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_p(test_target, y_predict_list): \n",
    "    ps = precision_score(test_target, y_predict_list)\n",
    "    rs = recall_score(test_target, y_predict_list)\n",
    "    fs = f1_score(test_target, y_predict_list)\n",
    "    roc = roc_auc_score(test_target, y_predict_list)\n",
    "    collect_list = [ps, rs, fs, roc]\n",
    "    return collect_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_df_from_estimator(estimator, num):\n",
    "    df_t = pd.DataFrame.from_dict(estimator, orient='index')\n",
    "    df_t.columns = [f'iter_{num}']\n",
    "    df_t.index.name = 'parameter'\n",
    "    return df_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_results(lgbm, model, train_scaled, val_scaled, test_scaled, train_target, val_target, test_target):\n",
    "# model = lgbmgs.best_estimator_  # 최적의 파라미터로 모델 생성\n",
    "    y_predict_train = model.predict(train_scaled)\n",
    "    y_predict_val = model.predict(val_scaled)\n",
    "    y_predict_test = model.predict(test_scaled)\n",
    "    result_dict= {}\n",
    "    result_dict['best_score'] = lgbm.best_score_ \n",
    "    result_dict['best_index'] = lgbm.best_index_\n",
    "    result_dict['acc_train'] = model.score(train_scaled, train_target)\n",
    "    result_dict['acc_val'] = model.score(val_scaled, val_target)\n",
    "    result_dict['acc_test'] = model.score(test_scaled, test_target)\n",
    "    result_dict['train_precision'] = precision_score(train_target, y_predict_train)\n",
    "    cm = confusion_matrix(train_target, y_predict_train)\n",
    "    result_dict['train_tn'] = cm[0,0]\n",
    "    result_dict['train_fp'] = cm[0,1]\n",
    "    result_dict['train_fn'] = cm[1,0]\n",
    "    result_dict['train_tp'] = cm[1,1]\n",
    "    result_dict['val_precision'] = precision_score(val_target, y_predict_val)\n",
    "    cm = confusion_matrix(val_target, y_predict_val)\n",
    "    result_dict['val_tn'] = cm[0,0]\n",
    "    result_dict['val_fp'] = cm[0,1]\n",
    "    result_dict['val_fn'] = cm[1,0]\n",
    "    result_dict['val_tp'] = cm[1,1]\n",
    "    result_dict['test_precision'] = precision_score(test_target, y_predict_test)\n",
    "    result_dict['recall'] = recall_score(test_target, y_predict_test)\n",
    "    result_dict['f1score'] = f1_score(test_target, y_predict_test)\n",
    "    result_dict['roc'] = roc_auc_score(test_target, y_predict_test)\n",
    "    cm = confusion_matrix(test_target, y_predict_test)\n",
    "    result_dict['test_tn'] = cm[0,0]\n",
    "    result_dict['test_fp'] = cm[0,1]\n",
    "    result_dict['test_fn'] = cm[1,0]\n",
    "    result_dict['test_tp'] = cm[1,1]\n",
    "#     result_dict['precision_neg'] = cm[0,0] / (cm[0,0] + cm[1,0])\n",
    "    return result_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_best_results(com_name, model, scaler, columns, df_base, df_params_search):\n",
    "    joblib.dump(model, f'{directory_for_model}{com_name}/best_model.pkl') # estimaor 저장\n",
    "    joblib.dump(scaler, f'{directory_for_model}{com_name}/best_scaler.pkl') # scaler 저장\n",
    "    save_to_pickle(f'{directory_for_model}{com_name}/best_model_p.pkl', model) # save with pickle.dump \n",
    "    save_to_pickle(f'{directory_for_model}{com_name}/best_scaler_p.pkl', scaler) # save with pickle.dump \n",
    "    save_to_pickle(f'{directory_for_model}{com_name}/best_columns.pkl', new_col) # save with pickle.dump \n",
    "    save_list_to_csv(f'{directory_for_model}{com_name}/best_columns.csv', new_col)\n",
    "    df_base.to_pickle(f'{directory_for_model}{com_name}/best_result.pkl')\n",
    "    df_params_search.to_pickle(f'{directory_for_model}{com_name}/best_params.pkl')\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_dict(*dicts):\n",
    "    dsum = {}\n",
    "    d_k = []\n",
    "    d_v = []\n",
    "    for dic in dicts:\n",
    "        d_k.extend(list(dic.keys()))  # sum keys list\n",
    "        d_v.extend(list(dic.values()))  # sum values list\n",
    "    for i in range(len(d_k)):\n",
    "        dsum[d_k[i]] = d_v[i]\n",
    "    return dsum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_combine_sorted(df_base_l, df_params_l):\n",
    "    o_num = list(df_base_l.index).index('best_score')\n",
    "    df1 = df_base.iloc[:o_num, :]\n",
    "    df2 = df_base.iloc[o_num:, :]\n",
    "    df3 = pd.concat([df1, df_params_l], axis=0)\n",
    "    df3.sort_index(axis=0, inplace=True)\n",
    "    df_sorted = pd.concat([df3, df2], axis=0)\n",
    "    return df_sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_std(lgbmbs_cv_results, num):\n",
    "    df_result =  pd.DataFrame(lgbmbs_cv_results)\n",
    "    df_sel = df_result.loc[:, ['mean_test_score', 'std_test_score', 'rank_test_score']].sort_values(by='mean_test_score', ascending=False)\n",
    "    df_sel = df_sel.reset_index()\n",
    "    df_sel = df_sel.set_index('rank_test_score')\n",
    "    df_sel = df_sel.reset_index(drop=True)\n",
    "    df_sel.columns = ['best_index', 'mean_test_score', 'std_test_score']\n",
    "    ddict = {'best_index': f'iter{iter}', 'mean_test_score': f'iter{iter}', 'std_test_score' : f'iter{iter}'}\n",
    "    gdict = dict((*df_sel.groupby(by=ddict, axis=1),))\n",
    "    df_return = pd.concat(gdict, axis=1)\n",
    "    return df_return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "\n",
    "module_path = os.path.abspath(os.path.join('../..')) # 현재 폴더로 이동\n",
    "if module_path+\"\\\\data\\\\base_data\\\\common_data\" not in sys.path:\n",
    "    sys.path.append(module_path+\"\\\\data\\\\base_data\\\\common_data\") #  공통으로 사용하는 각종 리스트, 코드 등 \n",
    "    \n",
    "import common_data as cd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_style(df_s, row_index_l):\n",
    "#     xin = [list(df_s.index).index(x) for x in row_index_l]\n",
    "    xin = []\n",
    "    for x in list(df_s.index):\n",
    "        try:\n",
    "            if x in row_index_l:\n",
    "                xin.append(list(df_s.index).index(x))\n",
    "        except:\n",
    "            pass\n",
    "    return df_s.reset_index('parameter').style.apply(\n",
    "        lambda x: ['background-color: yellow' if row in xin else '' for row in range(len(df_s.index))], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "code = cd.code_all # 전체 회사 코드\n",
    "code_good = cd.code_good\n",
    "code =  {'005930' : ['삼성전자', 'sec']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_inv1 = ['retail_1', 'foreigner_1', 'institution_1', 'financial_1', 'invtrust_1', 'pension_1', \n",
    "#             'privequity_1', 'bank_1', 'insurance_1', 'financeetc_1', 'corporateetc_1', \n",
    "            'privequity_1',  'insurance_1', 'corporateetc_1', # bank_1, 'financeetc_1 제외\n",
    "            'foreigneretc_1']\n",
    "col_inv2 = ['retail_2', 'foreigner_2', 'institution_2', 'financial_2', 'invtrust_2', 'pension_2',\n",
    "#             'privequity_2', 'bank_2', 'insurance_2', 'financeetc_2', 'corporateetc_2', \n",
    "            'privequity_2', 'insurance_2', 'corporateetc_2', # bank_2, 'financeetc_2 제외\n",
    "            'foreigneretc_2']\n",
    "col_his1 = ['open_1', 'high_1', 'low_1', 'close_1', 'vol_1']\n",
    "col_his2 = ['open_2', 'high_2', 'low_2', 'close_2', 'vol_2']\n",
    "col_cr = ['weekday', 'cr_00', 'cr_05', 'cr_10', 'cr_15', 'cr_20']\n",
    "col_common1 = [\"dji_cr\", \"dji_f_cr\", \"dxy_cr\", \"ixic_f_cr\", \"bond_kor_10_cr\", \"bond_kor_2_cr\", \"kosdaq_cr\", \"kospi_cr\", \n",
    "         \"krw_cr\", \"ixic_cr\", \"spx_f_cr\", \"sox_cr\", \"spx_cr\", \"bond_usa_10_cr\", \"bond_usa_2_cr\", \"bond_usa_3m_cr\", \n",
    "         \"vix_cr\", \"wti_cr\", \"spsy_cr\", \"spny_cr\", \"spxhc_cr\", \"splrcd_cr\", \"splrci_cr\", \"splrcu_cr\", \"splrcs_cr\",\n",
    "         \"splrct_cr\", \"splrcl_cr\", \"splrcm_cr\", \"ixbk_cr\", \"ixfn_cr\", \"ixid_cr\", \"ixis_cr\", \"ixk_cr\", \"ixtr_cr\",\n",
    "         \"ixut_cr\", \"nbi_cr\", \"bkx_cr\"]\n",
    "col_common2 = [\"dji_cr_2\", \"dji_f_cr_2\", \"dxy_cr_2\", \"ixic_f_cr_2\", \"bond_kor_10_cr_2\", \"bond_kor_2_cr_2\", \"kosdaq_cr_2\", \"kospi_cr_2\",\n",
    "         \"krw_cr_2\", \"ixic_cr_2\", \"spx_f_cr_2\", \"sox_cr_2\", \"spx_cr_2\", \"bond_usa_10_cr_2\", \"bond_usa_2_cr_2\", \"bond_usa_3m_cr_2\",\n",
    "         \"vix_cr_2\", \"wti_cr_2\", \"spsy_cr_2\", \"spny_cr_2\", \"spxhc_cr_2\", \"splrcd_cr_2\", \"splrci_cr_2\", \"splrcu_cr_2\",\n",
    "         \"splrcs_cr_2\", \"splrct_cr_2\", \"splrcl_cr_2\", \"splrcm_cr_2\", \"ixbk_cr_2\", \"ixfn_cr_2\", \"ixid_cr_2\",\n",
    "         \"ixis_cr_2\", \"ixk_cr_2\", \"ixtr_cr_2\", \"ixut_cr_2\", \"nbi_cr_2\", \"bkx_cr_2\"]\n",
    "col_futures = ['ixic_f_cr', 'ixic_f_cr_2', 'spx_f_cr', 'spx_f_cr_2', 'dji_f_cr', 'dji_f_cr_2',\n",
    "           'wti_cr','wti_cr_2', 'dxy_cr', 'dxy_cr_2', 'bond_usa_10_cr', 'bond_usa_10_cr_2' ]\n",
    "column_o = col_inv1 + col_common1 + col_his1 + col_inv2 + col_common2 + col_his2 + col_cr\n",
    "\n",
    "col_except_futures = [item for item in column_o if item not in col_futures]\n",
    "\n",
    "new_col = col_except_futures.copy()\n",
    "\n",
    "# bank, financeetc는 결측치가 많아서 사용하지 않음.\n",
    "# df.drop(['bank_1', 'bank_2', 'financeetc_1', 'financeetc_2'], axis=1, inplace=True)   \n",
    "\n",
    "# col_futures : futures는 당일 종료가 되지 않는 data이므로 제외"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 최초의 empty df 생성\n",
    "df_base =   pd.DataFrame(pd.Series([],dtype=pd.StringDtype(), name='parameter')).set_index('parameter')\n",
    "df_params = pd.DataFrame(pd.Series([],dtype=pd.StringDtype(), name='parameter')).set_index('parameter')\n",
    "df_std =    pd.DataFrame(pd.Series([],dtype=pd.StringDtype(), name='parameter')).set_index('parameter')\n",
    "iter = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "com_name = list(code.values())[0][1]\n",
    "\n",
    "directory_for_ml = '../../data/data_for_ml/predict/'\n",
    "directory_for_model = '../../data/data_for_ml/model/model/'\n",
    "fname = f'df_{com_name}_combine.pkl'\n",
    "f_name = directory_for_ml + fname\n",
    "df_o = pd.read_pickle(f_name) \n",
    "df_o = df_o.iloc[:-1] # /predict/를 사용할 경우 마지막 prediction data 제외"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "335"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_start_index = list(df_o.index).index(TRAIN_START_DATE)\n",
    "train_end_index = list(df_o.index).index(TRAIN_END_DATE)\n",
    "val_end_index = list(df_o.index).index(VAL_END_DATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of common_data and company data is DIFFERENT.\n",
      "different date is :  {datetime.date(2022, 11, 14)}\n"
     ]
    }
   ],
   "source": [
    "# opening date와 df_o date의 개수 일치 여부 확인\n",
    "base_data_directory = '../../data/base_data/stock_market_holydays/'\n",
    "OPENING_DAYS_KOR = pd.read_pickle(base_data_directory+'opening_days_kor.pkl') # 한국 개장일 데이터 \n",
    "OPENING_DAYS_USA = pd.read_pickle(base_data_directory+'opening_days_usa.pkl') # 미국 개장일 데이터 \n",
    "\n",
    "opening_day_filter = [item for item in list(OPENING_DAYS_KOR) if item in list(OPENING_DAYS_USA)]\n",
    "\n",
    "sttn = list(opening_day_filter).index(TRAIN_START_DATE)\n",
    "endn = list(opening_day_filter).index(TRAIN_END_DATE)\n",
    "# sttn = list(OPENING_DAYS_KOR).index(TRAIN_START_DATE)\n",
    "# endn = list(OPENING_DAYS_KOR).index(TRAIN_END_DATE)\n",
    "len_filter = endn - sttn\n",
    "\n",
    "len_train = train_end_index - train_start_index\n",
    "\n",
    "if (len_train != 0 ):\n",
    "    print(\"Length of common_data and company data is DIFFERENT.\")\n",
    "    diff_date = set(opening_day_filter[515:772]) - set(list(df_o.iloc[32:288].index))\n",
    "    print(\"different date is : \", diff_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_o_train = df_o.iloc[train_start_index:train_end_index+1]\n",
    "df_o_val = df_o.iloc[train_end_index+1:val_end_index+1]\n",
    "df_o_test = df_o.iloc[val_end_index+1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 아래는 기존 자료 이용시 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### read best fit parameters made from the previous fit training\n",
    "params_search_o = pd.read_pickle(f'{directory_for_model}{com_name}/best_params.pkl')\n",
    "list(params_search_o.to_dict().values())[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### 다중 iter columns으로 되어 있는 경우 아래의 예를 사용\n",
    "params_search_o = pd.read_pickle(f'{directory_for_model}{com_name}/params_bs_df_0621_1429.pkl')\n",
    "list(params_search_o.to_dict().values())[3] # 예: iter-3 column 선정시"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 여기까지"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 반복 작업시 여기서 부터 진행 (feature importance로 선정된 새로운 column으로)\n",
    "\n",
    "df = df_o_train[new_col]\n",
    "train_input = df.iloc[:, :-5]\n",
    "train_target = df.iloc[:, -5]\n",
    "\n",
    "df = df_o_val[new_col]\n",
    "val_input = df.iloc[:, :-5]\n",
    "val_target = df.iloc[:, -5]\n",
    "\n",
    "df = df_o_test[new_col]\n",
    "test_input = df.iloc[:, :-5]\n",
    "test_target = df.iloc[:, -5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = None\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(train_input)\n",
    "train_scaled = scaler.transform(train_input)\n",
    "val_scaled = scaler.transform(val_input)\n",
    "test_scaled = scaler.transform(test_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_space = {\n",
    "    'boosting_type' : ['gbdt', 'dart', ],# ['gbdt', 'dart', 'goss'],\n",
    "    'learning_rate': (0.0001, 0.004, 'log-uniform'),\n",
    "    'num_leaves': (3, 20),      \n",
    "    'max_depth': (0, 50), #[-1], #(0, 50),\n",
    "    'min_child_samples': (5, 40),\n",
    "#     'min_child_weight': (0.001, 10.0), # 값을 변동시 같은 값이 최적값으로 선정되더라고 precision이 틀림. 반드시 값을 바꾸면서 진행해야 함.\n",
    "#     'min_split_loss': (0, 5), \n",
    "    'max_bin': (100, 1000), # 사용시 0.75%로 올라감 default: 255\n",
    "#     'max_cat_threshold' : (1, 100),\n",
    "    'subsample': (0.005, 1.0, 'uniform'), # == bagging_fraction\n",
    "#     'subsample_freq': (1, 10), # mobis에서 적용시 값이 많이 올라감. default:0\n",
    "    'colsample_bytree': (0.5, 1.0, 'uniform'),\n",
    "    'subsample_for_bin': (100000, 500000),\n",
    "    'scale_pos_weight' : (0.2, 2.0, 'uniform'),\n",
    "    'n_estimators': (700, 5000),\n",
    "    'reg_alpha' : (0.0, 4.0, 'uniform'), # =='lambda_l1': [0, 5], # default 0\n",
    "    'reg_lambda' : (0.0, 4.0, 'uniform'), # == lambda_l2': [0, 5], #default 0    \n",
    "\n",
    "    'force_col_wise': ['true'], \n",
    "    'importance_type': ['split'], # ['gain'], default: split\n",
    "    'boost_from_average' : [True, False],\n",
    "    'objective': ['binary'],\n",
    "    'metric': ['binary_logloss'],\n",
    "}\n",
    "\n",
    "params_space = search_space.copy() # for Bayesian Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BayesSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'cv' : 10,  # usually 5 or 10, default: 3  : the lower the overfitting, the higher the hunderfitting\n",
    "#     'cv' : None,\n",
    "#     'scoring' : None,\n",
    "#     'scoring' : 'f1',\n",
    "#     'scoring' : 'roc_auc',\n",
    "    'scoring' : 'precision',\n",
    "#     'scoring' : 'average_precision',\n",
    "#     'scoring' : 'accuracy',\n",
    "    'num_col' : len(new_col),\n",
    "    'iterations' : 20,  # default : 50, number of parameter settings.\n",
    "    # The number of parameter settings that are tried is given by n_iter.\n",
    "    'second_best_index' : None  # second best index를 사용했는지 확인용으로 삽입\n",
    "             }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# directory_for_model = '../../data/data_for_ml/model/model/0. test/'\n",
    "directory_for_model = '../../data/data_for_ml/model/model/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# directory가 없으면 만드는 과정\n",
    "# if not os.path.exists(com_name):\n",
    "#     os.makedirs(com_name)\n",
    "if not os.path.exists(directory_for_model+com_name):\n",
    "    os.makedirs(directory_for_model+com_name)\n",
    "\n",
    "iter = iter + 1\n",
    "\n",
    "lgbm = None\n",
    "lgbmBS = None\n",
    "gsbs = 'bs'\n",
    "\n",
    "lgbmBS = BayesSearchCV( estimator = lightgbm.LGBMClassifier(verbose=0, random_state=42),\n",
    "                       search_spaces = params_space,\n",
    "                       scoring = param_grid['scoring'],\n",
    "                       cv = StratifiedKFold( n_splits=param_grid['cv'],\n",
    "                                             shuffle=True,\n",
    "                                             random_state=42 ),\n",
    "                       n_jobs = -1, # 자동 검색 적용\n",
    "                       n_iter = param_grid['iterations'],   \n",
    "                       verbose = 0, refit = True, random_state = 42,\n",
    "                       return_train_score = 'True',\n",
    "                      )\n",
    "\n",
    "print(\"*** after lgbm BS ******\")\n",
    "lgbmBS.fit(train_scaled, train_target, eval_metric = 'logloss') \n",
    "\n",
    "# save model\n",
    "stamp = datetime.datetime.today().isoformat() # 파일명 끝에 생성날짜 시간 추가\n",
    "dt = re.sub(r'[-:T]', '', stamp[5:16])\n",
    "dt = f'{dt[:4]}_{dt[4:]}'\n",
    "\n",
    "df_estimator = make_df_from_estimator(lgbmBS.best_estimator_.get_params(), iter)\n",
    "df_estimator.sort_index(inplace=True) # alphabet 순으로 보기 편하게\n",
    "\n",
    "# metrics accuracy,,,, 3단계 precision 등까지. dictionary\n",
    "result_dict = calc_results(lgbmBS, lgbmBS.best_estimator_, \n",
    "                           train_scaled, val_scaled, test_scaled,  \n",
    "                           train_target, val_target, test_target,\n",
    "                          )\n",
    "\n",
    "df_grid = make_df_from_estimator(param_grid, iter) # gridcv parameter\n",
    "df_grid.sort_index(inplace=True) # alphabet 순으로 보기 편하게\n",
    "df_result = make_df_from_estimator(result_dict, iter)  # dict 를 df로\n",
    "df_concat = pd.concat([df_grid, df_estimator, df_result])\n",
    "\n",
    "df_base = pd.merge(df_base,df_concat, how='outer', left_index=True, right_index=True)\n",
    "\n",
    "df_result =  list_std(lgbmBS.cv_results_, iter)\n",
    "df_std = pd.concat([df_std, df_result], axis=1, join='outer')\n",
    "\n",
    "# val_test = df_concat.loc['acc_val'].iloc[0]\n",
    "# acc_test = df_concat.loc['acc_test'].iloc[0]\n",
    "# precision = df_concat.loc['test_precision'].iloc[0]\n",
    "# f1score = df_concat.loc['f1score'].iloc[0]\n",
    "\n",
    "params_search = add_dict(param_grid, params_space)\n",
    "df_params_search = make_df_from_estimator(params_search, iter)\n",
    "df_params = pd.merge(df_params,df_params_search, how='outer', left_index=True, right_index=True)\n",
    "  \n",
    "print(\"******* No.{} BS Process is Done! ********\".format(iter))\n",
    "    \n",
    "print(\"**** End of BayesSearchCV Process ****\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_one = df_combine_sorted(df_base, df_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "row_index =  ['train_precision', 'val_precision', 'test_precision']\n",
    "df_style(df_one.iloc[52:80, :], row_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "row_index = ['n_estimators', 'reg_alpha', 'max_bin', 'subsample_for_bin']\n",
    "df_style(df_one.head(60), row_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_std.iloc[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# search_space, parameters and results save\n",
    "df_one.to_csv(f'{directory_for_model}{com_name}/params_results_{gsbs}_df_{dt}.csv')\n",
    "df_one.to_pickle(f'{directory_for_model}{com_name}/params_results_{gsbs}_df_{dt}.pkl')\n",
    "\n",
    "# search_space, parameters and results save\n",
    "df_std.to_csv(f'{directory_for_model}{com_name}/std_{gsbs}_df_{dt}.csv')\n",
    "df_std.to_pickle(f'{directory_for_model}{com_name}/std_{gsbs}_df_{dt}.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 아래 내용은 위의 df_one을 대체함.\n",
    "# parameter and results save\n",
    "df_base.to_csv(f'{directory_for_model}{com_name}/lgbm_{gsbs}_df_{dt}.csv')\n",
    "df_base.to_pickle(f'{directory_for_model}{com_name}/lgbm_{gsbs}_df_{dt}.pkl')\n",
    "\n",
    "# search_space, parameters save\n",
    "df_params.to_csv(f'{directory_for_model}{com_name}/params_{gsbs}_df_{dt}.csv')\n",
    "df_params.to_pickle(f'{directory_for_model}{com_name}/params_{gsbs}_df_{dt}.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "'''\n",
    "# save model\n",
    "# 필요시 저장. 가장 좋은 결과만 저장하고자할 때는 밑의 save_best_results만 진행하면 됨.\n",
    "joblib.dump(lgbmBS.best_estimator_, f'{directory_for_model}{com_name}/estimator_{gsbs}_{dt}_v{iter}.pkl') # bayessearchcv 저장\n",
    "save_to_pickle(f'{directory_for_model}{com_name}/model_{gsbs}_{dt}_v{iter}_p.pkl', lgbmBS.best_estimator_)\n",
    "joblib.dump(scaler, f'{directory_for_model}{com_name}/scaler_{gsbs}_{dt}_v{iter}.pkl') # scaler 저장\n",
    "save_to_pickle(f'{directory_for_model}{com_name}/columns_{gsbs}_{dt}_{len(new_col)}_{round(precision*100):2d}%_{round(f1score*100):2d}%_ver{iter}.pkl', new_col)\n",
    "save_list_to_csv(f'{directory_for_model}{com_name}/columns_{gsbs}_{dt}_{len(new_col)}_{round(precision*100):2d}%_{round(f1score*100):2d}%_ver{iter}.csv', new_col)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 최종 가장 좋은 결과 저장\n",
    "save_best_results(com_name, lgbmBS.best_estimator_, scaler, new_col, df_base, df_params_search)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(lgbmBS.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = lgbmBS.best_estimator_\n",
    "feature_df = pd.DataFrame(model.booster_.feature_importance(importance_type='gain'), \n",
    "                      index=train_input.columns, columns=['importance']).sort_values(by='importance', \n",
    "                                                                              ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(feature_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_df.tail(70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_col = list(feature_df.index[:35]) +  ['cr_00', 'cr_05', 'cr_10', 'cr_15', 'cr_20']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameter 조합별 측정치 비교 결과 확인후 robust한 index select하여 아래 진행 후 bayessearchcv 진행\n",
    "# 아래는 index가 44인 경우\n",
    "second_best_index =  24  # select from parameter 비교 from .cv_results_\n",
    "\n",
    "df_cv_param = pd.DataFrame(lgbmBS.cv_results_)\n",
    "\n",
    "search_space_secondbest = {\n",
    "    'boosting_type' : [df_cv_param.iloc[second_best_index,:]['param_boosting_type']],\n",
    "    'learning_rate': [df_cv_param.iloc[second_best_index,:]['param_learning_rate']],\n",
    "    'num_leaves': [df_cv_param.iloc[second_best_index,:]['param_num_leaves']],\n",
    "    'max_depth': [df_cv_param.iloc[second_best_index,:]['param_max_depth']],\n",
    "    'min_child_samples': [df_cv_param.iloc[second_best_index,:]['param_min_child_samples']],\n",
    "#     'min_child_weight': [df_cv_param.iloc[second_best_index,:]['param_min_child_weight']],\n",
    "#     'min_split_loss': [df_cv_param.iloc[second_best_index,:]['param_min_split_loss']],\n",
    "#     'max_bin': [df_cv_param.iloc[second_best_index,:]['param_max_bin']],\n",
    "    'max_bin': [100],\n",
    "#     'max_cat_threshold' : [df_cv_param.iloc[second_best_index,:]['param_max_cat_threshold']],\n",
    "    'subsample': [df_cv_param.iloc[second_best_index,:]['param_subsample']],\n",
    "#     'subsample_freq': [df_cv_param.iloc[second_best_index,:]['param_subsample_freq']],\n",
    "    'colsample_bytree': [df_cv_param.iloc[second_best_index,:]['param_colsample_bytree']],\n",
    "    'subsample_for_bin': [df_cv_param.iloc[second_best_index,:]['param_subsample_for_bin']],\n",
    "    'scale_pos_weight' : [df_cv_param.iloc[second_best_index,:]['param_scale_pos_weight']],\n",
    "#     'scale_pos_weight': [df_cv_param.iloc[second_best_index,:]['param_scale_pos_weight']],\n",
    "    'n_estimators': [df_cv_param.iloc[second_best_index,:]['param_n_estimators']],\n",
    "    'reg_alpha' : [df_cv_param.iloc[second_best_index,:]['param_reg_alpha']],\n",
    "    'reg_lambda' : [df_cv_param.iloc[second_best_index,:]['param_reg_lambda']],\n",
    "#     'reg_alpha' : [df_cv_param.iloc[second_best_index,:]['param_reg_alpha']],\n",
    "#     'reg_lambda' : [df_cv_param.iloc[second_best_index,:]['param_reg_lambda']],\n",
    "    'force_col_wise': [df_cv_param.iloc[second_best_index,:]['param_force_col_wise']],\n",
    "    'importance_type': [df_cv_param.iloc[second_best_index,:]['param_importance_type']],\n",
    "    'boost_from_average' : [df_cv_param.iloc[second_best_index,:]['param_boost_from_average']],\n",
    "    'objective':[df_cv_param.iloc[second_best_index,:]['param_objective']],\n",
    "    'metric': [df_cv_param.iloc[second_best_index,:]['param_metric']],\n",
    "}\n",
    "\n",
    "params_space = search_space_secondbest.copy() # for Bayesian Optimization\n",
    "\n",
    "param_grid = {\n",
    "    'cv' : param_grid['cv'],\n",
    "    'scoring' : param_grid['scoring'],\n",
    "    'num_col' :  param_grid['num_col'],\n",
    "    'iterations' :  2,\n",
    "    'second_best_index' : second_best_index, \n",
    "             }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.listdir(f'{directory_for_model}{com_name}') # 파일 리스트 확인후 parameter file 선택"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'params_bs_df_0621_1429.pkl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "5-1 결정 트리.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
