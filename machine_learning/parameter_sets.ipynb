{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/fasthill/ML-DL-study-alone/blob/main/5-1%20%EA%B2%B0%EC%A0%95%20%ED%8A%B8%EB%A6%AC.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5uA_6TRHEMHV"
   },
   "source": [
    "## Parameter Sets\n",
    "#### 기존 분석자료 기준으로 만든 최적 parameter - 다음 최적 parameter를 찾기 위한 initial input parameter로 이용"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "26KAIfzEEMHc"
   },
   "source": [
    "<table align=\"left\">\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/rickiepark/hg-mldl/blob/master/5-1.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />구글 코랩에서 실행하기</a>\n",
    "  </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "rIXeXBVTfZrS"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from functools import reduce\n",
    "import datetime\n",
    "import os, re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "import xgboost\n",
    "from lightgbm import LGBMClassifier\n",
    "import lightgbm\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression, SGDClassifier, SGDRegressor\n",
    "from sklearn.model_selection import cross_validate, train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten, Dropout\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "def save_to_pickle(path, filename):\n",
    "    open_file = open(path, \"wb\")\n",
    "    pickle.dump(filename, open_file)\n",
    "    open_file.close()\n",
    "\n",
    "def load_from_pickle(path):\n",
    "    open_file = open(path, \"rb\")\n",
    "    loaded_list = pickle.load(open_file)\n",
    "    open_file.close()\n",
    "    return loaded_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_p(test_target, y_predict_list): \n",
    "    ps = precision_score(test_target, y_predict_list)\n",
    "    rs = recall_score(test_target, y_predict_list)\n",
    "    fs = f1_score(test_target, y_predict_list)\n",
    "    roc = roc_auc_score(test_target, y_predict_list)\n",
    "    collect_list = [ps, rs, fs, roc]\n",
    "    return collect_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_df_from_estimator(estimator, num):\n",
    "    df_t = pd.DataFrame.from_dict(estimator, orient='index')\n",
    "    df_t.columns = [f'value{num}']\n",
    "    df_t.index.name = 'parameter'\n",
    "    return df_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "code = {'005930' : ['삼성전자', 'sec'], '373220' : ['LG에너지솔루션', 'lgenergy'], \n",
    "        '000660' : ['SK하이닉스', 'skhinix'], '207940' : ['삼성바이오로직스', 'ssbio'],\n",
    "        '006400' : ['삼성SDI', 'sdi'], '051910' : ['LG화학', 'lgchemical'],\n",
    "        '005935' : ['삼성전자우', 'secpre'], '005380' : ['현대차', 'hyunmotor'],\n",
    "        '035420' : ['NAVER', 'naver'], '000270' : ['기아','kia'],\n",
    "        '035720' : ['카카오', 'kakao'], '005490' : ['POSCO홀딩스', 'poscoholding'],\n",
    "        '105560' : ['KB금융', 'kbbank'], '028260' : ['삼성물산', 'sscnt'],\n",
    "        '068270' : ['셀트리온', 'celltrion'], '012330' : ['현대모비스', 'mobis'],\n",
    "        '055550' : ['신한지주', 'shgroup'], '066570' : ['LG전자', 'lgelec'],\n",
    "        '003670' : ['포스코케미칼', 'poscochemical'], '096770' : ['SK이노베이션', 'skinnovation'],\n",
    "        '033780' : ['KT&G', 'ktng']}\n",
    "\n",
    "code = {'005930' : ['삼성전자', 'sec']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_sec = {\n",
    "    \"boosting_type\" : ['gbdt'], # ['dart', 'goss'], # dart : 신경망의 드롭아웃을 적용시킨 방법, \n",
    "    \"num_iterations\" : [300], #[450, 500, 550], #\"num_iterations\" \n",
    "    \"learning_rate\": [0.03, 0.04, 0.051, 0.06, 0.07], \n",
    "    \"max_depth\": [-1, 2, 3, 4], # 가장 먼저 튜닝 필요 -1이 default (무한깊이) 일반적으로 default가 가장 좋음.\n",
    "    \"num_leaves\": [2, 3, 4, 5, 6], #  # 두번째로 중요, num_leaves는 작은 데이터면 작은 숫자로\n",
    "    \"subsample\": [1, 0.001, 0.002],# 세번째로 중요. = bagging fraction, row sampling. 아래 colsample_bytree과 같이 튜닝.\n",
    "    \"colsample_bytree\": [0.3, 0.35, 0.4, 0.45, 0.5], # = feature fraction, column sampling. 위의 subsample과 같이 튜닝.\n",
    "    \"objective\": ['binary'],\n",
    "    \"metric\": ['binary_logloss'], # \"metric\": ['loss'], # mae : mean absolute error, mse : mean squared error, \n",
    "                      # binary_logloss : loss for binary classification, multi_logloss : loss for multi classification\n",
    "\n",
    "    \"n_estimators\": [None, 3, 4, 5],\n",
    "    \"max_delta_step\": [0.4, 0.5, 0.6, 0.7, 0.8], # best value found, default = 0\n",
    "    \"scale_pos_weight\": [2, 10, 15], # class imbalance 경감, scale_pos_weight > 0.0, default=1.0\n",
    "#     \"is_unbalance\": [True],  # 불균형 셋 조정. 주의: \"scale_pos_weight\"과 동시에 사용할 수 없음\n",
    "    #---------------------------------------------\n",
    "#     \"scoring\": ['accuracy', 'precision'], \n",
    "#     \"early_stopping_rounds\" : [200],\n",
    "  \n",
    "# ************* 아래 하나씩 테스트해서 취사 선택해야 함. **************\n",
    "#     \"bagging_frequency\" : [5], \n",
    "#     \"num_threads\": [0], # == nthread=4, # Gpu 수, default=0, 자동 검색후 적용\n",
    "#     \"min_data_in_leaf\": [None], # 과적합 방지 파라미터\n",
    "#     \"lambda_l1\": [0], # default 0\n",
    "#     \"lambda_l2\": [0], #default 0\n",
    "#     \"min_gain_to_split\": [None], # 분기하기 위해 필요한 최소한의 gain을 의미, 정규화시 사용\n",
    "#     \"num_threads\": = 0, # == nthread=4, # Gpu 수, default=0, 자동 검색후 적용\n",
    "#     \"max_bin\": [32],  # data 준비할 때 사용하는 parameter     \n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Fitting 5 folds for each of 1296 candidates, totalling 6480 fits\n",
    "[LightGBM] [Warning] Unknown parameter: eval_metric\n",
    "Best Estimator: LGBMClassifier(\n",
    "\n",
    "colsample_bytree=0.41, \n",
    "eval_metric='logloss',\n",
    "learning_rate=0.004, \n",
    "max_delta_step=0.5, \n",
    "max_depth=2,\n",
    "metric='loss', \n",
    "n_estimators=5, \n",
    "num_iterations=500, \n",
    "num_leaves=3,\n",
    "objective='binary', \n",
    "random_state=42, \n",
    "subsample=0.005)\n",
    "    \n",
    "Best Parameters: {'boosting_type': 'gbdt', 'colsample_bytree': 0.41, 'learning_rate': 0.004, 'max_depth': 2, 'metric': 'loss', 'n_estimators': 5, 'num_leaves': 3, 'objective': 'binary', 'subsample': 0.005}\n",
    "Best Score: 0.9333333333333332, Best Index: 0\n",
    "train accuracy: 0.8081, val accuracy 0.7442, test accuracy 0.7593\n",
    "precision : 1.0000, recall : 0.2353, f1score : 0.3810, roc : 0.6176\n",
    "[[37  0]\n",
    " [13  4]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Fitting 5 folds for each of 18000 candidates, totalling 90000 fits\n",
    "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=0.5 will be ignored. Current value: feature_fraction=0.8\n",
    "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=0.5 will be ignored. Current value: bagging_fraction=0.9\n",
    "Best Estimator: LGBMClassifier(bagging_fraction=0.9, colsample_bytree=0.5, feature_fraction=0.8,\n",
    "               learning_rate=0.05, max_delta_step=0.5, max_depth=2,\n",
    "               metric='auc', num_iterations=500, num_leaves=4,\n",
    "               objective='binary', random_state=42, subsample=0.5)\n",
    "Best Parameters: {'boosting_type': 'gbdt', 'colsample_bytree': 0.5, 'learning_rate': 0.05, 'max_depth': 2, 'metric': 'auc', 'n_estimators': 100, 'num_leaves': 4, 'objective': 'binary', 'subsample': 0.5}\n",
    "Best Score: 0.8375180375180376, Best Index: 726\n",
    "train accuracy: 1.0000, val accuracy 0.7907, test accuracy 0.8148\n",
    "precision : 0.7059, recall : 0.7059, f1score : 0.7059, roc : 0.7854\n",
    "[[32  5]\n",
    " [ 5 12]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_o = {\n",
    "    \"boosting_type\" : ['gbdt'], # ['dart', 'goss'], # dart : 신경망의 드롭아웃을 적용시킨 방법, \n",
    "    \"num_iterations\" : [300], #[450, 500, 550], #\"num_iterations\" \n",
    "    \"learning_rate\": [0.03, 0.04, 0.051, 0.06, 0.07], \n",
    "    \"max_depth\": [-1, 2, 3, 4], # 가장 먼저 튜닝 필요 -1이 default (무한깊이) 일반적으로 default가 가장 좋음.\n",
    "    \"num_leaves\": [2, 3, 4, 5, 6], #  # 두번째로 중요, num_leaves는 작은 데이터면 작은 숫자로\n",
    "    \"subsample\": [1, 0.001, 0.002],# 세번째로 중요. = bagging fraction, row sampling. 아래 colsample_bytree과 같이 튜닝.\n",
    "    \"colsample_bytree\": [0.3, 0.35, 0.4, 0.45, 0.5], # = feature fraction, column sampling. 위의 subsample과 같이 튜닝.\n",
    "    \"objective\": ['binary'],\n",
    "    \"metric\": ['binary_logloss'], # \"metric\": ['loss'], # mae : mean absolute error, mse : mean squared error, \n",
    "                      # binary_logloss : loss for binary classification, multi_logloss : loss for multi classification\n",
    "\n",
    "    \"n_estimators\": [None, 3, 4, 5],\n",
    "    \"max_delta_step\": [0.4, 0.5, 0.6, 0.7, 0.8], # best value found, default = 0\n",
    "    \"scale_pos_weight\": [2, 10, 15], # class imbalance 경감, scale_pos_weight > 0.0, default=1.0\n",
    "#     \"is_unbalance\": [True],  # 불균형 셋 조정. 주의: \"scale_pos_weight\"과 동시에 사용할 수 없음\n",
    "    #---------------------------------------------\n",
    "#     \"scoring\": ['accuracy', 'precision'], \n",
    "#     \"early_stopping_rounds\" : [200],\n",
    "  \n",
    "# ************* 아래 하나씩 테스트해서 취사 선택해야 함. **************\n",
    "#     \"bagging_frequency\" : [5], \n",
    "#     \"num_threads\": [0], # == nthread=4, # Gpu 수, default=0, 자동 검색후 적용\n",
    "#     \"min_data_in_leaf\": [None], # 과적합 방지 파라미터\n",
    "#     \"lambda_l1\": [0], # default 0\n",
    "#     \"lambda_l2\": [0], #default 0\n",
    "#     \"min_gain_to_split\": [None], # 분기하기 위해 필요한 최소한의 gain을 의미, 정규화시 사용\n",
    "#     \"num_threads\": = 0, # == nthread=4, # Gpu 수, default=0, 자동 검색후 적용\n",
    "#     \"max_bin\": [32],  # data 준비할 때 사용하는 parameter     \n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_o = {\n",
    "    \"boosting_type\" : ['gbdt'], # ['dart', 'goss'], # dart : 신경망의 드롭아웃을 적용시킨 방법, \n",
    "#     \"num_iterations\" : [200], #[450, 500, 550], #\"num_iterations\" \n",
    "    \"learning_rate\": [0.06, 0.065, 0.07, 0.075, 0.08], \n",
    "    \"max_depth\": [-1, 2, 3, 4], # 가장 먼저 튜닝 필요 -1이 default (무한깊이) 일반적으로 default가 가장 좋음.\n",
    "    \"num_leaves\": [2, 3, 4, 5, 6], #  # 두번째로 중요, num_leaves는 작은 데이터면 작은 숫자로\n",
    "    \"subsample\": [1, 0.001, 0.002],# 세번째로 중요. = bagging fraction, row sampling. 아래 colsample_bytree과 같이 튜닝.\n",
    "    \"colsample_bytree\": [0.3, 0.35, 0.4, 0.45, 0.5], # = feature fraction, column sampling. 위의 subsample과 같이 튜닝.\n",
    "    \"objective\": ['binary'],\n",
    "    \"metric\": ['binary_logloss'], # \"metric\": ['loss'], # mae : mean absolute error, mse : mean squared error, \n",
    "                      # binary_logloss : loss for binary classification, multi_logloss : loss for multi classification\n",
    "\n",
    "    \"n_estimators\": [80, 100, 120],\n",
    "    \"max_delta_step\": [ 0.8, 0.9], # best value found, default = 0\n",
    "    \"scale_pos_weight\": [10], # class imbalance 경감, scale_pos_weight > 0.0, default=1.0\n",
    "#     \"scoring\": ['accuracy', 'precision'],   \n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.booster_.feature_importance(importance_type='gain')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.booster_.feature_importance(importance_type='split')\n",
    "# 큰 특징을 가지는 feature는 tree상위레벨에서 적게 사용됨."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 최초의 empty df 생성\n",
    "# df_base = pd.DataFrame(pd.Series([],dtype=pd.StringDtype(), name='parameter')).set_index('parameter')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_estimator = make_df_from_estimator(lgbmgs.best_estimator_.get_params(), 1)\n",
    "result_dict = calc_results(lgbmgs.best_estimator_, train_scaled, val_scaled, test_scaled, test_target)\n",
    "df_result = make_df_from_estimator(result_dict, 1)\n",
    "df_concat = pd.concat([df_estimator, df_result])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_concat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_concat.index =['boosting_type', 'class_weight', 'colsample_bytree', 'importance_type',\n",
    "       'learning_rate', 'max_depth', 'min_child_samples', 'min_child_weight',\n",
    "       'min_split_gain', 'n_estimators', 'n_jobs', 'num_leaves', 'objective',\n",
    "       'random_state', 'reg_alpha', 'reg_lambda', 'silent', 'subsample',\n",
    "       'subsample_for_bin', 'subsample_freq', 'max_delta_step', 'metric',\n",
    "       'num_iterations', 'scale_pos_weight', 'best_score', 'best_index',\n",
    "       'acc_train', 'acc_val', 'acc_test', 'precision', 'recall', 'f1score',\n",
    "       'roc', 'tn', 'fp', 'fn', 'tp']\n",
    "df_concat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "num_leaves = 20~3000\n",
    "max_depth = 3 ~12\n",
    "learning_rate = 0.01 ~ 0.3\n",
    "\n",
    "parameters = {'learning_rate': [0.01,0.02,0.03],\n",
    "                  'subsample'    : [0.9, 0.5, 0.2],\n",
    "                  'n_estimators' : [100,500,1000],\n",
    "                  'max_depth'    : [4,6,8]\n",
    "                 }\n",
    "\n",
    "bayesian optimization 확인"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "5-1 결정 트리.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
