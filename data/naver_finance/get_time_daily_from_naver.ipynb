{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "matched-interview",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup as bs\n",
    "import requests\n",
    "import datetime, time\n",
    "import pickle\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import io\n",
    "import shutil\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6a3f2606",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install cfscrape # 403 forbidden, cloudflare error을 해결하기 위한 모듈\n",
    "import cloudscraper\n",
    "scraper = cloudscraper.create_scraper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "08943027",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # !pip install cfscrape # 403 forbidden, cloudflare error을 해결하기 위한 모듈\n",
    "# import cfscrape\n",
    "# scraper = cfscrape.create_scraper()\n",
    "# # 이후 403 error이 발생한 곳에는 requests 대신 scraper 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "selective-ireland",
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = {'Referer': 'https://kr.investing.com/',\n",
    "           'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) \\\n",
    "           AppleWebKit/537.36 (KHTML, like Gecko) Chrome/125.0.0.0 Safari/537.36 Edg/125.0.0.0'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fa5d6156-f7ff-4aee-a063-7240845f438e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append('../../data/constant')\n",
    "from constants import COMPANY_CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "afbcbb53-e0e0-496d-adb7-a57c3341ae12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def concat_df(df_o, df, dup_col, sort_col):\n",
    "    df_o = pd.concat([df_o, df], ignore_index=True)\n",
    "    df_o.drop_duplicates(subset=[dup_col], keep='last', inplace=True) # dup_col 중첩제거 기준 컬럼 이름: \"time\", \"date\" 등\n",
    "#     df_o.drop_duplicates(subset=[dup_col], keep='first', inplace=True)\n",
    "    df_o.sort_values(by=[df_o.columns[sort_col]], inplace=True) # sort_col 정렬 기준 컬럼 번호\n",
    "    df_o.index = np.arange(0, len(df_o))  # 일련 번호 오름차순으로 재 설정\n",
    "    return df_o"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98171c7f-8e92-4ea7-a933-c4855c8bb1a8",
   "metadata": {},
   "source": [
    "## 시간별 시세"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3c80d5a0-f562-4f37-9363-ee1056d572b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 특정일의 시간별 시세 취득을 위한 페이지별 table 데이터 취득. page는 날짜별로 약 40쪽. 09:00:00부터 1분간격으로 15:58:00 까지.\n",
    "def get_piece_time_price(url_t):\n",
    "    res = scraper.get(url_t, headers=headers)\n",
    "    class_name = 'type2'\n",
    "    df = pd.read_html(io.StringIO(str(res.text)), attrs={\"class\": class_name}, flavor=[\"lxml\", \"bs4\"])[0]\n",
    "    \n",
    "    df = df.dropna(axis=0) # delete rows with nan values\n",
    "    df.columns = ['time', 'price', 'change', 'sell', 'buy', 'volume', 'change_volumn'] # rename column in english from korean\n",
    "\n",
    "    # convert character values to integer value : 보합= 0, 하락= -, 상승= +\n",
    "    df['change'] = df['change'].apply(lambda x: int(x[2:]) if x[:2] == '보합' \n",
    "                                      else (-int(x[4:].replace(',','')) if x[:2] == '하락' \n",
    "                                            else int(x[4:].replace(',',''))))  # convert characters to int\n",
    "    \n",
    "    df = df[['time', 'price', 'change', 'volume']]  # delete unnecessary columns\n",
    "\n",
    "    # define variable types\n",
    "    df['time'] = df['time'].apply(lambda x : datetime.datetime.strptime(x, \"%H:%M\").time())  # convert characters to datetime objet\n",
    "    df['price'] = df['price'].astype(int)\n",
    "    df['volume'] = df['volume'].astype(int)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "699280b8-7e5c-450f-864a-b27d9329a4a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 특정일의 시간대별 가격 40쪽을 한개의 df로 묶는 기능\n",
    "def get_time_price(url_base_t, code_com, collect_date):\n",
    "    \n",
    "    page_num = 1\n",
    "    \n",
    "    # make first data frame\n",
    "    collect_date_time = collect_date + '160000' # 장종료후 16시 00분 00초에 시간별 시세 추출\n",
    "    page = str(page_num)\n",
    "    \n",
    "    url = url_base_t + '?code=' + code_com + '&thistime=' + collect_date_time + '&page=' + page\n",
    "    df_base = get_piece_time_price(url)\n",
    "    \n",
    "    page_num = page_num + 1\n",
    "    \n",
    "    while True:\n",
    "        page = str(page_num)\n",
    "        \n",
    "        url = url_base_t + '?code=' + code_com + '&thistime=' + collect_date_time + '&page=' + page\n",
    "        df_p = get_piece_time_price(url)\n",
    "    \n",
    "        df_base = concat_df(df_base, df_p, 'time', 0)  # df concat후 'time' column을 기준으로 중복제거 후 0 column을 기준으로 정렬시킴.\n",
    "        # print('page_num', page_num)\n",
    "        \n",
    "        if df_p['time'].iloc[-1] == datetime.time(9, 00):\n",
    "            break\n",
    "    \n",
    "        page_num = page_num + 1\n",
    "    \n",
    "    df_base['date'] = datetime.datetime.strptime(collect_date, '%Y%m%d') # insert column with collecting date\n",
    "    df_base = df_base[['date', 'time', 'price', 'change', 'volume']]  \n",
    "\n",
    "    return df_base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a1c4d087-aaa6-405d-9002-5edd150271bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_start_end_date(): # 오늘부터 1주일 전까지 휴일, 휴장일 제외된 일자 리스트 구성. \n",
    "    end_date = datetime.datetime.today().date()\n",
    "    date_list = [end_date - datetime.timedelta(days=x) for x in range(8)] # day one week (8 days) before today\n",
    "    start_date = date_list[-1]\n",
    "    \n",
    "    datelist = pd.date_range(start_date, end_date, freq='B') # 토, 일을 제외한 주중 일자만 선택\n",
    "    # df_1 = pd.DataFrame(datelist, columns=['date']) # df로 구성\n",
    "\n",
    "    return list(datelist.map(lambda x : x.strftime('%Y%m%d')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e5b751c7-63fc-4f11-ac67-1ee03bee6d7d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, "
     ]
    }
   ],
   "source": [
    "# naver_dir = 'data/naver_finance/time_data'\n",
    "naver_dir = 'time_data'\n",
    "\n",
    "url_base = 'https://finance.naver.com/item/sise_time.naver'  # sise_date\n",
    "\n",
    "code_dic = {'005930': ['삼성전자', 'sec'], '005380': ['현대차', 'hyunmotor'], \n",
    "        '035420': ['NAVER', 'naver'], '033780': ['KT&G', 'ktng']}\n",
    "\n",
    "code_dic = COMPANY_CODE\n",
    "# code_list = list(code_dic.items())\n",
    "# code_company_name = code_list[0]\n",
    "# code = code_company_name[0] # 취득을 원하는 회사 주식 코드\n",
    "# code_dic = {'005930': ['삼성전자', 'sec'], '005380': ['현대차', 'hyunmotor'], }\n",
    "# code_dic = {'035420': ['NAVER', 'naver'], '033780': ['KT&G', 'ktng']}\n",
    "\n",
    "c_date = ['20240619', '20240620', '20240621', '20240624', '20240625', '20240626'] # 취득이 필요한 날짜 리스트\n",
    "c_date = get_start_end_date() # 취득이 필요한 날짜 리스트, naver는 오늘날짜부터 1주일전까지만 저장. c_date 시작일자는 20200101 부터 있음.\n",
    "# \n",
    "# c_date = ['20240626', '20240620'] \n",
    "# c_date = ['20240619'] \n",
    "for i, (code, company_name) in enumerate(code_dic.items()):\n",
    "    for collect_date in c_date:\n",
    "        df_collect = get_time_price(url_base, code, collect_date)\n",
    "        # add logic to save date for each date for each company\n",
    "        f_name = f'{naver_dir}/{company_name[1]}_{collect_date}.csv'\n",
    "        df_collect.to_csv(f_name)\n",
    "        df_collect.to_pickle(f_name.replace('csv','pkl'))\n",
    "\n",
    "    print(i+1, end=\", \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca0bd264-3f8f-4e16-a76b-51e45e1a38d1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac09bdd9-5d50-4de0-b3e5-e1e68f155ac9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
