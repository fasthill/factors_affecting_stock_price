{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "matched-interview",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup as bs\n",
    "import requests\n",
    "import datetime, time\n",
    "import pickle\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import io\n",
    "import shutil\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6a3f2606",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install cfscrape # 403 forbidden, cloudflare error을 해결하기 위한 모듈\n",
    "import cloudscraper\n",
    "scraper = cloudscraper.create_scraper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "08943027",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # !pip install cfscrape # 403 forbidden, cloudflare error을 해결하기 위한 모듈\n",
    "# import cfscrape\n",
    "# scraper = cfscrape.create_scraper()\n",
    "# # 이후 403 error이 발생한 곳에는 requests 대신 scraper 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "selective-ireland",
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = {'Referer': 'https://kr.investing.com/',\n",
    "           'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) \\\n",
    "           AppleWebKit/537.36 (KHTML, like Gecko) Chrome/125.0.0.0 Safari/537.36 Edg/125.0.0.0'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fa5d6156-f7ff-4aee-a063-7240845f438e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append('../../data/constant')\n",
    "from constants import COMPANY_CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "afbcbb53-e0e0-496d-adb7-a57c3341ae12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def concat_df(df_o, df, dup_col, sort_col):\n",
    "    df_o = pd.concat([df_o, df], ignore_index=True)\n",
    "    df_o.drop_duplicates(subset=[dup_col], keep='last', inplace=True) # dup_col 중첩제거 기준 컬럼 이름: \"time\", \"date\" 등\n",
    "#     df_o.drop_duplicates(subset=[dup_col], keep='first', inplace=True)\n",
    "    df_o.sort_values(by=[df_o.columns[sort_col]], inplace=True) # sort_col 정렬 기준 컬럼 번호\n",
    "    df_o.index = np.arange(0, len(df_o))  # 일련 번호 오름차순으로 재 설정\n",
    "    return df_o"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98171c7f-8e92-4ea7-a933-c4855c8bb1a8",
   "metadata": {},
   "source": [
    "## 시간별 시세"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3c80d5a0-f562-4f37-9363-ee1056d572b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 특정일의 시간별 시세 취득을 위한 페이지별 table 데이터 취득. page는 날짜별로 약 40쪽. 09:00:00부터 1분간격으로 15:58:00 까지.\n",
    "def get_piece_time_price(url_t):\n",
    "    res = scraper.get(url_t, headers=headers)\n",
    "    class_name = 'type2'\n",
    "    df = pd.read_html(io.StringIO(str(res.text)), attrs={\"class\": class_name}, flavor=[\"lxml\", \"bs4\"])[0]\n",
    "    \n",
    "    df = df.dropna(axis=0) # delete rows with nan values\n",
    "    df.columns = ['time', 'price', 'change', 'sell', 'buy', 'volume', 'change_volumn'] # rename column in english from korean\n",
    "\n",
    "    # convert character values to integer value : 보합= 0, 하락= -, 상승= +\n",
    "    df['change'] = df['change'].apply(lambda x: int(x[2:]) if x[:2] == '보합' \n",
    "                                      else (-int(x[4:].replace(',','')) if x[:2] == '하락' \n",
    "                                            else int(x[4:].replace(',',''))))  # convert characters to int\n",
    "    \n",
    "    df = df[['time', 'price', 'change', 'volume']]  # delete unnecessary columns\n",
    "\n",
    "    # define variable types\n",
    "    df['time'] = df['time'].apply(lambda x : datetime.datetime.strptime(x, \"%H:%M\").time())  # convert characters to datetime objet\n",
    "    df['price'] = df['price'].astype(int)\n",
    "    df['volume'] = df['volume'].astype(int)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "699280b8-7e5c-450f-864a-b27d9329a4a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 특정일의 시간대별 가격 40쪽을 한개의 df로 묶는 기능\n",
    "def get_time_price(url_base_t, code_com, collect_date):\n",
    "    \n",
    "    page_num = 1\n",
    "    \n",
    "    # make first data frame\n",
    "    collect_date_time = collect_date + '160000' # 장종료후 16시 00분 00초에 시간별 시세 추출\n",
    "    page = str(page_num)\n",
    "    \n",
    "    url = url_base_t + '?code=' + code_com + '&thistime=' + collect_date_time + '&page=' + page\n",
    "    df_base = get_piece_time_price(url)\n",
    "    \n",
    "    page_num = page_num + 1\n",
    "    \n",
    "    while True:\n",
    "        page = str(page_num)\n",
    "        \n",
    "        url = url_base_t + '?code=' + code_com + '&thistime=' + collect_date_time + '&page=' + page\n",
    "        df_p = get_piece_time_price(url)\n",
    "    \n",
    "        df_base = concat_df(df_base, df_p, 'time', 0)  # df concat후 'time' column을 기준으로 중복제거 후 0 column을 기준으로 정렬시킴.\n",
    "        # print('page_num', page_num)\n",
    "        \n",
    "        if df_p['time'].iloc[-1] == datetime.time(9, 00):\n",
    "            break\n",
    "    \n",
    "        page_num = page_num + 1\n",
    "    \n",
    "    df_base['date'] = datetime.datetime.strptime(collect_date, '%Y%m%d') # insert column with collecting date\n",
    "    df_base = df_base[['date', 'time', 'price', 'change', 'volume']]  \n",
    "\n",
    "    return df_base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a1c4d087-aaa6-405d-9002-5edd150271bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_start_end_date(): # 오늘부터 1주일 전까지 휴일, 휴장일 제외된 일자 리스트 구성. \n",
    "    end_date = datetime.datetime.today().date()\n",
    "    date_list = [end_date - datetime.timedelta(days=x) for x in range(8)] # day one week (8 days) before today\n",
    "    start_date = date_list[-1]\n",
    "    \n",
    "    datelist = pd.date_range(start_date, end_date, freq='B') # 토, 일을 제외한 주중 일자만 선택\n",
    "    # df_1 = pd.DataFrame(datelist, columns=['date']) # df로 구성\n",
    "\n",
    "    return list(datelist.map(lambda x : x.strftime('%Y%m%d')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e5b751c7-63fc-4f11-ac67-1ee03bee6d7d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# naver_dir = 'data/naver_finance/time_data'\n",
    "naver_dir = 'time_data'\n",
    "\n",
    "url_base = 'https://finance.naver.com/item/sise_time.naver'  # sise_date\n",
    "\n",
    "code_dic = {'005930': ['삼성전자', 'sec'], '005380': ['현대차', 'hyunmotor'], \n",
    "        '035420': ['NAVER', 'naver'], '033780': ['KT&G', 'ktng']}\n",
    "\n",
    "code_dic = COMPANY_CODE\n",
    "# code_list = list(code_dic.items())\n",
    "# code_company_name = code_list[0]\n",
    "# code = code_company_name[0] # 취득을 원하는 회사 주식 코드\n",
    "# code_dic = {'005930': ['삼성전자', 'sec'], '005380': ['현대차', 'hyunmotor'], }\n",
    "# code_dic = {'035420': ['NAVER', 'naver'], '033780': ['KT&G', 'ktng']}\n",
    "\n",
    "c_date = ['20240619', '20240620', '20240621', '20240624', '20240625', '20240626'] # 취득이 필요한 날짜 리스트\n",
    "c_date = get_start_end_date() # 취득이 필요한 날짜 리스트, naver는 오늘날짜부터 1주일전까지만 저장. c_date 시작일자는 20200101 부터 있음.\n",
    "# \n",
    "# c_date = ['20240626', '20240620'] \n",
    "# c_date = ['20240619'] \n",
    "for i, (code, company_name) in enumerate(code_dic.items()):\n",
    "    for collect_date in c_date:\n",
    "        df_collect = get_time_price(url_base, code, collect_date)\n",
    "        # add logic to save date for each date for each company\n",
    "        f_name = f'{naver_dir}/{company_name[1]}_{collect_date}.csv'\n",
    "        df_collect.to_csv(f_name)\n",
    "        df_collect.to_pickle(f_name.replace('csv','pkl'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bae7fc7-ceb8-4886-af2c-f856d2403168",
   "metadata": {},
   "source": [
    "## 일별시세"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ebbbd01b-12d7-4956-9bec-301933b1b685",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 일자별 주식 데이터를 페이지별로 10개씩 취득\n",
    "def get_piece_date_price(url_d):\n",
    "    res = scraper.get(url_d, headers=headers)\n",
    "    class_name = 'type2'\n",
    "    df = pd.read_html(io.StringIO(str(res.text)), attrs={\"class\": class_name}, flavor=[\"lxml\", \"bs4\"])[0]\n",
    "    \n",
    "    df = df.dropna(axis=0) # delete nan rows\n",
    "\n",
    "    df.columns = ['date', 'close', 'close_change', 'open', 'high', 'low', 'volume'] # rename column\n",
    "    df['date'] = df['date'].apply(lambda x: datetime.datetime.strptime(x, '%Y.%m.%d')) # convert character to datetime object\n",
    "    \n",
    "    # convert character values to integer value : 보합= 0, 하락= -, 상승= +\n",
    "    df['close_change'] = df['close_change'].apply(lambda x: int(x[2:]) if x[:2] == '보합' \n",
    "                                  else (-int(x[4:].replace(',','')) if x[:2] == '하락' \n",
    "                                        else int(x[4:].replace(',',''))))  # convert characters to int\n",
    "    # define variable types\n",
    "    df['open'] = df['open'].astype(int)\n",
    "    df['high'] = df['high'].astype(int)\n",
    "    df['low'] = df['low'].astype(int)\n",
    "    df['close'] = df['close'].astype(int)\n",
    "    df['volume'] = df['volume'].astype(int)\n",
    "    \n",
    "    df = df[['date', 'open', 'high', 'low', 'close', 'close_change', 'volume']]  # rearrange columns\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "309638e3-7ee2-4b51-b67c-c12e9f20f609",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10개씩의 일자별 데이터를 원하는 일자부터 현재일자까지 합하어 취득\n",
    "def get_date_price(url_base_d, code_com):\n",
    "    \n",
    "    page_num = 1\n",
    "    \n",
    "    # make first data frame\n",
    "    page = str(page_num)\n",
    "    url_date = url_base_d + '?code=' + code_com + '&page=' + page\n",
    "    df_base = get_piece_date_price(url_date)\n",
    "    \n",
    "    page_num = page_num + 1\n",
    "\n",
    "    startdate_str = '2020/1/2 00:00:00' # 데이터 수집 시작 일자, startdate_str\n",
    "    startdate = datetime.datetime.strptime(startdate_str, '%Y/%m/%d %H:%M:%S')\n",
    "    \n",
    "    while True:\n",
    "        page = str(page_num)\n",
    "        \n",
    "        url_date = url_base_d + '?code=' + code_com + '&page=' + page\n",
    "        df_p = get_piece_date_price(url_date)\n",
    "        # print(\"page_num\", page_num, end=\", \")\n",
    "        # print(\"length\", len(df_p))\n",
    "        df_base = concat_df(df_base, df_p, 'date', 0)  # df concat후 'time' column을 기준으로 중복제거 후 0 column을 기준으로 정렬시킴.\n",
    "        \n",
    "        if len(df_p) < 10: # 10개 이하가 아니면 완료\n",
    "            break\n",
    "        if (startdate == df_base['date']).any(): # 시작일자와 일치하는 row가 있으면 더 이상 진행하지 않음.\n",
    "            break\n",
    "\n",
    "        page_num = page_num + 1\n",
    "\n",
    "    return df_base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8624b98e-d394-4d8d-aeeb-545d7fcdd9cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "company_start ['삼성전자', 'sec']\n",
      "company_end ['삼성전자', 'sec']\n",
      "company_start ['LG에너지솔루션', 'lgenergy']\n",
      "company_end ['LG에너지솔루션', 'lgenergy']\n",
      "company_start ['SK하이닉스', 'skhynix']\n",
      "company_end ['SK하이닉스', 'skhynix']\n",
      "company_start ['삼성바이오로직스', 'ssbio']\n",
      "company_end ['삼성바이오로직스', 'ssbio']\n",
      "company_start ['삼성SDI', 'sdi']\n",
      "company_end ['삼성SDI', 'sdi']\n",
      "company_start ['LG화학', 'lgchemical']\n",
      "company_end ['LG화학', 'lgchemical']\n",
      "company_start ['삼성전자우', 'secpre']\n",
      "company_end ['삼성전자우', 'secpre']\n",
      "company_start ['현대차', 'hyunmotor']\n",
      "company_end ['현대차', 'hyunmotor']\n",
      "company_start ['NAVER', 'naver']\n",
      "company_end ['NAVER', 'naver']\n",
      "company_start ['기아', 'kia']\n",
      "company_end ['기아', 'kia']\n",
      "company_start ['카카오', 'kakao']\n",
      "company_end ['카카오', 'kakao']\n",
      "company_start ['POSCO홀딩스', 'poscoholding']\n",
      "company_end ['POSCO홀딩스', 'poscoholding']\n",
      "company_start ['KB금융', 'kbbank']\n",
      "company_end ['KB금융', 'kbbank']\n",
      "company_start ['삼성물산', 'sscnt']\n",
      "company_end ['삼성물산', 'sscnt']\n",
      "company_start ['셀트리온', 'celltrion']\n",
      "company_end ['셀트리온', 'celltrion']\n",
      "company_start ['현대모비스', 'mobis']\n",
      "company_end ['현대모비스', 'mobis']\n",
      "company_start ['신한지주', 'shgroup']\n",
      "company_end ['신한지주', 'shgroup']\n",
      "company_start ['LG전자', 'lgelec']\n",
      "company_end ['LG전자', 'lgelec']\n",
      "company_start ['포스코퓨처엠', 'poscochemical']\n",
      "company_end ['포스코퓨처엠', 'poscochemical']\n",
      "company_start ['SK이노베이션', 'skinnovation']\n",
      "company_end ['SK이노베이션', 'skinnovation']\n",
      "company_start ['KT&G', 'ktng']\n",
      "company_end ['KT&G', 'ktng']\n",
      "company_start ['KT', 'kt']\n",
      "company_end ['KT', 'kt']\n",
      "company_start ['LG', 'lg']\n",
      "company_end ['LG', 'lg']\n",
      "company_start ['SK', 'sk']\n",
      "company_end ['SK', 'sk']\n",
      "company_start ['삼성생명', 'sslife']\n",
      "company_end ['삼성생명', 'sslife']\n",
      "company_start ['하나금융지주', 'hana']\n",
      "company_end ['하나금융지주', 'hana']\n",
      "company_start ['삼성전기', 'sselec']\n",
      "company_end ['삼성전기', 'sselec']\n",
      "company_start ['한국전력', 'koreaelec']\n",
      "company_end ['한국전력', 'koreaelec']\n",
      "company_start ['두산에너빌리티', 'doosanener']\n",
      "company_end ['두산에너빌리티', 'doosanener']\n",
      "company_start ['고려아연', 'koreazinc']\n",
      "company_end ['고려아연', 'koreazinc']\n",
      "company_start ['SK텔레콤', 'sktelecom']\n",
      "company_end ['SK텔레콤', 'sktelecom']\n",
      "company_start ['HMM', 'hmm']\n",
      "company_end ['HMM', 'hmm']\n",
      "company_start ['삼성화재', 'ssfire']\n",
      "company_end ['삼성화재', 'ssfire']\n",
      "company_start ['LG생활건강', 'lglife']\n",
      "company_end ['LG생활건강', 'lglife']\n",
      "company_start ['S-Oil', 'soil']\n",
      "company_end ['S-Oil', 'soil']\n",
      "company_start ['크래프톤', 'crafton']\n",
      "company_end ['크래프톤', 'crafton']\n",
      "company_start ['삼성에스디에스', 'sds']\n",
      "company_end ['삼성에스디에스', 'sds']\n",
      "company_start ['현대중공업', 'hhi']\n",
      "company_end ['현대중공업', 'hhi']\n",
      "company_start ['대한항공', 'koreanair']\n",
      "company_end ['대한항공', 'koreanair']\n",
      "company_start ['엔씨소프트', 'ncsoft']\n",
      "company_end ['엔씨소프트', 'ncsoft']\n",
      "company_start ['한화솔루션', 'hanhwasol']\n",
      "company_end ['한화솔루션', 'hanhwasol']\n",
      "company_start ['우리금융지주', 'woorifg']\n",
      "company_end ['우리금융지주', 'woorifg']\n",
      "company_start ['아모레퍼시픽', 'amore']\n",
      "company_end ['아모레퍼시픽', 'amore']\n",
      "company_start ['롯데케미칼', 'lottechem']\n",
      "company_end ['롯데케미칼', 'lottechem']\n",
      "company_start ['기업은행', 'ibk']\n",
      "company_end ['기업은행', 'ibk']\n",
      "company_start ['메리츠금융지주', 'meritz']\n",
      "company_end ['메리츠금융지주', 'meritz']\n",
      "company_start ['카카오페이', 'kakaopay']\n",
      "company_end ['카카오페이', 'kakaopay']\n",
      "company_start ['LG이노텍', 'lginnotek']\n",
      "company_end ['LG이노텍', 'lginnotek']\n",
      "company_start ['삼성엔지니어링', 'ssengineering']\n",
      "company_end ['삼성엔지니어링', 'ssengineering']\n",
      "company_start ['SK아이이테크놀로지', 'skietech']\n",
      "company_end ['SK아이이테크놀로지', 'skietech']\n",
      "company_start ['현대글로비스', 'glovis']\n",
      "company_end ['현대글로비스', 'glovis']\n",
      "company_start ['SK바이오사이언스', 'skbio']\n",
      "company_end ['SK바이오사이언스', 'skbio']\n"
     ]
    }
   ],
   "source": [
    "# naver_dir = 'data/naver_finance/date_data'\n",
    "naver_dir = 'date_data'\n",
    "\n",
    "url_base = 'https://finance.naver.com/item/sise_day.naver'  # sise_day\n",
    "\n",
    "code_dic = {'005930': ['삼성전자', 'sec'], '005380': ['현대차', 'hyunmotor'], \n",
    "        '035420': ['NAVER', 'naver'], '033780': ['KT&G', 'ktng']}\n",
    "\n",
    "code_dic = COMPANY_CODE\n",
    "# code_dic = {'005930': ['삼성전자', 'sec'],}\n",
    "\n",
    "for i, (code, company_name) in enumerate(code_dic.items()):\n",
    "    df_collect = get_date_price(url_base, code)\n",
    "    f_name = f'{naver_dir}/{company_name[1]}.csv'\n",
    "    df_collect.to_csv(f_name)\n",
    "    df_collect.to_pickle(f_name.replace('csv','pkl'))\n",
    "    print(i, end=\", \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6669a8ad-efec-4a67-be11-700c6c08bf43",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_collect.tail(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "920d4419-038b-4f9f-bb67-14562fd58e62",
   "metadata": {},
   "source": [
    "## 여기까지"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
